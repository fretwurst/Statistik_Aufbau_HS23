[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistik Aufbau",
    "section": "",
    "text": "Einleitung\nSicher freuen Sie sich schon auf «Statistik: Aufbau», und ich glaube, Sie haben allen Grund dazu. Manche freuen sich weniger – was ja auch normal und ok ist. Wieder andere, denken lieber daran, wie das Leben so sein wird, wenn Sie «Statistik: Aufbau» hinter sich haben. Ihnen allen will ich zur Seite stehen, damit Sie aus dem Modul das für sich Beste rausholen. Diejenigen, die in der Statistik ein mächtiges Tool entdecken, will ich ein tiefergehendes Verständnis ermöglichen. Denen, die die Statistik einfach gut absolvieren wollen, soll das Wichtigste vermittelt werden und die mit Graus auf das Modul schauen, soll das Grauen genommen und etwas Greifbares und Handhabbares angeboten werden, das sich – mit zumutbaren Investitionen – lösen lässt. Hier in der Einleitung schreibe ich Ihnen, was ich über den Sinn, die Mächtigkeit und die möglichen Ursachen für das Unbehagen denke.\nLiebe Grüsse\nBenjamin Fretwurst"
  },
  {
    "objectID": "index.html#was-bringt-uns-statistik",
    "href": "index.html#was-bringt-uns-statistik",
    "title": "Statistik Aufbau",
    "section": "Was bringt uns Statistik",
    "text": "Was bringt uns Statistik\nStatistik ermöglicht es, eine Unmenge an Beobachtungen in Beziehung zueinander zu setzen. Die Art dieser Beziehungen wird aus der Alltagswahrnehmung abgeleitet und durch Formulierung wissenschaftlicher Hypothesen konkretisiert.\nWenn wir zum Beispiel davon sprechen, dass die Leute einfach nur das wichtig finden, was Ihnen die Medien vorgeben, dann wird damit ein Zusammenhang formuliert. Etwas konkreter würde ein KW-ler sagen: Die Menschen lernen aus der Thematisierung in den Medien, was wichtige Themen sind. Und weil das eine Theorie ist, bekommt sie auch noch einen Namen: «Agenda-Setting» (AS).\nGegen den AS könnte man einwenden: «Das gilt nicht immer. Die Leute kriegen schon mit, wenn die Preise steigen – dazu brauchen sie nicht die Medien.» Der AS gilt also nicht für alle Themen, sondern nur für solche, die die Leute nicht am eigenen Leib erfahren können. Es wird also in ‹obtrusive› und ‹nonobtrusive Issues› unterschieden. Jetzt haben wir einen Zusammenhang formuliert, der zusätzlich Randbedingungen enthält. Abgesehen von der Theorie könnte man die Forschungsfrage stellen, ob AS in gleichem Masse für Gebildetere und weniger Gebildete gilt. In der Alltagsbeobachtung wird es jetzt schon kompliziert, da wir diese Randbedingungen schwerlich alle gleichzeitig gegeneinander halten können. Selbst wenn wir den Bildungsstand mitbeobachten können, ist das nicht mit der vollen Differenziertheit möglich. Die wissenschaftliche Datenerhebung dient der Aufzeichnung vieler unabhängiger Beobachtungen. Multivariate Statistik ermöglicht es uns, diese Beobachtungen so zueinander in Beziehung zu setzen, dass wir am Ende einfache Kennwerte bekommen, die für Zusammenhänge stehen.\n\nWas beschreibt die Funktion von Statistik am besten?\n\n\n\n\n Studierende in Angst und Schrecken versetzen. Sichert die Stabilität von Gebäuden. Eine Kulturtechnik zur gedanklichen Verarbeitung von Beziehungen zwischen Messungen."
  },
  {
    "objectID": "index.html#überblick-analysemethoden",
    "href": "index.html#überblick-analysemethoden",
    "title": "Statistik Aufbau",
    "section": "Überblick Analysemethoden",
    "text": "Überblick Analysemethoden\n\n\n\nSystematik gesamt"
  },
  {
    "objectID": "index.html#zur-verwendung-dieser-seite",
    "href": "index.html#zur-verwendung-dieser-seite",
    "title": "Statistik Aufbau",
    "section": "Zur Verwendung dieser Seite",
    "text": "Zur Verwendung dieser Seite\n\nZitation: Fretwurst, B. (2022). Statistik und Datenanalyse: Aufbau. Begleittext zum Modul am IKMZ im HS22. https://www.ikmz.uzh.ch/static/methoden/Statistik-Aufbau/. Abrufdatum: [aktuelles Datum]."
  },
  {
    "objectID": "01_Wiederholung.html#die-folien-zur-sitzung",
    "href": "01_Wiederholung.html#die-folien-zur-sitzung",
    "title": "1  Was bisher geschah",
    "section": "1.1 Die Folien zur Sitzung",
    "text": "1.1 Die Folien zur Sitzung"
  },
  {
    "objectID": "01_Wiederholung.html#univariate-statistik",
    "href": "01_Wiederholung.html#univariate-statistik",
    "title": "1  Was bisher geschah",
    "section": "1.2 Univariate Statistik",
    "text": "1.2 Univariate Statistik\nAlle Statistik baut auf Masse zentraler Tendenz und Streumasse auf. Für das, was wir in diesem Semester tun, brauchen wir eigentlich nur den Mittelwert und die Varianz (bzw. die Standardabweichung). Der Mittelwert entspricht der Frage: Wie siehts denn normalerweise aus? Die Standardabweichung beantwortet die Frage: Gibt der «Normalwert» (Mittelwert) eine brauchbare Auskunft über die untersuchten Personen oder sind die doch alle sehr unterschiedlich. Mit dem Mittelwert und der Abweichung vom Mittelwert können wir Verteilungen beschreiben, die sich nach bekannten Gesetzen verteilen: den Wahrscheinlichkeitsgesetzen. Die Normalverteilung ist dabei die wichtigste Verteilung. Bei dieser gedachten Idealverteilung steht der Mittelwert genau in der Mitte und die Streuung ist ordentlich symmetrisch um den Mittelwerqt herum verteilt. Wegen dieser Eigenschaften, geben hier also der Mittelwert und die Standardabweichung die volle Information über die Art der Verteilung ab \\(N(\\overline{x},s_x)\\). Die Standardnormalverteilung (nach der z-Transformation) hat einen Mittelwert von 0 und eine Standardabweichung von 1.\n\\[\\begin{align}    \n\\overline{x} = & \\frac{1}{n}\\sum_i^n(x_i) \\label{eq:Mittelwert-x}\\\\     \n\\overline{y}  = & \\frac{1}{n}\\sum_i^n(y_i) \\label{eq:Mittelwert-y}\\\\   \n   s^2 = V = & \\frac{1}{n} \\sum_i^n(x_i-\\overline{x})^2 \\label{eq:Varianz-x}\\\\    \n    s  = & \\sqrt{\\frac{1}{n} \\sum_i^n(x_i-\\overline{x})^2} \\label{eq:SD-x}\\\\   \n      z_i= &\\frac{x_i-\\overline{x}}{s} \\label{eq:z-Transformation}\n\\end{align}\\]\nEine z-verteilte Grösse hat immer: \\(\\overline{x}=0\\) und \\(s = 1\\). - Nach der z-Transformation gibt es immer eine Entsprechung zwischen den ursprünglichen Ausprägungen und denen der z-Verteilung. - Werte einer standardisierten Verteilungen sind vergleichbar."
  },
  {
    "objectID": "01_Wiederholung.html#bivariate-statistik",
    "href": "01_Wiederholung.html#bivariate-statistik",
    "title": "1  Was bisher geschah",
    "section": "1.3 Bivariate Statistik",
    "text": "1.3 Bivariate Statistik\n\n1.3.1 Kreuztabellen\nWenn wir Daten analysieren, dann können wir sie visualisieren. Eine sehr starke Form der Visualisierung sind Tabellen. Der Vorteil von Tabellen ist, dass sie sehr dicht an den Daten sind, wir also z.B. sehen können dass, 57 Prozent der Haushalte noch über mindestens ein Radiogerät verfügen. In der Deutschschweiz (DS) sind es sogar 62 Prozent, während es in der Romandie (FS) 44 Prozent sind und im Tessin und den weiteren Teilen der italienischen Schweiz (IS) 63 Prozent. Man kann aus Kreuztabellen für die beiden berücksichtigten Variablen den ursprünglichen Datensatz rekonstruieren, weil man weiss wie viele Leute befragt wurden, wie viel Prozent aus der DS sind, aus der FS und aus der IS und wie viele Leute jeweils mindestens ein Radio haben. Irgendwann kann ich Ihnen mal zeigen, wie man aus einer Kreuztabelle eine Korrelation berechnen kann – man braucht nicht mehr. Also gut, die Tabellen enthalten viele Informationen und können, mit ein bisschen Anleitung, von fast jedem gelesen werden (anders als unserer schönen Regressions- oder Strukturgleichungsmodelle). Das Problem ist aber, dass man sehr schnell einschläft, wenn einem so eine Tabelle vorgelesen wird. Wenn man sie selbst liesst, ist es einfach sehr viel, worauf man schauen muss, was man vergleichen und dann vielleicht sogar noch texten muss. Eine Korrelation fast in einem direkt lesbaren Wert das Zusammen, was Sie vielleicht aus 5*5, also 25 Tabellenzellen mühsam herauslesen und dann immer noch nur so vom Gefühl her texten können. Tabellen sind also relativ voraussetzungsfrei interpretierbar, aber es ist sehr ermüdend und langwierig.\n\nBeispiel für eine Kreuztabelle mit Link zu mehr (interaktiven) Kreuztballen:\n\n\n\n\n1.3.2 Covarianz und Korrelation\nVor ein paar Zeilen hatte ich behauptet, man brauche als Kenngrössen in der Statistik nicht mehr als Mittelwert und Standardabweichung. Das sollte der Beruhigung dienen und ist ein bisschen gelogen. Ein Merkmal kann man mit diesen Kenngrössen in der Regel gut beschreiben, aber um das Zusammenspiel zweier Variablen beschreiben zu können, braucht es noch eine Kenngrösse: die Kovariation. Wieder etwas vereinfacht und doch nicht ganz falsch: fast alle multivariate Statistik baut auf Mittelwerten, Varianzen und Kovarianzen auf.\nKovarianz (cov oder auch C) sieht genauso aus, wie die Definition der Varianz, nur, dass nicht die Mittelwertabweichungen einer Variablen quadriert werden, sondern die Mittelwertabweichungen zweier Variablen multipliziert werden. Berechnet man die Kovarianz einer Variable mit sich selbst, kommt wieder die Varianz raus. Da Varianz und Mittelwert nicht standardisiert sind, ist auch die Kovarianz nicht standardisiert. Rechnet man mit standardisierten Variablen, kommt auch ein standardisierter Wert für die Kovarianz raus. Da der für Vergleiche von unvergleichlicher Bedeutung ist, hat auch dieser Kennwert einen eigenen Namen bekommen: Korrelation (r).\n\\[\\begin{align}\n      cov = C =  & \\frac{1}{n}\\sum_i^n(x_i-\\overline{x})\n          (y_i-\\overline{y}) \\label{eq:Covarianz} \\\\\n      r = &  \\frac{\\sum_i^n(x_i-\\overline{x})\n          (y_i-\\overline{y})}{n \\cdot s_x \\cdot s_y} \\label{eq:Korrelation}\n\\end{align}\\]\n\nDiese kleine shiny-App lässt Sie spielerisch raten, wie gross wohl eine Korrelation ist, wenn Sie eine Punktwolke sehen. Sie werden feststellen, dass die Statistik feiner gucken kann als der Mensch. Wo Sie kaum eine Korrelation feststellen können, misst die Statistik durchaus noch Zusammenhänge:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.3 Bivariate Regression\nDie bivariate Regression ist im Grunde eine gehaltvollere Korrelation. Bei der Regression bestimme ich, im Unterschied zur Korrelation, was die abhängige Variable sein soll (AV) und was die unabhängige Variable sein soll (UV). Was die Regression bringt, sieht man in folgenden Formeln. Wenn wir in der ersten Zeile der Formeln @ref(eq:Mittelwert-Modell) davon ausgehen, dass wir keine UV haben, die die AV erklärt, dann haben wir als beste Information nur den Mittelwert. In dem Fall wäre unser Minimodell also: Die Werte in der Variable \\(Y_i\\) sind am besten durch ihren Mittelwert \\(\\overline{Y}\\) «erklärt». Übrig bleiben die Abstände zwischen den gefundenen Werten und diesem Minimodell, also dem Mittelwert von Y. In der zweiten Zeile @ref(eq:Regressions-gleichung) sind wir schon klüger und nehmen an, dass eine Variable X dafür verantwortlich ist, dass die \\(Y_i\\) so ausfallen, wie sie ausfallen. In der zweiten Zeile heisst es im Grunde: Die Werte in der AV Y sind abhängig von der UV X, wobei jeder Wert X mit einem b multipliziert werden muss und dann ein Rest \\(e_i\\) bleibt. Damit Y nicht 0 sein muss, wenn X 0 ist (b*0=0), kommt zu dem \\(b_2\\) noch ein \\(b_1\\). Fertig ist die bivariate Regressionsgleichung.\n\\[\\begin{align}\n  Y_i & = \\overline{Y} + e_i  \\label{eq:Mittelwert-Modell} \\\\\n  Y_i & = b_1 + b_2X_i + e_i \\label{eq:Regressions-gleichung} \\\\\n  \\hat{Y_i} & = b_1 + b_2X_i \\label{eq:Regressions-Modell} \\\\\n  Y_i & = \\hat{Y_i}+e_i \\label{eq:Varianz-Modell-Residuum}\n\\end{align}\\]\nJetzt könnten wir noch die vorhergesagten \\(\\hat{Y}\\) (gekennzeichnet durch ein Dach) durch das Modell abbilden, wobei dann einfach kein \\(e_i\\) bleibt (dritte Zeile @ref(eq:Regressions-Modell)). In der untersten Formel @ref(eq:Varianz-Modell-Residuum) haben wir die in der Stichprobe gemessenen \\(Y_i\\), die gleich gesetzt sind mit den geschätzten \\(Y_i\\) und dem Rest \\(e_i\\).\nDas \\(b_2\\) gibt jetzt an, um wie viel Y grösser (oder kleiner) ist, wenn X um eine Einheit seiner Skala grösser wird. Die b’s sind also skalenabhängig. Weil das nicht immer leicht zu interpretieren ist und uns oft die Skalierung nicht weiter interessiert, wurden die standardisierten Regressionkoeffizienten erfunden. Bei diesen steht im Grunde wieder eine z-Transformation im Hintergrund, die die Skala «herausrechnet», indem durch die Standardabweichung geteilt wird.\nDie standardisierten Regressionskoffizienten geben einen Zusammenhang in Standardabweichungen an: Wenn x um eine Standardabweichung grösser ist, um wie viele Standardabweichungen ist dann y grösser (kann negativ sein)?\nSie sind definiert als: \\(BETA = b\\cdot\\frac{s_x}{s_y}\\)\nDie BETAs1 sind den partiellen Korrelationen sehr ähnlich: +1 ist ein perfekter positiver Zusammenhang, 0 kein Zusammenhang und -1 ein perfekter negativer Zusammenhang. Interpretieren würde ich ab 0.1, wenn sie signifikant sind."
  },
  {
    "objectID": "01_Wiederholung.html#inferenzstatistik",
    "href": "01_Wiederholung.html#inferenzstatistik",
    "title": "1  Was bisher geschah",
    "section": "1.4 Inferenzstatistik",
    "text": "1.4 Inferenzstatistik\nWas in den Daten ist, die wir analysieren, das ist in den Daten – Punkt. Wir können Daten mit all diesen Tools untersuchen. Dafür brauchen wir natürlich vollständige Daten über das, was wir untersuchen wollen. Die haben wir aber oft nicht, weil es schlicht zu teuer ist und alle Menschen unfassbar nerven würde, wenn alle Sozialforscher:innen und Psycholog:innen und Wirtschaftswissenschaftler:innen usw. dauernd alle Menschen befragen würden. Das machen wir nicht, weil eine faszinierende Eigenschaft der Statistik ist, dass wir aus Teildaten Schlüsse auf die «Gesamt-Daten» ziehen können. Dieses Schliessen nennt man Inferenz (besser über das Englische zu merken: to infer). Dafür ist es notwendig, dass wir die Daten ohne bewussten Bias aus den Gesamtdaten entnommen haben.\nEinen Bias ausschliessen können wir, wenn wir den Zufall walten lassen. Zufall ist nichts anderes als das Nicht-bewusst-oder-unbewusst-Auswählen. Wenn wir zufällig gezogen haben, dann haben wir eine gute Chance auf ein unverzerrtes Abbild der Grundgesamtheit, die uns interessiert. Dann berechnen wir die ganzen Kennwerte der deskriptiven Statistik für die Stichprobe und können anhand der Zufallsgesetze bzw. Wahrscheinlichkeitstheorie auf die Verteilung in der Grundgesamtheit schliessen. Dabei bleibt eine Unsicherheit, weil der Zufall zufällig ungünstig ausfallen kann (wir haben auch schon Pferde kotzen sehen).\nWir können allerdings sagen, wie wahrscheinlich bzw. unwahrscheinlich es ist, dass wir extrem viel Pech hatten mit unserer Stichprobenziehung. Dabei sind wir sehr vorsichtig und schätzen und testen konservativ. Die Frage ist also immer: Kann ich, wenn ich ganz vorsichtig und konservativ bin, ausschliessen, dass ich sehr viel Pech bei der Ziehung meiner Zufallsstichprobe hatte? Das Pech bei der Zufallsziehung wird als Nullhpyothese bezeichnet (H0). Die sagt nämlich, wir haben eigentlich keinen Unterschied oder Zusammenhang und trotzdem in unserer Stichprobe den Unterschied (abweichend von 0) gefunden, den wir gefunden haben. Wenn wir – bei aller Vorsicht – sagen, dass ein Effekt (Unterschied oder Zusammenhang) so gross ist, dass die Nullhypothese so unwahrscheinlich wird (5% Irrtumswahrscheinlichkeit), dass wir sie ablehnen können, dann haben wir etwas gefunden, etwas Signifikantes!\nOb das signifikante Ergebnis unserer Theorie entspricht oder nicht, das müssen wir dann noch schauen, aber erstmal können wir festhalten, dass wir überhaupt etwas gefunden haben und nicht sagen müssen: Wir haben zwar einen positiven Zusammenhang gefunden, aber statistisch müssen wir die Nullhypothese beibehalten, dass der Zusammenhang positiv sein könnte, aber auch 0 oder negativ. Wenn wir nichts ausschliessen können, wissen wir nichts. Die H0 bedeutet also, dass wir statistisch nichts feststellen können und daher auch nicht zu Wissen gelangen. Wenn Sie irgendwo lesen, dass jemandem in seinen Analysen eine H0 zugute kommt oder die Annahme einer Nullhypothese als Erkenntnis gefeiert wird, seien Sie sehr skeptisch! Meistens liegt ein Denkfehler zugrunde: Dass ich 0 nicht ausschliessen kann, heisst nicht, dass ein Zusammenhang oder Unterschied nicht existiert. Wenn wir z.B. in einer Stichprobe einen Unterschied zwischen zwei Gruppen gefunden haben, dann ist die Wahrscheinlichkeit dafür, dass der Unterschied in der Grundgesamtheit 0 ist genauso gross, wie die Wahrscheinlichkeit, dass der Unterschied in der Grundgesamtheit doppelt so gross ist, wie der den wir gefunden haben. Das liegt daran, dass die Glockenkurve der Fehlerverteilung um einen gefundenen Wert symmetrisch um den gefunden Unterschied verteilt ist.\n\nMit dieser App lässt sich etwas mit der Normalverteilung experimentieren:\n\n\n\n1.4.1 Konfidenzintervalle für Mittelwerte\nKonfidenzintervalle geben einen Wertebereich an, in dem die Parameter (GG) der Stichprobenkennwerte mit einer angebbaren Wahrscheinlichkeit liegen.\n\\[\\begin{align}\n   \\text{ KI:}& \\overline{X}\\pm z_1 \\cdot SE \\label{eq:Konfidenz-Intervall-1}\\\\\n   \\text{KI:}& \\overline{X}\\pm z_1 \\cdot \\frac{s_x}{\\sqrt{n}} \\label{eq:Konfidenz-Intervall-2}\\\\\n   \\text{KI}_{l.05} =& \\overline{x} - 1.96 \\cdot \\frac{s_x}{\\sqrt{n}} \\label{eq:Konfidenz-Intervall-3}\\\\\n   \\text{KI}_{r.05} =& \\overline{x} + 1.96 \\cdot \\frac{s_x}{\\sqrt{n}}\\label{eq:Konfidenz-Intervall-4}\n\\end{align}\\]\n\nMit dieser kleinen Onlineapp können Sie sich mal einen Eindruck davon verschaffen, wie Konfidenzintervalle reagieren. Schalten Sie sich mal «Show Confidence Intervals» und «Show True Parameter» an und stellen die «Number of Experiments» auf 20. Dann suchen Sie mal, wie viele Konfidenzintervalle den wahren Wert nicht enthalten:\n\n\n\nKI-App\n\n\n\n\n\n1.4.2 Hypothesentesten\nErkenntnistheoretisch prüfen wir unsere Hypothesen, um unser Wissen immer weiter abzusichern, bzw. Veränderungen festzustellen. Eine Theorie muss sich in der empirischen Sozialforschung immer wieder an neuen Daten bewähren. Wiedersprechen meine Daten immer wieder (replizierbar) der Theorie, wird sie modifiziert oder aufgegeben, wenn wir was Besseres haben.\nBei der statistischen Analyse gibt es neben unseren wissenschaftlichen Hypothesen auch rein statistische Fragen. Wenn wir einen Mittelwert in unserer Stichprobe finden, ist der sicher nicht identisch mit dem entsprechenden Parameter \\(\\mu\\) in der Grundgesamtheit (Population). Es stellt sich also immer die Frage nach möglichen Unschärfen. Über die statistischen Hypthesen können wir Aussagen treffen, wenn wir den Gesetzen der Wahrscheinlichkeit (Stochastik) eine Chance geben, also wenn wir Zufallsstichproben ziehen. Diese Zufallsstichproben ziehen wir aber nicht aus der Grundgesamtheit, also allen Objekten oder Subjekten, für die unsere Theorie Gültigkeit beansprucht. Wir ziehen Stichproben in einem abgesteckten Zeitrahmen und unter den Bedingungen der Machbarkeit. Wer kein Telefon hat, wird nicht angerufen. Um dieser Unterscheidung nachdruck zu verleihen, unterscheide ich Grundgesamtheit und Auswahlgesamtheit. Für letztere gilt: Jedes Element der Auswahlgesamtheit hat eine von 0 verschiedene Chance in die Stichprobe zu gelangen.\n\nKönnte in der Auswahlgesamtheit der wahre Wert auch 0 sein, oder ein anderes Vorzeichen haben?\nDie Nullhypothese ist eine statistische Hypothese gegen Falschentscheidungen aufgrund von Zufallsziehungen.\nNullhypothesen werden anhand von bekannten Verteilungen getestet.\n\n\n\n\n\n\nHypothesentesten\n\n\n\n\n\nLink zu einer guten App zum Probieren:"
  },
  {
    "objectID": "02_GLM.html#varianzanalyse",
    "href": "02_GLM.html#varianzanalyse",
    "title": "2  Das lineare Modell",
    "section": "2.1 Varianzanalyse",
    "text": "2.1 Varianzanalyse\nBei der Varianzanalyse werden Streuungen zerlegt. Das bedeutet, dass die Unterschiede zwischen den Fällen (z.B. Personen) in Bezug auf eine Variable (z.B. Links-Rechts-Spektrum) einen Mittelwert (Durchschnitt) haben und Abweichungen von diesem Mittelwert. Da die Abweichungen von einer Mitte negativ sind (links vom Mittel) und positiv sind (rechts vom Mittel) würde sich eine Summe aus allen Mittelwertabweichungen auf 0 aufaddieren – darum ja eben auch Mittelwert, weil er in der Mitte liegt. Darum nehmen wir von den Mittelwertabweichungen immer das Quadrat. Die Quadrate (Minus * Minus = Plus und Plus * Plus = Plus) ergeben in der Summe einen positiven Wert und wenn man den durch die Anzahl n der Fälle teilt, dann hat man die Varianz. Anders gesagt: Die Varianz ist die durchschnittliche quadrierte Mittelwertabweichung (so steht’s ja auch schon oben).\nBei der Varianzanalyse teilen wir unsere Stichprobe in Unterstichproben auf, also z.B. anhand von zwei Gruppen, wie Stadtbevölkerung und Landbevölkerung und schauen dann wo die Stadtbevölkerung im Mittelwert auf der Links-Rechts-Skala liegt und wo die Landbevölkerung im Mittel der Links-Rechts-Skala liegt. Mit hoher Sicherheit werden sich die Mittelwerte unterscheiden. Nimmt man nun die Varianz der Landbevölkerung um ihren Mittelwert und die Varianz der Stadtbevölkerung um ihren Mittelwert und addiert die zusammen, kommt ein kleinerer Wert heraus, als wenn man die Varianz für die Links-Rechts-Skala berechnet (nur wenn beide Mittelwerte identisch wären, wäre auch die Summe der Varianzen identisch), ohne die Unterscheidung zwischen Stadt und Land zu machen. Der Wert wird immer kleiner, je weiter die beiden Mittelwerte voneinander entfernt liegen. Wenn für die Links-Rechts-Skala die Summe der Varianzen der beiden Gruppen deutlich kleiner ist als die Gesamtvarianz, dann hat die Unterscheidung zwischen Stadt- und Landbevölkerung Varianz «aufgeklärt». Das ist Varianzaufklärung und die Basis der Varianzanalyse. Es geht kurz gesagt darum, ob die Unterschiede in einer Variable gross sind, weil sie durch eine andere Variable bedingt werden, für die die Varianz zerlegt wird. Darum sagt man auch «Varianzzerlegung». Wir werden uns das noch ausführlich bis genüsslich anschauen in diesem Semester.\n\nHier können Sie mit einer kleinen Onlineapp mit einer ANOVA (ANalysis Of VAriance) interaktiv herumprobieren:\n\n\n\nANOVA\n\n\n\nNun stellen Sie sich vor, Sie machen eine Auswertung und haben eine Variable die Sie erklären wollen, also eine abhängige Variable (AV), wie z.B. die Verweildauer auf TikTok-Videos. Die erklären Sie damit, wie viel Spass jemand an einem gezeigten Video hat. Sie könnten den Spass einteilen in «kein Spass», «wenig Spass» und «viel Spass». Dann könnten Sie für die drei Gruppen eine Varianzanalyse rechnen, testen Ihre Hypothesen mit t-Tests oder einer One-Way-Anova und kommen anhand der t-Werte oder auch F-Werte zu einer Entscheidung über statistische Hypothese: Der Zusammenhang ist signifikant oder nicht. Ok. – Wenn das «lineare Modell» Varianzanalyse ist und Regression, dann müsste man doch dieselbe Analyse mit einer Regression machen können. Und ja, das kann man. Und es kommen ganz genau dieselben Ergebnisse raus: Dieselben t-Werte, dieselben F-Werte und natürlich, darauf fussend, dieselben p-Werte (Wahrscheinlichkeit, dass die t- und F-Werte zustandekommen, obwohl die Nullhypothese gilt). Wir probieren das mal: Und es wäre extrem peinlich für mich, wenn da unterschiedliche Werte rauskommen – aufregend!\nIn R kann man mit Extrapaketen auch Varianzanalysen machen. Es gibt aber keine Pakete für Regressionen. Das liegt daran, dass das lineare Modell (lm) in R die Regression ist! Wir werden uns im ersten Teil des Moduls also mit dem linearen Modell auseinandersetzen."
  },
  {
    "objectID": "02_GLM.html#regression",
    "href": "02_GLM.html#regression",
    "title": "2  Das lineare Modell",
    "section": "2.2 Regression",
    "text": "2.2 Regression\nDie Regression ist das einfachste und gleichzeitig mächtigste Werkzeug multivariater Datenanalyse. Aus den Kovarianzen mehrerer Variablen wird eine Funktion mit wenigen Kennwerten berechnet. Diese Kennwerte als Einzelwerte geben Auskunft über die Zusammenhänge zwischen einzelnen Prädiktoren und einer abhängigen Variablen. Man muss also vorher sagen, was man erklären will und womit man es erklären will. Die zu erklärende Grösse nennt man in der Sozialwissenschaft (und anderen Disziplinen): abhängige Variable (AV oder DV) und die Erklärungsgrössen nennt man: unabhängige Variablen (UV oder IV).\nDie Regression baut auf Kovarianzen auf (bzw. Korrelationen, die wir uns besser vorstellen können). Die Regressionsgerade wird bei einer bivariaten Regression durch eine Konstante (in der Abbildung @ref(fig:KorrelationRegression) ist sie 1) und einem Anstieg je Variable gekennzeichnet (in der Abbildung ist es 0,5 für die eine UV = x).\n\n\n\n\n\nVon der Korrelation zur Regression\n\n\n\n\nDas bedeutet, dass bei einer bivariaten Regression zwei b’s die Lage der Regressionsgeraden bestimmen: Das ist zum einen die Konstante \\(b_1\\) und zum anderen der Anstieg \\(b_2\\) für die Gerade. In der Abbildung @ref(fig:RegressionsGeraden) sehen Sie links zwei Regressionsgeraden mit unterschiedlichen \\(b_2\\) (rot positiv und grün negativ). Auf der rechten Seite sehen Sie drei Regressionsgeraden mit unterschiedlichen \\(b_1\\), wobei das b der roten Gerade am grössten ist (knapp 70), grün am kleinsten (bischen über 20) und blau in der Mitte liegt (knapp 40).\n\n\n\n\n\nB’s bei bivariaten Regressionen\n\n\n\n\n\nWelche Funktion und Eigenschaften haben die Regressionskoeffizienten b?\n\n\n\n\n Wenn verschiedene Regressionsgeraden den selben Anstieg haben, dann ist ihr b gleich. \\(b_2\\) gibt in der Regression den Anstieg der Regressionsgerade für die UV \\(x_2\\) an. \\(b_3\\) ist eine Konstante, die den Schnittpunkt mit der y-Achse angibt (engl. intercept) Wenn b = 1, dann ist der Zusammenhang perfekt.\n\n\n\n\n\n\n\n\n\n2.2.1 Das Modell und die Regressionsgleichung als Schätzung\nDie formelle Schreibweise eines Regressionsmodells enthält griechische Buchstaben um zu signalisieren, dass es sich hier um unbekannte Grössen, die Parameter in der Grundgesamtheit, handelt. So lange wir über die Qualität und die Eigenschaften von Regressionsrechnungen sprechen, wird uns der Unterschied zwischen \\(\\beta\\)’s und b’s interessieren.\nAls Gleichung heisst das, dass die Abhängige Variable \\(Y_i\\) durch eine gewichtete Summe (siehe Formel @ref(eq:RegressionParameter)) von einer oder mehreren unabhängigen Variablen erklärt wird. Diese UVs werden in der Regel mit X gekennzeichnet und weil es mehrere davon geben kann, werden sie durchnummeriert. Also mit dem Subscript i für das Durchzählen werden sie griechisch für die Parameter als \\(\\beta_2X_{i2}\\) bezeichnet oder eben als \\(\\beta_3X_{i3}\\) usw. Dann gibt es noch den Rest \\(U_i\\). Das ist also das theoretische statistische Modell, dessen Parameter wir mit Kennwerten schätzen wollen.\n\\[\\begin{align}\n   Y_i&=\\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i3} + U_i \\label{eq:RegressionParameter}\n\\end{align}\\]\nWenn man mal genau schaut was hier eigentlich noch variabel ist, nach der Stichprobenziehung, dann wird klar, dass die \\(Y_i\\) ja in der Datenerhebung gemessen wurden und damit Werte enthalten, die wir statistisch nicht mehr ändern. Das Gleiche gilt für die \\(X_i\\)-Werte der Variablen \\(X_2\\) und \\(X_3\\). Also sind diese Grössen eigentlich keine «Variablen» mehr, sondern längst durch echte Werte fixiert. Zu schätzen sind nur die Bs, also \\(b_1\\), \\(b_2\\) und \\(b_3\\) (übrigens nummerieren wir die so durch, weil sie später als Vektor in der Matrizenrechnung die erste Zeile belegen, die zweite und dritte usw.). Wenn wir die Regressionskoeffizienten, die b’s in unserer Stichprobe, berechnet haben, müssen wir uns noch fragen, wie gut, also unverzerrt und genau sie die unbekannten Parameter (\\(\\beta\\)s) messen, also – etwas technischer ausgedrückt – ob die b die \\(\\beta\\) erwartungstreu und effizient schätzen. Dafür gibt es einige Voraussetzungen, die wir uns später [in Kapitel noch nicht da] noch anschauen werden. Am Ende der Formel steht das \\(e_i\\) für die Fehler, also den unerklärten Rest der Varianz, der zwischen den durch das Modell geschätzten Werten (gekennzeichnet mit einem Dach als \\(\\hat{Y_i}\\)) und den gemessenen Werten liegt. Während die s’s die Schätzer für die \\(\\beta\\)s sind, ist das \\(e_i\\) kein Schätzer für \\(U_i\\). Das liegt daran, dass das \\(e_i\\) nur eine Fehlerstreuung in der Stichprobe ist und \\(U_i\\) viel mehr angibt, dass unberücksichtigte Einflussgrössen und ein stochastischer Rest nicht vom Modell abgebildet sind.\n\\[\\begin{align}\nY_i&=b_1 + b_2X_{i2} + b_3X_{i3}+e_i\n\\end{align}\\]\nBei einer Regression mit zwei UVs wird praktisch eine Ebene in die Punktwolke gelegt (siehe @ref(fig:Regressionsebene)). Wir schätzen aber eine multivariate Regression, damit wir bivariat interpretieren können, also je UV sagen, wie stark der Effekt auf die AV ist. Insofern interpretieren wir je Variable nur ein b oder ein (BETA), was dem Ansteig (Zusammenhang) einer Variablen entspricht. Das können wir machen, weil die Statistik bzw. unser Statistikprogramm für uns die Kontrolle der übrigen Variablen übernimmt und wir schön die kontrollierte bivariate Beziehung interpretieren können. Die b’s beschreiben dabei die Gerade, die die Ebene an der Stelle bildet, die für die andere Variable der Durchschnitt ist. Bei drei UVs spannen die b’s zusammen eigentlich einen Raum auf, was sich aber niemand mehr visuell vorstellen kann. Die Statistik kann das aber und erledigt das so für uns, dass wir uns immer nur die Beziehungen anhand der jeweiligen b’s der einzelnen UV’s anschauen können.\n\n\n\n\n\nR-Quadrat\n\n\n\n\n\nHier wird eine Regression recht gut als Punktwolke visualisiert: http://shiny.calpoly.sh/3d_regression/.\n\n\n\nRegression\n\n\n\n\nSchreiben Sie die Formel für die einfache bivariate Regression auf?\n\n\nLösung anschauen\n\n\\[\\begin{align}\nY_i&=b_1 + b_2X_{i2} +e_i\n\\end{align}\\]\n\n\n\n\n2.2.2 OLS\nEine der einfacheren und grundlegenden Methoden um die b’s zu bestimmen ist die Methode der kleinsten Quadrate bzw. OLS, was das Akronym für Ordinary Least Squares ist. Mit dieser Methode legt die Mathematik eine Gerade in eine Punktwolke, weil sie es nicht visuell und intuitiv machen kann. Das Prinzip ist recht einfach: Man versucht b’s zu finden, für die die Fehler möglichst klein sind. Das ist im Grunde die Optimierungsaufgabe der OLS-Methode. Genau das machen wir auch, wenn wir eine Gerade in eine Punktwolke legen, wir bauen sie so ein, dass sie «optimal reinpasst» also die Abstände zu den einzelnen Punkten minimal sind.\n\nSehr gut hier zum anschauen und spielen:\n\n\n\nOLS-App\n\n\n\nAls Beispiel hatte ich in der Vorlesung gebracht, dass man auch mal überlegen könnte, welcher Wert eine Verteilung einer Variablen optimal repräsentieren würde. Wenn wir dieses Optimierungsproblem an OLS übergeben würden, dann würden wir sagen: Suche einen Wert a aus allen möglichen a-Werten, der für eine Variable x die kleinsten quadrierten Abstände hat. Damit es OLS versteht würden wir schreiben: \\(\\text{OLS bitte minimiere folgende Gleichung:} \\sum_i{(x_i-a)^2}\\)\nJetzt wissen wir, dass die quadrierten Abweichungen gross sein müssen, wenn a links vom Optimum liegt und immer kleiner wird, wenn wir uns dem optimalen a-Wert annähern. Dann wird die Summe der quadratischen Abstände wieder grösser. Also haben wir eine Funktion, die einer quadratischen Funktion folgt (dass die so aussieht, müssen wir garnicht wissen, aber es hilft vielleicht der Vorstellung). Wenn wir wissen wollen, wo diese Funktion ihr Minimum hat, dann können wir die Funktion ableiten und dann nach der Nullstelle der abgeleiteten Funktion suchen. An der Stelle liegt dann der a-Wert, der die Streuung einer jeden Variablen optimal abbildet, weil wir diese Ableitung völlig abstrakt und ohne konkrete Werte gemacht haben und sie daher immer gilt. Also:\n\\[\\begin{align}\n  \\frac{df}{da} = & \\sum_i{(x_i-a)^2}^{\\prime} = 0 \\label{eq:OLS-Ableitung} \\\\\n  0 = & \\sum_i{[x_i^2 - 2x_ia + a^2]}^{\\prime} \\label{eq:OLS-Ableitung2}\n\\end{align}\\]\nIn der ersten Zeile das df/da bedeutet, dass abgeleitet (differenziert) werden soll und zwar die Funktion f nach a. In der zweiten Zeile sehen wir dann schon die Ableitung nach Ableitungsregeln (wer extrem Bock hat, kann sich die ja nochmal angucken) und gleich auch schon mit 0 gleichgesetzt.\nIn der nächsten Zeile @ref(eq:Umstellen1) wird ein bischen aufgelöst und umgestellt (müssen Sie nicht können).\n\\[\\begin{align}\n  0 = & -2\\sum_i{x_i} + 2na & |:2n\\ |+\\sum_i{x_i} \\label{eq:Umstellen1}\\\\\n  \\frac{\\sum_i{x_i}}{n} = & a \\label{eq:Umstellen2} \\\\\n  a = &\\overline{x} \\label{eq:Mittelwert-Optimum}\n\\end{align}\\]\nAm Ende kommt als Lösung für den nach OLS besten Repräsentanten einer Variablen heraus: \\(\\frac{\\sum_i{x_i}}{n} = a\\) @ref(eq:Umstellen2). Der linke Teil ist genau die Definition von \\(\\overline{x}\\), also dem Mittelwert. Damit haben wir mit einer Ableitungen der OLS herausgefunden, dass der Mittelwert die kleinste Summe der quadrierten Abstände jedes Wertes zu einem Wert a hat, also der gesuchte beste Repräsentant für eine Variable der Wert \\(a=\\overline{x}\\) ist @ref(eq:Mittelwert-Optimum). Dasselbe könnten wir für die Formel \\(Y_i = b_1 + b_2X_i + e_i\\) machen. Wenn wir (mit ein paar Annahmen) das für jedes \\(b_1\\) bis \\(b_3\\) machen würden, dann hätten wir die b’s mit OLS bestimmt. Da das ungleich komplizierter ist als für den Mittelwert, schlage ich vor, wir lassen das an dieser Stelle.\n\n\n\n\n\n\nIYI (nur für Formelliebende): Ableitung der OLS-Funktion\n\n\n\n\n\nAusgangspunkt für die Ableitung der OLS-Funktion ist die Idee, den vom Modell nicht erklärten Rest, also die Residuen (\\(e_i\\)) zu minimieren. Die Residuen sind wie folgt definiert:\n\\[\ne_i=Y_i-\\hat{Y}_i=Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\n\\tag{2.1}\\]\nDie Residuen werden minimmiert, wenn die Summe der quadrierten Residuen (auch Error) minimiert werden, also die Summe \\(sum_{i=1}^n e_i^2\\) möglichst klein ist. Für die Summe der quadrierten Fehler (Sum of Squared Errors: SSE) können wir schreiben:\n\\[\n\\sum_{i=1}^n e_i^2=\\sum_{i=1}^n\\left(Y_i-\\hat{Y}_i\\right)^2=\\sum_{i=1}^n\\left(Y_i-b_1-b_2 X_2-b_3 X_{i 3}\\right)^2\n\\tag{2.2}\\]\nWenn wir die Summe der quadrierten Fehler (SSE) minimieren wollen, leiten wir die SSE nach den gesuchten b ab (das \\(\\partial\\) steht für differenzieren, also ableiten; das \\(\\partial b\\) unter dem Bruchstrich bedeutet, dass nach b abgeleitet wird und nicht etwa, dass irgendwie durch b geteilt wird): \\[\n\\frac{\\partial S S E}{\\partial b}=\\frac{\\partial\\left(\\sum_{i=1}^n e_i^2\\right)}{\\partial b}=\\frac{\\partial\\left(e_1^2\\right)}{\\partial b}+\\frac{\\partial\\left(e_2^2\\right)}{\\partial b}+\\cdots+\\frac{\\partial\\left(e_i^2\\right)}{\\partial b}=\\sum_{i=1}^n \\frac{\\partial\\left(e_i^2\\right)}{\\partial b}\n\\tag{2.3}\\]\nNach den Ableitungsregeln kann man die Ableitung einer Summe zerlegen in die Ableitung der einzelnen Summanden. Das steht in Gleichung 2.3. Nach den ableitungsregeln kann man daraus Folgendes machen (schauen Sie nur darauf, wonach jeweils abgeleitet wird. Alle anderen Teile fallen weg. In Gleichung 2.1 wird zB nach \\(b_1\\) abgeleitet, und die Ableitung einer Konstanten (\\(b_1\\)) ist 1 und mit dem Minuszeichen davor, bleibt eben -1 übrig. In Gleichung 2.3 wird nach \\(b_2\\) abgeleitet. Darum bleibt aus der Formel \\(b_2X_{i 3}\\) übrig, was nach Ableitungsregeln \\(X_{i 2}\\) entspricht und wieder mit einem Minuszeichen aus der Formel versehen ist.):\n\\[\n\\frac{\\partial\\left(e_i^2\\right)}{\\partial b}=\\frac{\\partial\\left(e_i^2\\right)}{\\partial e_i} \\frac{\\partial e_i}{\\partial b}=2 e_i \\frac{\\partial e_i}{\\partial b}\n\\tag{2.4}\\]\nund nach Gleichung Gleichung 2.1, \\[\n\\begin{gathered}\n\\frac{\\partial e_i}{\\partial b_1}=\\frac{\\partial\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\\right)}{\\partial b_1}=-1,\n\\end{gathered}\n\\tag{2.5}\\]\n\\[\n\\begin{gathered}\n\\frac{\\partial e_i}{\\partial b_2}=\\frac{\\partial\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\\right)}{\\partial b_2}=-X_{i 2},\n\\end{gathered}\n\\tag{2.6}\\]\n\\[\n\\begin{gathered}\n\\frac{\\partial e_i}{\\partial b_3}=\\frac{\\partial\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\\right)}{\\partial b_3}=-X_{i 3} .\n\\end{gathered}\n\\tag{2.7}\\]\nJetzt müssen alle Formeln von Gleichung 2.5 bis Gleichung 2.7 zusammengefügt und die einzelnen Ableitungen gleich 0 gesetzt werden, um die Gesamtfunktion zu minimieren: \\[\n\\begin{aligned}\n\\frac{\\partial S S E}{\\partial b_1} & =2 \\sum_{i=1}^n e_i \\frac{\\partial e_i}{\\partial b_1}=2 \\sum_{i=1}^n\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\\right)(-1) \\\\\n& =-2 \\sum_i Y_i+2 \\sum_i b_1+2 b_2 \\sum_i X_{i 2}+2 b_3 \\sum_i X_{i 3}\n\\end{aligned}\n\\]\ndafür können wir schreiben: \\[\n-\\sum_i Y_i+n b_1+b_2 \\sum_i X_{i 2}+b_3 \\sum_i X_{13}=0\n\\tag{2.8}\\]\nJetzt wollen wir die SSE noch für bzw. nach \\(b_2\\) ableiten: \\[\n\\frac{\\partial S S E}{\\partial b_2}=2 \\sum_{i=1}^n e_i \\frac{\\partial e_i}{\\partial b_2}=2 \\sum_{i=1}^n\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\\right)\\left(-X_{i 2}\\right)\n\\]\noder nach der Zerlegung der Summe in die einzelnen Summanden, die jeweils mit \\(-X_{i 2}\\) multipliziert wird und sich darum immer das Vorzeichen umkehrt.\n\\[\n-\\sum_i Y_i X_{i 2}+b_1 \\sum_i X_{i 2}+b_2 \\sum_i X_{i 2}^2+b_3 \\sum_i X_{i 3} X_{i 2}=0\n\\tag{2.9}\\]\nNun fehlt nur noch die Ableitung der SSE nach \\(b_2\\): \\[\n\\frac{\\partial S S E}{\\partial b_3}=2 \\sum_{i=1}^n e_i \\frac{\\partial e_i}{\\partial b_3}=2 \\sum_{i=1}^n\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\\right)\\left(-X_{i 3}\\right)\n\\]\nund wie bei \\(b_2\\): \\[-\n\\begin{gathered}\n-\\sum_i Y_i X_{i 3}+b_1 \\sum_i X_{i 3}+b_2 \\sum_i X_{i 2} X_{i 3}+b_3 \\sum_i X_{i 3}^2=0 .\n\\end{gathered}\n\\tag{2.10}\\]\nJetzt teilen wir jeweils die Gleichung 2.8 bis Gleichung 2.10 durch die Fallzahl, also \\(n\\), woraus sich ergibt (etwas konventionaller geschrieben und erstmal übersichtlicher):\n\\[\n\\begin{aligned}\nb_1+a_1 b_2+a_2 b_3 & =c_1, \\\\\na_1 b_1+a_3 b_2+a_4 b_3 & =c_2, \\\\\na_2 b_1+a_4 b_2+a_3 b_3 & =c_3,\n\\end{aligned}\n\\]\nwobei sich hinter den a’s und c’s folgende Elemente verbergen, die am Ende eigentlich immer recht einfach (\\(\\bar{X}_2\\) und so) ausfallen:\n\\[\n\\begin{gathered}\na_1=\\frac{1}{n} \\sum X_{i 2}=\\bar{X}_2, \\quad a_2=\\frac{1}{n} \\sum X_{i 3}=\\bar{X}_3, \\quad a_3=\\frac{1}{n} \\sum X_{i 2}^2, \\\\\na_4=\\frac{1}{n} \\sum X_{i 2} X_{i 3}, \\quad a_5=\\frac{1}{n} \\sum X_{i 3}^2, \\\\\nc_1=\\frac{1}{n} \\sum Y_i=\\bar{Y}, \\quad c_2=\\frac{1}{n} \\sum Y_i X_{i 2}, \\quad c_3=\\frac{1}{n} \\sum Y_i X_{i 3} .\n\\end{gathered}\n\\tag{2.11}\\]\nDurch Einsetzten erhalten wir also: \\[\n\\bar{Y}=b_1+b_2 \\bar{X}_2+b_3 \\bar{X}_3 \\quad \\text { umgestellt } \\quad b_1=\\bar{Y}-b_2 \\bar{X}_2-b_3 \\bar{X}_3\n\\tag{2.12}\\]\nund Gleichung 2.9 sowie Gleichung 2.10 sind\n\\[\n\\begin{aligned}\n& \\bar{X}_2 b_1+\\left(\\frac{1}{n} \\sum X_{i 2}^2\\right) b_2+\\left(\\frac{1}{n} \\sum X_{i 2} X_{i 3}\\right) b_3=\\frac{1}{n} \\sum Y_i X_{i 2} \\\\\n& \\bar{X}_3 b_1+\\left(\\frac{1}{n} \\sum X_{i 2} X_{i 3}\\right) b_2+\\left(\\frac{1}{n} \\sum X_{i 3}^2\\right) b_3=\\frac{1}{n} \\sum Y_i X_{i 3} .\n\\end{aligned}\n\\tag{2.13}\\]\nWenn man jetzt das \\(b_1\\) aus Gleichung 2.12 einsetzt, ergibt sich \\[\n\\begin{aligned}\n& b_2\\left(\\frac{1}{n} \\sum X_{i 2}^2-\\bar{X}_2^2\\right)+b_3\\left(\\frac{1}{n} \\sum X_{i 2} X_{i 3}-\\bar{X}_2 \\bar{X}_3\\right)=\\left(\\frac{1}{n} \\sum Y_i X_{i 2}-\\bar{Y} \\bar{X}_2\\right) \\\\\n& b_2\\left(\\frac{1}{n} \\sum X_{i 2} X_{i 3}-\\bar{X}_2 \\bar{X}_3\\right)+b_3\\left(\\frac{1}{n} \\sum X_{i 3}^2-\\bar{X}_3^2\\right)=\\left(\\frac{1}{n} \\sum Y_i X_{i 3}-\\bar{Y} \\bar{X}_3\\right)\n\\end{aligned}\n\\tag{2.14}\\]\nDie Varianzen der Variable X ist \\(\\left[V_X=(1 / n) \\sum X_i^2-\\bar{X}^2\\right]\\) und die Kovarianz von \\(X\\) und \\(Y\\) ist \\(\\left[C_{X Y}=(1 / n) \\sum X_1 Y_i-\\bar{X} \\bar{Y}\\right]\\), also kann man für die Gleichung 2.14 etwas übersichtlicher schreiben: \\[\nb_2 V_{X_2}+b_3 C_{X_2 X_3}=C_{Y X_2}, \\quad b_2 C_{X_2 X_3}+b_3 V_{X_3}=C_{Y X_3}\n\\]\nDas ist damit auch das Ergebnis der ganzen Ableitung: Die b’s lassen sich aus den Varianzen und Kovarianzen der Variablen bestimmen!\nUm eine noch übersichtlichere Schreibweise zu bekommen, lassen wir jetzt noch die Subscripte der ganzen X weg. Also schreiben wir ddie Varianzt von \\(X_2\\) nicht mehr als \\(V_{X_2}\\), sondern einfach als \\(V_2\\) und die Kovarianz zwischen \\(X_2\\) und \\(X_3\\) statt \\(C_{X_2 X_3}\\) als \\(C_{2 3}\\). Dann vereinfacht sich das Ganze für \\(b_2\\) zu:\n\\[\n\\begin{aligned}\nb_2=\\left(V_3 C_{Y 2}-C_{23} C_{Y 3}\\right) /\\left(V_2 V_3-C_{23}^2\\right) .\n\\end{aligned}\n\\tag{2.15}\\]\nund für \\(b_3\\):\n\\[\n\\begin{aligned}\n& b_3=\\left(V_2 C_{Y 3}-C_{23} C_{Y 2}\\right) /\\left(V_2 V_3-C_{23}^2\\right) .\n\\end{aligned}\n\\tag{2.16}\\]\nUnd weil die Korrelelation \\(r_{Y 2} = C_{Y 2} / S_2S_Y\\) ist und die Varianz \\(V = S^2\\), kann man für die Gleichung 2.15 kann man, statt der Covarianzen und Varianzen, Korrelationen schreiben:\n\\[\nb_2 = \\frac{\\left(V_3 C_{Y 2}-C_{23} C_{Y 3}\\right)}{\\left(V_2 V_3-C_{23}^2\\right)}=\\frac{r_{Y 2}-r_{23} r_{Y 3}}{\\left(1-r_{23}^2\\right)} \\frac{S_Y}{S_2} .\n\\tag{2.17}\\] (Wer Lust hat, zeigt, dass das die Gleichung 2.17 stimmt.)\n\n\n\n\nIch habe Ihnen eine Excel-Datei gebaut, mit der Sie sich das Prinzip von OLS interaktiv anschauen können:\n\n\n\nOLS-xlsx\n\n\n\n\nWelche Funktion und Eigenschaften hat OLS\n\n\n\n\n Mit OLS kann die Grösse der b's bestimmt werden. OLS ist voraussetzungslos Es gibt noch andere Optimierungsverfahren zur Bestimmung von b's.  OLS ist das englische Akronym für Only Linear Systems\n\n\n\n\n2.2.3 B’s\nWenn wir mit Hilfe der OLS-Methode eine Formel für die b’s gesucht haben, kommt folgende Formel @ref(eq:FormelFuerBs) für das \\(b_2\\) der Variable \\(x_2\\) heraus :\n\\[\\begin{align}\nb_2 = \\frac{r_{y2}-r_{23}r_{y3}}{(1-r_{23}^2)}\\frac{s_y}{s_2} \\label{eq:FormelFuerBs}\n\\end{align}\\]\nDie Formel hat es in sich. Aber schauen Sie sich die Formel mal ganz in Ruhe und stückchenweise an. Als eines der ersten Elemente taucht \\(r_{y2}\\) auf, was so viel heisst, wie die einfache Korrelation zwischen y und der ersten x-Variable, die ja das \\(b_2\\) hat und darum kurz und knapp nur noch mit dem Subscript 2 bedacht wird. Also hängt das b mit der Korrelation zwischen der zugehörigen x-Variable und y zusammen. Da b skalenabhängig ist und r nicht, steht hinten noch dieses \\(\\frac{S_y}{S_2}\\). Dieser Termin sorgt nur dafür, dass b in der Skala von y angegeben ist (darum auch multipliziert mit \\(s_y\\)) – den Teil können Sie schon mal vergessen. Interessanter ist der zweite Teil der Gleichung über dem Bruchstrich: Wir ziehen da das Produkt aus \\(r_{23}\\) und \\(r_{y3}\\) ab. Das heisst, wir gehen von der bivariaten Korrelation aus, rechnen jetzt aber noch die Korrelation raus, die die beiden unabhängigen Variablen \\(x_2\\) und \\(x_3\\) untereinander haben. Wir ziehen allerdings nicht einfach \\(r_{23}\\) ab, sondern multiplizieren das auch noch mit \\(r_{y3}\\). Das bedeutet, wir haben einen Zusammenhang \\(r_{y2}\\) und rechnen aus dem den Anteil gemeinsamer Varianz, also der Zusammenhänge der Varialbe \\(x_2\\) heraus, die diese mit \\(x_3\\), wobei wir nur so viel rausrechnen, wie die dritte Variable \\(x_3\\) wiederum mit y gemeinsam hat. Wären die beiden Variablen \\(x_2\\) und \\(x_3\\) unkorrelliert, dann wäre auch das Produkt \\(r_{23}r_{y3} = 0\\), weil \\(0 \\cdot r_{y3} = 0\\). Wenn \\(x_2\\) und \\(x_3\\) korrellieren, aber \\(x_3\\) und y nicht, dann würden wir auch nichts von \\(r_{y2}\\) abziehen. Im Storchenbeispiel würden wir also sagen, wir sehen den Zusammenhang zwischen Geburtenrate und Anzahl Störche. Wir müssen aber aus dieser Korrelation herausrechnen, dass die Drittvariable (\\(x_3\\)) Bevölkerungsdichte (Stadt vs. Land) stark mit der Geburtenrate korrelliert und mit der Anzahl der Störche, die in einer Region leben.\n\n\n2.2.4 Das Bestimttheitsmass \\(R^2\\)\nDas Bestimmtheitsmass gibt an, wie gut die Werte der AV durch die Werte der UV vorhergesagt werden können.\nWie viel von der Varianz der AV durch ein Modell aufgeklärt werden kann, stellt man fest, indem zunächst die Summe der quadrierten Abweichungen (Sum of Squares) für alle \\(Y_i\\) Werte gezählt werden. Also die totale Varianz der AV, die geschrieben wird als \\(SS_T\\) (Sum of Squares Total). Jetzt ist die Frage, wie viel von dieser Sum of Squares Total durch die Sum of Squares des Modells (\\(SS_M\\)) erklärt werden kann. Darum setzen wir diese beiden Summen der Quadrate (wenn man jeweils durch n teilen würde, wären das die Varianzen) ins Verhältnis zueinander und bekommen einen Prozentwert. Also rechnen wir \\(\\frac{SS_M}{SS_T}\\) und bekommen einen Wert zwischen 0 und 1 bzw. 0% und 100% (% heisst ja «von Hundert» bzw. «geteilt durch 100»). Das ist der aufgeklärte Varianzanteil und den nennen wir \\(R^2\\).\n\n\\(SS_T\\): Summe der quadrierten Abweichungen für die AV (Y).\n\\(SS_M\\): Summe der quadrierten Abweichungen des Modells (der Punkte auf der Geraden, bzw. die geschätzten \\(\\hat{Y_i}\\)-Werte).\n\nAlso: \\(R^2 = \\frac{SS_M}{SS_T}\\)\nBei dieser Formel @ref(eq:Varianzaufklaerung) können wir durch n teilen, also über und unter dem Bruch \\(1/n\\) ergänzen und hätten:\n\\[\\begin{align}\nR^2 = \\frac{SS_M/n}{SS_/T} \\label{eq:Varianzaufklaerung}\n\\end{align}\\]\nWas in Worten ausgedrückt bedeutet:\n\\[\\begin{align}\nR^2 = \\frac{\\text{aufgeklärte Varianz}}{\\text{Gesamtvarianz}} \\label{eq:Varianzaufklaerung-verbal}\n\\end{align}\\]\n\n\n\n\n\nR-Quadrat\n\n\n\n\nMit dem Bestimmtheitsmass können wir angeben, wie gut ein Modell insgesamt ist. Wir werden später noch diskutieren, wie sinnvoll das ist. Spoiler: Nicht immer sehr sinnvoll, weil \\(R^2\\) eigentlich mehr eine Stichprobeneigenschaft ist und wenig über die Welt sagt und recht einfach hochgeschraubt werden kann, indem man triviale und langweilige Variablen in ein Modell einbaut.\n\n\n\n\n\n\nIYI: Ableitung für \\(R^2\\)\n\n\n\n\n\n\\[\ns^2=\\frac{1}{n-3} \\sum\\left(e_i-\\bar{e}\\right)^2=\\frac{1}{n-3} \\sum e_i^2 .\n\\] ar \\(\\bar{e}=0\\).) The denominator \\(n-3\\) reflects the fac and \\(b_3\\) ) have been estimated; in the general cs tions minus the number of parameters estims alue of \\(\\sum e_i^2, s^2\\) can be shown to be an unbias\n\\(s_{b_2}^2=\\frac{s^2}{n} \\frac{1 / V_2}{1-r_{23}^2} \\quad\\) and \\(\\quad s_{b_3}^2=\\frac{s^2}{n} \\frac{1 / V_3}{1-r_{23}^2}\\) where \\(s_{b_2}^2\\) and \\(s_{b_3}^2\\) denote our estimates of the coefficients’ variance. Since \\(E\\left(s^2\\right)=\\sigma^2\\), and the \\(X^{\\prime}\\) ‹s are fixed, this gives an unbiased estimate of the coefficients› variance. The estimated standard errors of the coefficients, symbolized as \\(s_{b_2}\\) and \\(s_{b_3}\\), are simply the square root of these expressions. The information about the error variance-as estimated from the sum of squared residuals-supplies us with a description of the probability function that generated the errors in the true model. However, there are two transformations of \\(s^2\\) that produce more easily interpreted statistics.\nThe first transformation is simply the square root of \\(s^2\\). This statistic \\(s\\) is important enough to deserve its own name: the standard error of estimate. ’In the first place, \\(s\\) is measured in the same units as \\(Y\\) (whereas \\(s^2\\) is in the units of \\(Y\\) squared). The standard error of estimate is also useful because it gives some feel for the size of the dispersion when compared to tables for the normal distribution. For a normal distribution, we expect about \\(95 \\%\\) of all values to lie within plus or minus two standard deviations of the mean. Thus, if the \\(U_i\\) are normally distributed (as we often assume), we can expect \\(95 \\%\\) of all actual values of \\(Y_i\\) in our sample to be within plus or minus two standard errors of estimate away from the estimated line. \\({ }^8\\) The estimate \\(s\\) thus allows an easier interpretation of the magnitude of the error terms.\nThe second transformation of the error variance makes a comparison with the total amount of variance in behavior \\(\\operatorname{var}(Y)\\) existing in the sample. One may wish to compare how well an estimated model does when matched with a «naïve» guess at the behavior in question.\nWith no information about the underlying behavioral relationships, one guess of the value of any \\(Y_i\\) is the mean value of all the observed \\(Y^{\\prime}\\) s. The variance of \\(Y\\) is then the sum of squared deviations around this guess divided by the number of observations on the behavior. The sum of squared residuals \\(\\left(\\sum e_i^2\\right)\\) is a measure of how far our «sophisticated» guesses, represented by \\(\\hat{Y}_i=b_1+b_2 X_{i 2}+b_3 X_{i 3}\\), diverge from the actual values of \\(Y_i\\). Thus, comparing the residual variance \\(\\left(\\sum e_i^2 / n\\right)\\) to the variance of \\(Y\\) gives some indication of the overall performance of our model relative to the simpler «model.» If we look at \\(\\Sigma e_i^2 / \\Sigma\\left(Y_i-\\bar{Y}\\right)^2\\), we see that this can range from zero when \\(\\Sigma e_i^2=0\\) to\na maximum value of one. (Why, according to the least squares procedure, can this ratio never exceed one?) By convention, we create a new statistic, defined as, \\[\nR^2=1-\\left[\\sum e_i^2 / \\sum\\left(Y_i-\\bar{Y}\\right)^2\\right] .\n\\] This statistic, referred to simply as \\(R\\)-squared (or as the coefficient of determination) has the following properties: (1) when all points fall on the estimated plane so that \\(Y_i \\equiv \\hat{Y}_i, R^2\\) equals one (its maximum); (2) when the mean does as well at predicting \\(Y_i\\) as the estimated equation, \\(R^2\\) equals zero (its minimum); (3) between these two extremes, \\(R^2\\) gives an ordinal measure of how well the model predicts the sample values of \\(Y\\).\nAnother way to look at \\(R^2\\) is found by transforming \\(\\Sigma\\left(Y_i-\\bar{Y}\\right)^2\\) as follows: \\[\n\\begin{aligned}\n\\sum\\left(Y_i-\\bar{Y}\\right)^2 & =\\sum\\left[\\left(Y_i-\\hat{Y}_i\\right)+\\left(\\hat{Y}_i-\\bar{Y}\\right)\\right]^2 \\\\\n& =\\sum\\left(Y_i-\\hat{Y}_i\\right)^2+\\sum\\left(\\hat{Y}_i-\\bar{Y}\\right)^2+2 \\sum\\left(Y_i-\\hat{Y}_i\\right)\\left(\\hat{Y}_i-\\bar{Y}\\right) .\n\\end{aligned}\n\\] Since \\(Y_i-\\hat{Y}_i=e_i\\) and \\(\\hat{Y}_i-\\bar{Y}=b_2\\left(X_{i 2}-\\bar{X}_2\\right)+b_3\\left(X_{i 3}-\\bar{X}_3\\right)\\), the last summation is simply equal to \\(2 b_2 \\Sigma\\left(X_{i 2}-\\bar{X}_2\\right) e_i+2 b_3 \\Sigma\\left(X_{i 3}-\\bar{X}_3\\right) e_i\\). However, by the arithmetic properties of least squares demonstrated above, this is identically zero. Thus \\[\n\\sum\\left(Y_i-\\bar{Y}\\right)^2=\\sum\\left(Y_i-\\hat{Y}\\right)^2+\\sum\\left(\\hat{Y}_i-\\bar{Y}\\right)^2=\\sum e_i^2+\\sum\\left(\\hat{Y}_i-\\bar{Y}\\right)^2\n\\] This says that the variance of \\(Y\\) can be divided into two components. The first \\(\\left(\\Sigma e_i^2\\right)\\) is the «unexplained» portion or the residual portion of the model. The second, called the «explained» portion, indicates how much better the estimated model does than using a fixed estimate of the mean would do. By rearranging (3.12), we see that \\[\nR^2=\\sum\\left(\\hat{Y}_i-\\bar{Y}\\right)^2 / \\sum\\left(Y_i-\\bar{Y}\\right)^2\n\\] Thus, \\(R^2\\) can be interpreted as the proportion of the variation in the sample \\(Y_i\\) explained by the regression equation.\nFinally, if one correlates the actual and predicted values of \\(Y\\), one arrives at the correlation coefficient \\(R\\). Squaring this yields the same value of \\(R^2\\) as found in (3.12) and (3.14).\n\n\n\n\nWas wissen Sie über das Bestimmtheitsmass \\(R^2\\)?\n\n\n\n\n \\(R^2\\) ist die Gesamtvarianz geteilt durch die Modellvarianz. \\(R^2\\) ist ein Zusammenhangsmass und wird für jede UV angegeben. \\(R^2\\) ist ein Mass für die Modellgüte. \\(R^2\\) liegt immer zwischen 0 und 1\n\n\n\n\n2.2.5 Kennwerte der Regression\n\n\n\n\n\nKennwerte auf Modellgüte\n\n\n\n\n\n\n\n\n\nKennwerte auf Variablenebene\n\n\n\n\n\n\n\n\n\nKennwerte der Signifikanz der b’s\n\n\n\n\n\n\n\n\n\nKennwerte der Multikollinearität"
  },
  {
    "objectID": "02_GLM.html#vorraussetzung-für-blue",
    "href": "02_GLM.html#vorraussetzung-für-blue",
    "title": "2  Das lineare Modell",
    "section": "2.3 Vorraussetzung für BLUE",
    "text": "2.3 Vorraussetzung für BLUE\nDamit unsere b’s aus der OLS die besten linearen unverzerrten Schätzer (BLUE:Best Linear Unbiased Estimator) für die \\(\\beta\\)s sind, müssen ein paar Voraussetzungen erfüllt sein. Diese Voraussetzungen gucken wir uns in diesem Kapitel an. Zusammengefasst sind es:\nV1. Die UVs und die AV dürfen keine Konstanten sein.\nV2. Das Skalenniveau der UVs muss metrisch oder dichotom (0/1) sein.\nV3. Die Werte der X müssen fix sein.\nV4. Das Modell muss voll spezifiziert sein. D.h.: Keine Korrelation mit externen Variablen.\nV5. Es darf keine perfekte oder heftige Multikollinearität geben.\nV6. Die Residuen müssen bei jedem Wert jeder UV gleich streuen (Homoskedastizität).\nV7. Die Residuen müssen grob normalverteilt sein.\nV8. Die Residuen dürfen nicht autokorreliert sein.\n\nWas verbirgt sich hinter demm Akronym BLUE (ausgeschrieben)?\n\n\nLösung anschauen\n\nBest Linear Unbiased Estimator\n\n\n\n2.3.1 Variablenskalierung (V1.-V2.)\nDie beiden ersten Voraussetzungen (V1. und V2.) betreffen die Skalierung der Variablen.\n\n2.3.1.1 Variablen dürfen keine Konstanten sein (V1.)\nDie UVs und die AV dürfen keine Konstante sein. Das ist insofern recht trivial, als dass eine Konstante mit nichts kovariieren kann, weil Konstanten nicht variieren. Je grösser «\\(\\pi\\), desto \\(...\\)» macht einfach keinen Sinn. Da Konstanten nicht variieren (keine Varianz haben), können sie nicht kovariieren und können daher in keinen Erklärungsmodellen als Variablen einbezogen werden. An dieser Stelle klingt das sehr trivial. Und doch kommt es immer wieder vor, dass in Hypothesen Variablen einfliessen, die in der gewählten Stichprobe konstant sind. Zum Beispiel ist in der Hypothese «Wenn über Sport berichtet wird, zählen Superlative besonders.» Das Konstrukt «über Sport berichtet» ist eine Konstante, wenn nur der Sportteil untersucht werden soll. Hypothesen sind keine Annahmen über Zusammenhänge mehr, wenn eines der Konstrukte, die in Hypothesen zusammengebracht werden, in den Daten eine Konstante ist. Oftmals kommen solche Hypothesen mit Konstanten zustande, wenn der Fokus auf eine Ausprägung einer Variablen gelegt wird und die Abweichung von dieser Ausprägung nicht erhoben wird. Annahmen über den Wandel von Kriegsberichterstattung kann als zeitlicher Prozess nicht untersucht werden, wenn nur das Heute untersucht wird. Oft genug kommen Konstanten in Hypothesen vor, wenn das Forschungsinteresse aus dem Interesse der Forschenden eigentlich deskriptiv ist, also nur die Verteilung von einzelnen Variablen gefragt ist, und dann posthoc Hypothesen formuliert werden sollen, weil das von den Dozierenden oder Reviewern verlangt bzw. erwartet wird. ;-)\n\n\n2.3.1.2 Variablen sollen metrisch sein (V2.)\nDie AV und die UVs sollen metrisch sein. Das klingt nach einer recht harten Voraussetzung. Allerdings gibt es die schöne Eigenschaft von Dummyvariablen (0/1), dass sie sich verhalten wie metrische Variablen, weil ihr Mittelwert und ihre Streuung sinnvoll interpretierbar sind. Dummyvariablen können also gut als UVs eingesetzt werden. Nun ist diese spezielle Form der dichotomen Variable (zwei Ausprägungen) nur die eine Form der nominalen Variablen. Dichotome Variablen können immer als Dummyvariable dargestellt werden. Man muss ja nur eine Ausprägung in 0 umkodieren und die andere in 1. Bei den kategorialen Variablen gibt es mehr Ausprägungen. Zum Beispiel Gender mit 1 = weiblich, 2 = männlich, 3 = divers1. Das Gute wiederum ist, dass kategoriale Variablen vollständig mit Dummyvariablen abgebildet werden können. Das geht dann so: Man baut eine Variable «Weiblich», die die Ausprägungen 1 = «trifft zu» und 0 = «trifft nicht zu» hat. Dann gibt es eine zweite Variable für «männlich» mit 0 und 1 und auch eine Dummy für «Divers». Diesem Vorgehen sind eigentlich keine Grenzen gesetzt. Man könnte also auch noch erweitern oder differenzieren in «transgender», «genderqueer», «genderfluid», «bigender», «pangender», «trigender», «agender», «demigender», «abinär» und zur Sicherheit in Deutschland auch «Taucher»2.\nIn den linearen Modellen können Sie also auch kategoriale Variablen einbauen3. Auch die AV kann eine Dummyvariable sein. Das führt allerdings zu ein paar Problemen mit dem einfachen linearen Modell. Deshalb werden bei einer AV mit nur den Ausprägungen 0 und 1 logistische Regressionen gerechnet. Damit befassen wir uns später. Es geht auch, dass die AV kategorial ist. Das ist dann so ähnlich wie mit den Dummys als UV, weil dann mehrere Regressionen mit mehreren Dummys für die AV gerechnet werden. Das wird multinominale Regression genannt (auch bekannt als Diskriminanzanalyse).\nDann bleiben im Grunde nur die ordinalen Variablen übrig, die mehr Informationen über Ordnung der Ausprägungen (Rangordnung) enthalten, aber die Zahlenwerte (numerisches Relativ) mit ihren identischen Abständen (1 zu 2 wie 2 zu 3 und 3 zu 4 usw.) nicht abbilden, dass die Abstände der gemessenen Ausrägungen (empirisches Relativ) nicht annähernd gleich sind (1 = «arm», zwei = «reich», 3 gleich «superreich»). Dafür gibt es drei Lösungen, um ordinale Variablen auch in lineare Modelle einbeziehen zu können.\n\nOrdinale Variablen werden als metrisch oder quasimetrisch behandelt und wie metrische in ein Modell aufgenommen. Das geschieht praktisch häufig, wenn z.B. Schulnoten einfach in ein lineares Modell aufgenommen werden. Wir wissen, dass die Abstände zwischen der Schweizer Bestnote 6.0 und 5.5 nicht genauso gross sind, wie zwischen 5.5 und 5.0 oder gar 4.0 und 3.5. Dennoch sind die Schätzer der linearen Modelle relativ robust gegen diese Verletzung. Gerade wenn es eigentlich nur darum geht, zu prüfen, ob Schulnoten einen signifikanten Effekt auf eine AV haben, dann kann man diese ordinalen Variablen getrost als «quasimetrisch» verwenden. In diesen Fällen sollte man nur etwas vorsichtiger sein, wenn eine Signifikanzschwelle nur knapp gerissen wurde oder b als Effekt nur knapp die Schwelle der Interpretierbarkeit übersprungen hat, dann sollte man bescheiden sein und klar machen, dass aufgrund der Datenlage und dem Skalennivau der Variablen die Zahlen nicht überinterpretiert werden sollten.\nEs gibt auch die Möglichkeit, ordinale Variablen als kategoriale Variablen zu behandeln (womit ihr Datenniveau aber eigentlich herabgestuft wird). Dann würden wir die Ausprägungen der ordinalen UVs wiederum in Dummyvariablen umkodieren und nur die Dummys interpretieren. Im besten Fall werden in solche Interpretationen die zugrundeliegende Rangfolge der Dummys berücksichtigt, also die erste Gruppe mit der zweiten, die zweite mit der Dritten und dann die erste mit der Dritten, aber mit Rücksicht auf die Bedeutung der Rangfolge.\nWenn eine oder mehrere UVs klar ordinal sind, also die Abstände zwischen den Zahlenwerte deutlich auseinandergehen oder vielleicht sogar variieren (Laufwettkampf mit mal sehr knappen Unterschieden und mal sehr grossen von Platz eins zu Platz zwei, wenn Kipchoge mitläuft), dann sollten die ordinalen nicht einfach als metrische betrachtet werden. Wenn solche ordinalen Variablen zentral sind, dann kann auch nicht einfach auf Dummys ausgewichen werden. Dafür gibt es aber inzwischen Analysemethoden der ordinalen Regression, die in diesen Fällen eingesetzt werden können. Mit dem Verständnis der normalen linearen Modelle ist es nicht mehr schwer, sich so gut selbständig in die ordinale Regression einzuarbeiten, dass sie gewinnbringend eingesetzt werden kann.\n\n\n\n\n2.3.2 Modellspezifikation und Multikollinearität (V3.-V5.)\n\n2.3.2.1 V3. Fixe X\nDass die UVs fix sein sollen, bedeutet im Grunde nur, dass sich die UVs nicht ständig ändern sollen, sondern in unserer Auswahlgesamtheit stabil sind. Wenn sich zum Beispiel die Berichterstattung insgesamt häufig stark ändert, dann wäre es nicht gut, wenn wir mit der Stichprobe einer Inhaltsanalyse arbeiten, die in einer sehr speziellen Zeit erhoben wurde (z.B. ein Kriegsanfang). Diese Stichprobe in einer Spezialzeit würde zu verzerrt geschätzten B’s in der Normalzeit führen [vgl. @Wolling2015]. Da wir nicht davon ausgehen können und wollen, dass unsere Theorien in der Sozialwissenschaft immer und ewig gelten, verlangen wir nur mittelfristig gültige Theorien («middle range theory» [@Merton2012]) und dass unsere Variablen mittelfristig relativ stabil bzw. fix sind. Das bedeutet insbesondere, dass wir bei der Stichprobenziehung aufpassen müssen, dass wir nicht eine sehr spezielle Stichprobe in einer ganz besonderen Phase erheben, die Effekte hat, die sonst sehr untypisch sind. Das ist das, was mit fixe X gemeint ist.\n\n\n\n\n\n\nIYI: Fixe X in Formeln abgeleitet\n\n\n\n\n\n\\[\n\\begin{aligned}\nb_2 & =\\frac{V_3 C_{2 Y}-C_{23} C_{3 Y}}{D} \\\\\n& =\\frac{(1 / n) \\sum_{i=1}^n\\left\\{\\left[V_3\\left(X_{i 2}-\\bar{X}_2\\right)-C_{23}\\left(X_{i 3}-\\bar{X}_3\\right)\\right]\\left(Y_i-\\bar{Y}\\right)\\right\\}}{D}\n\\end{aligned}\n\\] where \\(D=V_2 V_3-C_{23}^2\\). From the true model for \\(Y\\) and from averaging the \\(Y_i\\) over the sample, we know that \\[\n\\begin{aligned}\nY_i-\\bar{Y} & =\\beta_1+\\beta_2 X_{i 2}+\\beta_3 X_{i 3}+U_i-\\left(\\beta_1+\\beta_2 \\bar{X}_2+\\beta_3 \\bar{X}_3+\\bar{U}\\right) \\\\\n& =\\beta_2\\left(X_{i 2}-\\bar{X}_2\\right)+\\beta_3\\left(X_{i 3}-\\bar{X}_3\\right)+\\left(U_i-\\bar{U}\\right)\n\\end{aligned}\n\\] where \\(\\bar{U}\\) is the mean of all error terms implicit in the sample. By substitution, we have \\[\n\\begin{aligned}\n& b_2=\\frac{1}{N D}\\left\\{\\sum_{i=1}^n\\left[V_3\\left(X_{i 2}-\\bar{X}_2\\right)-C_{23}\\left(X_{i 3}-\\bar{X}_3\\right)\\right]\\right. \\\\\n&\\left.\\times\\left[\\beta_2\\left(X_{i 2}-\\bar{X}_2\\right)+\\beta_3\\left(X_{i 3}-\\bar{X}_3\\right)+\\left(U_i-\\bar{U}\\right)\\right]\\right\\} \\\\\n&=\\frac{1}{D}\\left\\{\\frac{\\beta_2}{N} V_3 \\sum\\left(X_{i 2}-\\bar{X}_2\\right)^2-\\frac{\\beta_2}{N} C_{23} \\sum\\left(X_{i 3}-\\bar{X}_3\\right)\\left(X_{i 2}-\\bar{X}_2\\right)\\right. \\\\\n&+\\frac{\\beta_3}{N} V_3 \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(X_{i 3}-\\bar{X}_3\\right)-\\frac{\\beta_3}{N} C_{23} \\sum\\left(X_{i 3}-\\bar{X}_3\\right)^2 \\\\\n&\\left.+\\frac{1}{N} \\sum\\left[V_3\\left(X_{i 2}-\\bar{X}_2\\right)-C_{23}\\left(X_{i 3}-\\bar{X}_3\\right)\\right]\\left(U_i-\\bar{U}\\right)\\right\\}\n\\end{aligned}\n\\] The first summation can be written as \\(\\beta_2 V_3(1 / N) \\Sigma\\left(X_{i 2}-\\bar{X}_2\\right)^2=\\beta_2 V_3 V_2\\).\n\\[\n\\begin{aligned}\n& E\\left(C_{2 U}\\right)=\\frac{1}{N} \\sum\\left(X_{i 2}-\\bar{X}_2\\right) E\\left(U_i-\\bar{U}\\right)=\\frac{1}{N} \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(\\bar{U}_i-\\mu\\right)=C_{2 \\bar{U}_i} \\\\\n& E\\left(C_{3 U}\\right)=\\frac{1}{N} \\sum\\left(X_{i 3}-\\bar{X}_3\\right) E\\left(U_i-\\bar{U}\\right)=\\frac{1}{N} \\sum\\left(X_{i 3}-\\bar{X}_3\\right)\\left(\\bar{U}_i-\\mu\\right)=C_3 \\bar{U}_i,\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nb_1 & =\\bar{Y}-b_2 \\bar{X}_2-b_3 \\bar{X}_3=\\beta_1+\\beta_2 \\bar{X}_2+\\beta_3 \\bar{X}_3+\\bar{U}=b_2 \\bar{X}_2-b_3 \\bar{X}_3 \\\\\n& =\\beta_1+\\left(\\beta_2-b_2\\right) \\bar{X}_2+\\left(\\beta_3-b_3\\right) \\bar{X}_3+\\bar{U}_1\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nb_2 & =\\frac{\\beta_2 V_3 V_2-\\beta_2 C_{23}^2+\\beta_3 V_3 C_{23}-\\beta_3 C_{23} V_3}{D}+\\frac{V_3 C_{2 U}-C_{23} C_{3 U}}{D} \\\\\n& =\\frac{\\beta_2\\left(V_2 V_3-C_{23}^2\\right)}{D}+\\frac{V_3 C_{2 U}-C_{23} C_{3 U}}{D}=\\beta_2+\\frac{V_3 C_{2 U}-C_{23} C_{3 U}}{D} .\n\\end{aligned}\n\\] Similar treatment of Eq. (2.16) gives \\[\nb_3=\\beta_3+\\frac{V_2 C_{3 U}-C_{23} C_{2 U}}{D} \\text {. }\n\\]\n\\[\n\\begin{aligned}\n& E\\left(b_2\\right)=E\\left(\\beta_2\\right)+E\\left[\\frac{V_3 C_{2 U}-C_{23} C_{3 U}}{D}\\right] \\\\\n& E\\left(b_3\\right)=E\\left(\\beta_3\\right)+E\\left[\\frac{V_2 C_{3 U}-C_{23} C_{2 U}}{D}\\right] .\n\\end{aligned}\n\\] \\(\\beta_2\\) and \\(\\beta_3\\) are constants, so \\(E\\left(\\beta_2\\right)=\\beta_2\\) and \\(E\\left(\\beta_3\\right)=\\beta_3\\). Since we treat the values for the \\(X\\) ’s as nonstochastic, or as fixed during all replicated experiments, \\(V_2, V_3, C_{23}\\), and \\(D\\) can be treated as constants in taking expected values. Thus \\[\n\\begin{aligned}\n& E\\left(b_2\\right)=\\beta_2+\\frac{V_3 E\\left(C_{2 U}\\right)}{D}-\\frac{C_{23} E\\left(C_{3 U}\\right)}{D}, \\\\\n& E\\left(b_3\\right)=\\beta_3+\\frac{V_2 E\\left(C_{3 U}\\right)}{D}-\\frac{C_{23} E\\left(C_{2 U}\\right)}{D} .\n\\end{aligned}\n\\] Our estimates will be unbiased if \\(E\\left(C_{2 U}\\right)=0=E\\left(C_{3 U}\\right)\\). The development concentrates upon these terms, where \\[\n\\begin{aligned}\n& E\\left(C_{2 U}\\right)=E\\left[\\frac{1}{n} \\sum_i\\left(X_{i 2}-\\bar{X}_2\\right)\\left(U_i-\\bar{U}\\right)\\right] \\\\\n& E\\left(C_{3 U}\\right)=E\\left[\\frac{1}{n} \\sum_i\\left(X_{i 3}-\\bar{X}_3\\right)\\left(U_i-\\bar{U}\\right)\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n2.3.2.2 V4. Voll spezifizierte Modelle\n\n\n\n\n\n\nIYI: Ableitung für \\(E(U_i-\\overline{U}) = 0.\\)\n\n\n\n\n\n\\[\n\\begin{aligned}\nb_1 & =\\bar{Y}-b_2 \\bar{X}_2-b_3 \\bar{X}_3=\\beta_1+\\beta_2 \\bar{X}_2+\\beta_3 \\bar{X}_3+\\bar{U}=b_2 \\bar{X}_2-b_3 \\bar{X}_3 \\\\\n& =\\beta_1+\\left(\\beta_2-b_2\\right) \\bar{X}_2+\\left(\\beta_3-b_3\\right) \\bar{X}_3+\\bar{U}_1\n\\end{aligned}\n\\]\n\\[\nE\\left(b_1\\right)=E\\left(\\beta_1\\right)+E\\left(\\beta_2-b_2\\right) \\bar{X}_2+E\\left(\\beta_3-b_3\\right) \\bar{X}_3+E(\\bar{U}) .\n\\] This expression will equal \\(\\beta_1\\) if two conditions hold. If \\(U_i\\) is distributed independently of \\(X_2\\) and \\(X_3\\), then our estimates \\(b_2\\) and \\(b_3\\) are unbiased so that \\(E\\left(\\beta_2-b_2\\right)=E\\left(\\beta_3-b_3\\right)=0\\). Secondly, the expected value of \\(\\bar{U}\\) must equal zero, \\(E(\\bar{U})=0\\). If both conditions hold, \\(E\\left(b_1\\right)=\\beta_1\\). Thus, for all estimated coefficients to be unbiased, we modify assumption A.3 to be \\(E\\left(U_i\\right)=0\\). Note that this is a stronger assumption than is needed for unbiasedness of the slope coefficients. \\(^3\\) Arithmetic Properties Unfortunately, we cannot test either of these assumptions, \\(E(\\bar{U})=0\\) and \\(\\sum\\left(X_{i k}-\\bar{X}_k\\right)\\left(U_i-\\bar{U}\\right)=0\\), with the observed data. Intuitively, one would test these assumptions by examining the mean of the residuals from the estimated equations and the correlation between these residuals and \\(X\\). Our intuition, however, is wrong in this case. The mean of the residuals is \\[\n\\frac{1}{n} \\sum_{i=1}^n\\left(Y_i-\\hat{Y}_i\\right)=\\frac{1}{n} \\sum\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{3 t}\\right)\n\\] However, this is simply \\(-1 / n\\) times Eq. (2.11) which by construction equals zero. Thus the mean of the estimated residuals must be zero, whether or not the mean of the true errors is zero.\nSince \\(\\bar{e}=0\\), the numerator of the correlation between the residuals and \\(X_2\\) is \\[\n\\begin{aligned}\n\\frac{1}{n} \\sum e_i\\left(X_{i 2}-\\bar{X}_2\\right) & =\\frac{1}{n} \\sum\\left(Y_i-\\hat{Y}_i\\right)\\left(X_{i 2}-\\bar{X}_2\\right) \\\\\n& =\\frac{1}{n} \\sum\\left(Y_i-\\hat{Y}_i\\right) X_{i 2}-\\frac{1}{n} \\sum\\left(Y_i-\\hat{Y}_i\\right) \\bar{X}_2 \\\\\n& =\\frac{1}{n} \\sum\\left(Y_i-b_1-b_2 X_{i 2}-b_3 X_{i 3}\\right)\\left(X_{i 2}\\right)-\\frac{1}{n} \\bar{X}_2 \\sum e_i\n\\end{aligned}\n\\]\n\n\n\nUnsere B’s sind nur dann unverzerrt, wenn das Modell voll spezifiziert ist in Bezug auf Einflüsse, die mit unseren B’s in Wirklichkeit zusammenhängen. Wenn wir vergessen in unsere Überlegungen und Messungen einzubeziehen, dass die Storchenpopulation einer Gegend nur darum mit der Geburtenrate zu tun hat, weil in ländlichen Regionen die Geburtenrate höher ist und mehr Störche leben als in der Stadt; wenn wir also diesen Dritteinfluss vergessen, dann scheint es einen Zusammenhang zwischen Geburtenrate und Storchenpopulation zu geben. Wir würden falsche Schlüsse ziehen, weil der Zusammenhang verzerrt geschätzt würde. Journalistinnen vom Berliner Kurier könnten glauben, dass der Storch die Kinder bringt. Wir müssen also theoretisch erarbeiten, welche Einflüsse von Bedeutung sein könnten für unsere AV oder den Zusammenhang zwischen den UVs und der AV beeinflussen könnten. Das ist Theoriearbeit. Dieser Zusammenhang muss sich auch mathematisch in der Statistik abbilden, was er auch tut.\nWenn wir mal annehmen, dass die wahren Zusammenhänge gut durch die Formel @ref(eq:Spez1) dargestellt wären, aber die Theorie zu dem Thema auf dem Stand ist, dass die einfacheren Zusammenhänge aus der Formel @ref(eq:Spez2) gelten, also eine wichtige Einflussgrösse (\\(X_4\\)) nicht berücksichtigt wurde. Wenn dem so wäre, dann würde das Unbekannte (\\(U_i\\)) in Formel @ref(eq:Spez2) nicht nur den einfachen stochastischen Rest umfassen, sondern zusätzlich \\(\\beta_4X_{i4}\\). Dann wäre der Erwartungswert (also der Wert, um den unsere Stichprobenparameter b streuen) nicht mehr das erhoffte \\(\\beta_2\\) sondern \\(\\beta_2 + \\beta_4b_{42}\\), wie in Formel @ref(eq:Spez3). Das würde zu einem Fehler führen, der bei \\(\\frac{r_{42}-r_{32}r_{43}}{1-r^2_{32}}\\sqrt{\\frac{V_4}{V_2}}\\) liegt. Wenn wir also ewig Stichproben ziehen würden und jedes Mal ein \\(b_2\\) bestimmen würden, dann würden diese \\(b_2\\)s nicht um \\(\\beta_2\\) streuen. Das Mass, um das wir uns verschätzen würden, wäre so gross wie in @ref(eq:Spez4) notiert. Auch unsere Signifikanztests wären falsch und die Konfidenzintervalle würden an der falschen Stelle liegen. Unsere ganze Analyse wäre falsch.\n\\[\\begin{align}\n  \\text{wahr:} Y_i=&\\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i3} + \\beta_4X_{i4}+U_i \\label{eq:Spez1}\\\\\n  \\text{geschätzt: } Y_i=&\\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i3}\n                          +U^\\star_i \\text{\\qquad wobei \\quad } U^\\star_i = \\beta_4X_{i4}+U_i \\label{eq:Spez2}\\\\\n  \\text{also: } E(b_2) =& \\beta_2 + \\beta_4b_{42} \\label{eq:Spez3}\\\\\n  \\text{mit: }\n  b_{42}=&\\frac{r_{42}-r_{32}r_{43}}{1-r^2_{32}}\\sqrt{\\frac{V_4}{V_2}} \\label{eq:Spez4}\n\\end{align}\\]\nWie geht man nun mit dieser Tyrannei um, dass man alle Einflüsse kennen sollte, die schlicht unbekannt sind. Nur Chuck Norris weiss, wann ein Modell voll spezifiziert ist. Wir können nie wissen, wann wir am Ende der Wissenschaft angekommen sind, weil wir alles vollständig und für immer gültig spezifiziert haben. Es geht bei dieser Überlegung der Spezifikation mehr darum, dass wir die Spezifikation der bestehenden Modelle verbessern. Das kann heissen, dass wir falsche Alltagsvorstellungen korrigieren, indem wir den Kindern irgendwann sagen, dass das bivariate Regressionsmodell mit den Störchen und den Kindern, nicht voll spezifiziert ist und Sex, Verhütung und viele mehr einen gewissen Einfluss hat auf die Geburtenrate. Wir klären aber nicht nur in der Alltagswelt auf, sondern verbessern auch unsere Modelle stetig, indem wir uns fragen, welche Einflussgrössen bei der Erklärung eines Phänomens noch eine Rolle spielen könnten.\nDie statistisch, mathematische Anforderung an die Modellspezifikation bedeutet also, dass wir unsere Theorie gut und gründlich entwickeln müssen. Bei einer schlechten Theorie und entsprechend zu wenig erfasster oder einbezogener Modells sind unsere Ergebnisse verzerrt und damit falsch oder mindestens nicht state of the art. Darum muss man immer erst schauen, was der Forschungsstand ist. Der kann repliziert und damit kontrolliert werden, und wenn wir das Modell weiter spezifizieren und neue Ergebnisse erlangen, dann haben wir die Theorie erweitert und einen wissenschaftlichen Mehrwert geschaffen. Es werden auch noch Generationen nach uns und Ihnen kommen, die unsere Theorien überarbeiten und dabei feststellen, dass wir unserer Modelle unterspezifiziert hatten. Das ist dann der wissenschaftliche und zivilisatorische Fortschritt. Wissenschaft wird also nicht irgendwann fertig sein und wichtig bleiben.\n\n\n\n\n\n\nIYI: Ableitung Modellspezifikation\n\n\n\n\n\nthe true model \\(Y_i=\\beta_1+\\beta_2 X_{i 2}+\\beta_3 X_{i 3}+\\beta_4 X_{i 4}+U_i\\). When such a misspecification occurs, the influence attributed to the included variables is actually a combined influence of the included and excluded variables. For example, if all \\(X\\) variables exert a positive influence on \\(Y_i\\) and these variables are themselves positively correlated, the estimated coefficients for the included variables will be overstated and imply that each included variable is more important than it actually is. The mathematics of this case is straightforward. Assume that the true model is \\[\nY_i=\\beta_1+\\beta_2 X_{i 2}+\\beta_3 X_{i 3}+\\beta_4 X_{i 4}+U_i,\n\\] and that instead we estimate \\[\nY_i=\\beta_1+\\beta_2 X_{i 2}+\\beta_3 X_{i 3}+U_i^* \\quad \\text { where } \\quad U_i^*=\\beta_4 X_{i 4}+U_i\n\\] The least squares estimators from Chapter 2 are \\[\n\\begin{aligned}\n& b_2=\\frac{V_3 C_{2 Y}-C_{23} C_{3 Y}}{V_2 V_3-C_{23}^2}=\\beta_2+\\frac{V_3 C_{2 U^*}-C_{23} C_{3 U^*}}{V_2 V_3-C_{23}^2}, \\\\\n& b_3=\\frac{V_2 C_{3 Y}-C_{23} C_{2 Y}}{V_2 V_3-C_{23}^2}=\\beta_3+\\frac{V_2 C_{3 U^*}-C_{23} C_{2 U^*}}{V_2 V_3-C_{23}^2} .\n\\end{aligned}\n\\] Substituting \\(U_i^*=\\beta_4 X_{i 4}+U_i\\) into the covariance expressions involving \\(U^*\\) gives \\[\n\\begin{aligned}\nC_{2 U^*} & =\\frac{1}{n} \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(U_i^*-\\bar{U}^*\\right)=\\frac{1}{n} \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(\\beta_4 X_{i 4}+U_i-\\beta_4 \\bar{X}_4-\\bar{U}\\right) \\\\\n& =\\frac{1}{n} \\beta_4 \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(X_{i 4}-\\bar{X}_4\\right)+\\frac{1}{n} \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(U_i-\\bar{U}\\right) \\\\\n& =\\beta_4 C_{24}+C_{2 U} \\\\\nC_{3 U^*} & =\\beta_4 C_{34}+C_{3 U}\n\\end{aligned}\n\\] Taking the expected value of \\(b_2\\) and \\(b_3\\), assuming fixed \\(X\\) and \\(E\\left(U_i\\right)=0\\), we obtain \\[\n\\begin{aligned}\n& E\\left(b_2\\right)=\\beta_2+\\beta_4\\left(\\frac{V_3 C_{24}-C_{23} C_{34}}{V_2 V_3-C_{23}^2}\\right)+E\\left[\\frac{V_3 C_{2 U}-C_{23} C_{3 U}}{V_2 V_3-C_{23}^2}\\right]=\\beta_2+\\beta_4 b_{42} \\\\\n& E\\left(b_3\\right)=\\beta_3+\\beta_4\\left(\\frac{V_2 C_{34}-C_{23} C_{24}}{V_2 V_3-C_{23}^2}\\right)+E\\left[\\frac{V_2 C_{3 U}-C_{23} C_{2 U}}{V_2 V_3-C_{23}^2}\\right]=\\beta_3+\\beta_4 b_{43}\n\\end{aligned}\n\\] where \\[\nb_{42}=\\frac{\\left(r_{42}-r_{32} r_{43}\\right)}{1-r_{32}^2} \\sqrt{\\frac{V_4}{V_2}} \\quad \\text { and } \\quad b_{43}=\\frac{\\left(r_{43}-r_{32} r_{42}\\right)}{1-r_{32}^2} \\sqrt{\\frac{V_4}{V_3}} \\text {. }\n\\]\n\n\n\n\n\n2.3.2.3 Keine perfekte oder heftige Multikollinearität (V5.)\nWenn perfekte Multikollinearität vorliegt, dann kann eine Variable perfekt aus den übrigen Variablen vorhergesagt werden (technischer: eine UV ist eine Linearkombination der übrigen UVs). Ein lineares Modell gibt dann keine Antwort auf die ihm gestellte Frage, wenn zwei UVs identisch sind, also untrennbar verwoben. Das liegt daran, dass die Frage an das lineare Modell ist: «Wie starkt ist der Effekt jeder einzelnen UV, wenn die Effekte der übrigen UV herausgerechnet werden?». Wenn eine Variable eine Linearkombination der übrigen Variablen ist, dann bleibt von ihr exakt nichts übrig, wenn die Linearkombination der übrigen Variablen aus ihr herausgerechnet werden. Ist ihre Varianz dadurch 0, ist sie im Grunde eine Konstante, und wie in V1. diskutiert, kann mit Konstanten keine Kovarianz und damit auch kein lineares Modell gerechnet werden. Jedes Statistikprogramm würde also an dieser Stelle aussteigen und ihnen sagen, dass das Modell so nicht gerechnet werden kann, weil perfekte Multikollinearität vorliegt. Das muss also nicht extra getestet werden.\nPerfekte Multikollinearität entsteht meistens, wenn eine Variable aus dem Rohdatensatz umkodiert wurde und die Originalvariable und die einfach umkodierte mit im Modell sind. Die schuldige Variable findet man recht schnell. Etwas weniger direkt ersichtlich ist so eine perfekte Multikollinearität durch Datenaufbereitung, wenn ein Index und alle Variablen, aus denen der Index berechnet wurden, mit in das Modell aufgenommen wurden. Wenn Sie also z.B. die Durchschnittsnote im Abi in das Modell packen und alle Noten der einzelnen Fächer auch, die zusammen exakt die Durchschnittsnote ergeben. Suchen Sie in solchen Fällen nach den Indizes. Wenn Sie in dem Beispiel die Durchschnittsnote rausnehmen oder ein paar Fächer, die ihnen für die Erklärung der AV nicht so wichtig erscheinen, dann wird das Problem der perfekten Multikollinearität schnell gelöst sein.\nEtwas Multikollinearität ist allerdings nicht nur erlaubt, sondern der Grund dafür, dass wir multivariate Modelle rechnen. Wären die UVs untereinander alle unkorreliert, dann wären alle B’s dieselben, wenn nur bivariate Regressionen gerechnet werden würden. In der Formel @ref(eq:Bs1) für \\(b_2\\) sieht man das auch sehr gut: Wenn \\(r_{23} = 0\\), also keine Multikollinearität beim Modell mit zwei UVs (\\(X_2\\) und \\(X_3\\)), dann kommt für \\(b_2\\) dasselbe raus, wie ohne \\(X_3\\) (in @ref(eq:Bs1) wird $r_{23} = 0 gesetzt und in @ref(eq:Bs3) sieht man, dass \\(X_3\\) oder \\(r_3\\) keine Rolle spielen).\n\\[\\begin{align}\n   b_2& = \\frac{r_{Y2}-r_{23}r_{Y3}}{(1-r_{23}^2)}\\frac{S_y}{S_2} \\label{eq:Bs1}\\\\\n   b_2& = \\frac{r_{Y2}-0\\cdot r_{Y3}}{(1-0^2)}\\frac{S_y}{S_2} \\label{eq:Bs2}\\\\\n   b_2& = r_{Y2}\\frac{S_y}{S_2} \\label{eq:Bs3}\n\\end{align}\\]\nWenn es etwas Multikollinearität gibt, wird das Produkt aus \\(r_{23}r_{Y3}\\) aus dem bivariaten \\(b_2\\) subtrahiert (herausgerechnet). Zusätzlich wird mit einer Korrektur unter dem Bruchstrich von \\(1-r^2_{23}\\) angepasst. In Worten bedeutet das so viel wie: Wenn wir untersuchen wollen, ob der Storch (UV) die Kinder bringt (AV), aber wissen, dass das auch noch mit Urbanität (\\(X_3\\)) zusammenhängt, dann müssen wir berücksichtigen (herausrechnen) wie stark Urbanität (\\(X_3\\)) und Storchenpopulation (\\(X_2\\)) zusammenhängen (\\(r_23\\)), wenn bzw. in dem Masse, wie auch die Geburtenrate (Y) mit der Urbanität zusammenhängt (\\(r_{Y2}\\)). Das steht über dem Bruch der Formel @ref(eq:Bs1). Da wir nicht mehr mit den vollen 100% der Varianz von \\(X_2\\) rechnen können, wird unter dem Bruchstrich der Formel @ref(eq:Bs1) auch noch herausgerechnet, um wie viel \\(X_2\\) durch \\(X_3\\) beklaut wird (\\(1-r^2_{23}\\)). Über diesen Teil der Formel lohnt es sich, etwas länger nachzudenken.\nToleranz und VIF\nWenn Multikollinearität bedeutet, dass eine Variable durch eine andere stark bestimmt wird, haben wir für die Bestimmtheit einer Variablen durch andere ein Mass: Das Bestimmtheitsmass \\(R^2\\). In der Formel @ref(eq:Bs1) steht unter dem Bruch ein \\(r^2_{23}\\), das man besser auch schreiben könnte als \\(R^2_{2.3}\\), einfach um deutlicher zu machen, dass es um eine multiple Korrelation geht und darum, dass die Regression auf \\(X_2\\) gemeint ist, von allen übrigen Variablen. Wenn es mehr als nur die \\(X_3\\) gibt, würden wir in der Formel für \\(b_2\\) schreiben \\(R^2_{2.34567...}\\) und bei \\(b_3\\) \\(R^2_{3.24567...}\\). Nun ist Multikollinearität nichts Gutes, sondern ein Problem. Darum steht in Formel @ref(eq:Bs1) auch \\(1-r^2_{23}\\). Hier ist also angegeben, wie viel von den 100% Varianz von \\(b_2\\) übrig bleiben, wenn man herausgerechnet hat, wie stark die übrigen UVs die Variable \\(X_2\\) bestimmen (\\(R^2_{2.34567...}\\)). Man könnte auch sagen, dass damit für die Multikollinearität angegeben ist, wie stark ihre Toleranz gegenüber den übrigen Variablen ist. Wenn also zum Beispiel die übrigen Variablen 40% der Variable \\(X_2\\) erklären, dann wäre die Toleranz \\(1-0.4\\), also 60%. Diesen Toleranzwert (TOL) sollte man sich bei jeder Regression mit rausgeben lassen, um zu prüfen, wie stark die einzelnen Variablen von Multikollinearität betroffen sind. In Publikationen sieht man diese Werte oft nicht, weil sie von den Forschenden geprüft und für nicht problematisch befunden wurden (wenn diese Forschenden gründlich arbeiten).\nMultikollinearität hat vor allem auch eine Bedeutung für die Fehlervarianz der B’s, also wie unsicher oder wackelig die b’s sind. Darum steckt in der Formel für die \\(s_{b_2}^2\\) auch das \\(1-R_{23}^2\\) unter dem Bruchstrich des Faktors drin, der hinten steht. Dieser hintere Faktor ist demnach der Faktor, um den die Fehlervarianz der B’s steigt, wenn die Toleranz (\\(1-R_{2.3}^2\\)) klein ist, weil die jeweilige UV stark durch die übrigen Variablen bestimmt wird (\\(R_{2.3}^2\\)). Mit diesem Faktor wird auch gearbeitet, indem in Regressionsanalysen in Outputs häufig der Varianz-Inflations-Faktor (VIF) mit angezeigt wird. Wenn also zum Beispiel die Varianz der Variablen \\(X_2\\) zu 90% durch die übrigen Variablen im Modell aufgeklärt wird, dann ist die Wert TOL nur noch \\(1-.9 = .1\\). Der Variablen \\(X_2\\) würden also nur noch 10% seiner Ursprungsvarianz bleiben, um die AV erklären zu können. Das ist nicht viel, worauf eine stabile Regressionsgerade angepasst werden könnte. Darum wackelt das \\(b_2\\) viel mehr, als wenn die anderen Variablen nicht berücksichtigt worden wären. Die Unsicherheit wurde um den Faktor \\(\\frac{1}{1-R^2_{2.34567...}}\\) inflationiert, also um das Zehnfache! Da muss man sich dann schon fragen, was da eigentlich übrig bleibt.\n\\[\\begin{align}\n  s_{b_2}^2&=\\frac{s^2}{n}\\cdot\\frac{1}{V_2}\\cdot\\frac{1}{1-R_{2.3}^2} \\label{eq:sb1}\\\\\n  s_{b_3}^2&=\\frac{s^2}{n}\\cdot\\frac{1}{V_3}\\cdot\\frac{1}{1-R_{3.2}^2} \\label{eq:sb2}\n\\end{align}\\]\n\n\n\n2.3.3 Homoskedastizität (V6.)\n\n\n\n\n\n\nIYI: Effizienz als Varianz von \\(b_2\\) und \\(b_3\\)\n\n\n\n\n\nAssuming that \\(b_2\\) is unbiased (or that assumption A.3 holds), the variance of our estimator \\(b_2\\) can be found from a rearrangement of Eq. (3.1): \\[\n\\begin{gathered}\nb_2-\\beta_2=\\frac{V_3 C_{2 U}-C_{23} C_{3 U}}{D} \\\\\n\\operatorname{var}\\left(b_2\\right)=E\\left[b_2-E\\left(b_2\\right)\\right]^2=E\\left(b_2-\\beta_2\\right)^2=E\\left(\\frac{V_3 C_{2 U}-C_{23} C_{3 U}}{D}\\right)^2 \\\\\n=\\frac{1}{D^2} E\\left(V_3^2 C_{2 U}^2-2 V_3 C_{23} C_{2 U} C_{3 U}+C_{23}^2 C_{3 U}^2\\right) \\\\\n=\\frac{1}{D^2}\\left[V_3^2 E\\left(C_{2 U}^2\\right)-2 V_3 C_{23} E\\left(C_{2 U} C_{3 U}\\right)+C_{23}^2 E\\left(C_{3 U}^2\\right)\\right]\n\\end{gathered}\n\\] (Note, by assumption A.2, the \\(X\\) values are fixed.) We need to investigate the three expected value terms in detail: \\[\n\\begin{aligned}\n& E\\left(C_{2 U}^2\\right)=E\\left\\{\\frac{1}{n^2}\\left[\\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(U_i-\\bar{U}\\right)\\right]^2\\right\\} \\\\\n&=\\frac{1}{n^2} E {\\left[\\left(X_{12}-\\bar{X}_2\\right)\\left(U_1-\\bar{U}\\right)+\\left(X_{22}-\\bar{X}_2\\right)\\left(U_2-\\bar{U}\\right)\\right.} \\\\\n&\\left.+\\cdots+\\left(X_{n 2}-\\bar{X}_2\\right)\\left(U_T-\\bar{U}\\right)\\right]^2\n\\end{aligned}\n\\] (from expanding the summation) \\[\n\\begin{aligned}\n&=\\frac{1}{n^2} E\\left[\\sum_{i=1}^n\\left(X_{i 2}-\\bar{X}_2\\right)^2\\left(U_i-\\bar{U}\\right)^2\\right. \\\\\n&\\left.+2 \\sum_{i=1}^{n-1} \\sum_{s=t+1}^n\\left(X_{i 2}-\\bar{X}_2\\right)\\left(X_{s 2}-\\bar{X}_2\\right)\\left(U_i-\\bar{U}\\right)\\left(U_s-\\bar{U}\\right)\\right]\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n&=\\frac{1}{n^2}[ \\sum_{i=1}^n\\left(X_{i 2}-\\bar{X}_2\\right)^2 E\\left(U_i-\\bar{U}\\right)^2 \\\\\n&\\left.+2 \\sum_{i=1}^n \\sum_{s=t+1}^n\\left(X_{i 2}-\\bar{X}_2\\right)\\left(X_{s 2}-\\bar{X}_2\\right) E\\left(U_i-\\bar{U}\\right)\\left(U_s-\\bar{U}\\right)\\right] \\\\\n& \\text { (from fixed } X \\text { 's) } \\\\\n&= \\frac{1}{n^2}\\left[\\sum\\left(X_{i 2}-\\bar{X}_2\\right)^2 \\sigma_i^2+2 \\sum \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(X_{s 2}-\\bar{X}_2\\right) \\sigma_{i s}\\right], \\text { (3.6) }\n\\end{aligned}\n\\] where \\(\\sigma_i^2=E\\left(U_i-\\bar{U}\\right)^2\\) and \\(\\sigma_{i s}=E\\left(U_i-\\bar{U}\\right)\\left(U_s-\\bar{U}\\right)\\). Similar expressions for \\(E\\left(C_{2 U} C_{3 U}\\right)\\) and \\(E\\left(C_{3 U}^2\\right)\\) are \\[\n\\begin{aligned}\n& E\\left(C_{3 U}^2\\right)=\\frac{1}{n^2}\\left[\\sum\\left(X_{i 3}-\\bar{X}_3\\right)^2 \\sigma_i^2+2 \\sum \\sum\\left(X_{i 3}-\\bar{X}_3\\right)\\left(X_{s 3}-\\bar{X}_3\\right) \\sigma_{i s}\\right], \\\\\n& E\\left(C_{2 U} C_{3 U}\\right)=\\frac{1}{n^2}\\left[\\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(X_{i 3}-\\bar{X}_3\\right) \\sigma_i^2\\right. \\\\\n& \\left.+2 \\sum \\sum\\left(X_{i 2}-\\bar{X}_2\\right)\\left(X_{s 3}-\\bar{X}_3\\right) \\sigma_{i s}\\right] \\text {. } \\\\\n&\n\\end{aligned}\n\\] These expressions are quite complicated, but they can be simplified with two more assumptions about the distribution of the errors. These assumptions, however, are not employed just to simplify the algebra. They are also important for developing the properties of the OLS estimator relative to other estimators. If all error terms have the same variance (we have already assumed they have the same mean), then A.4a \\[\nE\\left(U_i-\\bar{U}\\right)^2=\\sigma_i^2=\\sigma^2 \\quad \\text { for all } t .\n\\] Further, if all the error terms are drawn independently of each other so that all the possible error terms associated with one observation are independent of, and thus uncorrelated with, the error terms at other observations, then A.4b \\[\nE\\left(U_i-\\bar{U}\\right)\\left(U_s-\\bar{U}\\right)=\\sigma_{i s}=0 \\quad \\text { for } \\quad t \\neq s .\n\\] With these two assumptions, or restrictions on the error terms, the above simplify to \\[\n\\begin{aligned}\n& E\\left(C_{2 U}^2\\right)=\\frac{1}{n^2} \\sum\\left(X_i-X_2\\right)^2 \\sigma^2=\\frac{\\sigma^2 V_2}{n}, \\\\\n& E\\left(C_{3 U}^2\\right)=\\frac{\\sigma^2 V_3}{n},\n\\end{aligned}\n\\]\nWhen these are substituted into Eq. (3.5), we get \\[\n\\begin{aligned}\n\\operatorname{var}\\left(b_2\\right) & =\\frac{\\sigma^2}{n D^2}\\left[V_3^2 V_2-2 V_3 C_{23}^2+C_{23}^2 V_3\\right]=\\frac{\\sigma^2 V_3}{n D^2}\\left[V_3 V_2-C_{23}^2\\right] \\\\\n& =\\frac{\\sigma^2 V_3}{n D}=\\frac{1}{n} \\sigma^2\\left[\\frac{V_3}{V_2 V_3-C_{23}^2}\\right]=\\frac{\\sigma^2}{n}\\left[\\frac{1 / V_2}{1-r_{23}^2}\\right] .\n\\end{aligned}\n\\] The similar expression for \\(\\operatorname{var}\\left(b_3\\right)\\) is \\[\n\\operatorname{var}\\left(b_3\\right)=\\frac{1}{n} \\sigma^2\\left[\\frac{V_2}{V_2 V_3-C_{23}^2}\\right]=\\frac{\\sigma^2}{n}\\left[\\frac{1 / V_3}{1-r_{23}^2}\\right] .\n\\] [A useful exercise for the student is to show for the bivariate case that \\(\\left.\\operatorname{var}\\left(b_2\\right)=\\sigma^2 / \\Sigma\\left(X_i-\\bar{X}\\right)^2=\\sigma^2 / n \\operatorname{var}(X).\\right]\\)\n\n\n\nHomoskedastizität bedeutet, dass die Streuung der Fehler um die Regressionsgerade überall ungefähr gleich (homo) gross sein sollte. Heteroskedastizität bedeutet, dass die Fehlerstreuung um unsere Regressionsgerade mit der grösse unserer UVs unterschiedlich ist, also z.B. grösser wird, weil Kodierer:innen wenn sie sehr lange nacheinander (weil vielleicht in letzter Minute) kodieren, mit der Zeit immer mehr Fehler machen. Oder weil Kodierer:innen regelmässig ein bisschen kodieren und dabei immer besser werden und immer weniger Fehlerstreuung entsteht. Wenn diese Streuung um die Regressionsgerade mit einer Variablen korreliert wie in Abb. @ref(fig:Heteroskedastizitaet), dann sind die Standardfehler der b’s nicht gut und gültig geschätzt. Mithin sind die t-Werte nicht korrekt, damit die p-Werte und Konfidenzintervalle falsch und schliesslich unsere Entscheidung über die Gültigkeit oder auch die Entscheidbarkeit der Hypothese (H0 oder H1) falsch.\n\n\n\n\n\nHeteroskedastizität\n\n\n\n\nNeben diesem breiter oder schmaler werden der Streuung um die Regressionsgerade entsteht Heteroskedastizität oftmals, wenn wir eine Gerade in einen kurvlinearen Zusammenhang einpassen. In der Abb. @ref(fig:Hetero-Nicht-Linearitaet) ist gut zu erkennen, dass in (a) die Verteilung der standardisierten Fehler recht gleichmässig ist. In (b) geht eben die Schultüte (bzw. Tüte Marroni) auseinander und stellt damit Heteroskedastizität dar. In (c) kommt die Heteroskedastizität durch eine erzwungene Gerade bei gegebener kurvlinearer Beziehung zwischen den Variablen (das sieht in (b) recht kubisch aus). In (d) wäre es beides zusammen, also ein (vermutlich quadratischer) Zusammenhang, bei dem mit steigendem X auch noch die Streuung steigt.\n\n\n\n\n\nNicht-Linearität der Beziehungen\n\n\n\n\nLösen kann man die Probleme mit der Heteroskedastizität, indem man GLS rechnet, also (Generalized Least Squares) und dabei zunächst das korrekte b bestimmst, dann die Streuung berechnet und im 2-Stage-Least-Squares mit den gewichteten Residuen rechnen würde. Das zu vermitteln geht über diesen Kurs hinaus. Einfacher ist es mit den kurvilinearen Beziehungen. Die können wir linearisieren. Wir schauen uns also die Verteilung der Residuen an und wenn wir da so eine kurvlineare Beziehung sehen, dann modellieren wir die so, dass sie linear geschätzt werden kann. Das ist gut in Abb. @ref(fig:Kurvlineare) abgebildet. Dabei ist nicht entscheidend, dass Sie jetzt schon den Aufbau der Formel verstehen, sondern, dass es komplexere Formeln gibt als die einfache additiv lineare, und durch diese Formeln doch wieder das lineare Modell angewendet werden kann, weil die Formeln für eine «Linearisierung» (Transformation) sorgen.\n\n\n\n\n\nLinearisierung kurvlinearer Beziehungen\n\n\n\n\n\n\n\n\n\n\nIYI: Herleitung, wann OLS «Best» ist\n\n\n\n\n\nThe proof that \\(b_2\\) is best is only sketched here. A complete proof is shown in Appendix 5.1. The proposition to be demonstrated is that, among all linear and unbiased estimators of \\(\\beta_2\\) and \\(\\beta_3\\), the least squares estimators \\(b_2\\) and \\(b_3\\) have the minimum variance when assumptions A.1-A.4 hold. We first define an arbitrary linear estimator \\(b_2^{\\#}\\). Linear refers to the fact that the estimator is a linear function of the \\(Y_i, b_2^{\\#}=\\Sigma C_{i 2}^{\\#}\\left(Y_i-\\bar{Y}\\right)\\), where \\(C_{i 2}^{\\#}\\) is any set of weights. (The weights are \\[\nC_{i 2}=\\frac{1}{n} \\frac{V_3\\left(X_{i 2}-\\bar{X}_2\\right)-C_{23}\\left(X_{i 3}-\\bar{X}_3\\right)}{D}\n\\] for the least squares estimator of \\(\\beta_2\\).) With complete generality, we can write \\(C_{i 2}^{\\#}\\) as the least squares weight plus an arbitrary number \\(g_{i 2}, C_{i 2}^{\\#}=C_{i 2}+g_{i 2}\\). The restriction of unbiasedness implies that \\(E\\left[\\Sigma C_{i 2}^{\\#}\\left(Y_i-\\bar{Y}\\right)\\right]=\\beta_2\\). However, for OLS we showed that \\(E\\left[\\Sigma C_{i 2}\\left(Y_i-\\bar{Y}\\right)\\right]=\\beta_2\\). This implies that \\(E\\left[\\Sigma g_{i 2}\\left(Y_i-\\right.\\right.\\) \\(\\bar{Y})]=0\\) since \\(E\\left[\\Sigma C_{i 2}^{\\#}\\left(Y_i-\\bar{Y}\\right)\\right]=E\\left[\\Sigma C_{i 2}\\left(Y_i-\\bar{Y}\\right)+\\Sigma g_{i 2}\\left(Y_i-\\bar{Y}\\right)\\right]\\). Using this restriction and assumption A.4, the variance of \\(b_2^{\\#}\\) is \\[\n\\operatorname{var}\\left(b_2^{\\#}\\right)=\\operatorname{var}\\left(b_2\\right)+\\sigma^2 \\sum_{i=1}^n g_{i 2}^2,\n\\] where \\(\\operatorname{var}\\left(b_2\\right)\\) is the variance of the least squares estimator. Since \\(g_{i 2}^2 \\geqslant 0\\), \\(\\operatorname{var}\\left(b_2^{\\#}\\right)\\) cannot be less than the variance of the least squares estimator \\(b_2\\). Further, it can equal \\(\\operatorname{var}\\left(b_2\\right)\\) only if each perturbation \\(\\left(g_{i 2}\\right)\\) from the least squares weight is identically zero. (Similar developments can be done for \\(b_1\\) and \\(b_3\\).)\nAn important aspect of this development, however, is that we have accepted and used the assumptions about the error distribution and fixed \\(X\\) ’s. That is, this proof holds when \\(E\\left(U_i-\\bar{U}\\right)=0, E\\left(U_i-\\bar{U}\\right)^2=\\sigma^2\\), and \\(E\\left(U_i-\\right.\\) \\(\\bar{U})\\left(U_s-\\bar{U}\\right)=0\\) for all \\(i\\) and \\(s \\neq i\\).\n\n\n\n\n\n2.3.4 Verteilung der Residuen (V7. und V8.)\nEin Modell und die zugrundeliegenden Beziehungen ist oft dann gut, wenn die Verteilung der nicht erklärten Varianzanteile sich wie eine einfache Zufallsverteilung verhält bzw. wie Schrott.\n\n2.3.4.1 Normalverteilung der Fehler (V7.)\nDie Residuen (also der nicht erklärte Rest bzw. Modellfehler oder einfach Fehler) bezieht sich immer auf die nicht erklärte Streuung in der AV. Wenn wir also unser Modell haben und mit unseren Daten berechnen, dann bekommen wir vorhergesagte Werte und den Rest. Wenn wir den Rest anschauen, dann sollte der nicht zu stark von einer Normalverteilung abweichen.\nIn der Abb. @ref(fig:Hetero1) sieht man recht gut, dass links eine relativ gleichmässige Verteilung vorliegt, also kein Zusammenhang zwischen Fehlern und geschätzten Werten zu erkennen ist (Wäre perfekt 0, wenn die rote Linie exakt auf der gestrichelten Null-linie liegen würde.). Im zweiten Fall namens «Case 2» sieht man deutlich, dass es hier eine kuvlineare Abweichung gibt. Hier würde es sich sicher lohnen, ein quadratisches Modell anzupassen.\n\n\n\n\n\nResiduen gegenüber Modell\n\n\n\n\nIn der Grafik @ref(fig:Hetero2) sind Normal Q-Q-Plots abgebildet. Bei dieser visuellen Darstellung werden die standardisierten Residuen gegen die theoretischen Quantile abgetragen, wobei «theoretisch» hier die zu erwartende Verteilung nach Wahrscheinlichkeitstheorie also nach Normalverteilung. Wenn die Punkte alle auf der Gerade liegen, dann ist der Normalverteilung der Residuen nicht stark widersprochen. Wenn sie, wie im zweiten Fall (typisch Case 2!) abweichen, dann ist die Annahme der Normalverteilung verletzt. Dann würden wir nach einem R-Paket suchen, das mit diesem Problem umgehen kann.\n\n\n\n\n\nNormal Q-Q\n\n\n\n\n\n\n\n\n\n\nIYI: Ableitung für \\(U_i \\sim N\\left(0, \\sigma^2\\right)\\) .\n\n\n\n\n\nIf \\(U_i \\sim N\\left(0, \\sigma^2\\right)\\), and independent of \\(U_s\\), then the \\(b\\) ’s, which are linear functions of \\(U_i\\), are normally distributed with a variance given in Eq. (3.5a), that is, \\(b_2 \\sim N\\left(\\beta_2, \\sigma^2 / n V_2\\left(1-r_{23}^2\\right)\\right)\\). This implies that \\[\nZ_2=\\left(b_2-\\beta_2\\right) / \\sigma_{b_2} \\quad \\text { and } \\quad Z_3=\\left(b_3-\\beta_3\\right) / \\sigma_{b_3}\n\\] are distributed according to the standard normal distribution \\(N(0,1)\\). (This distribution is discussed in Appendix I.) Since the Z’s are \\(N(0,1)\\), standard tables of cumulative normal distributions would yield the probability of \\(Z\\) being greater than any given value, or in turn would give the probability of the estimated value being more than any given distance from the true value of \\(\\beta\\), i.e., \\(b-\\beta\\). However, (3.16) depends upon \\(\\sigma\\), which is unknown. \\(s^2\\) is an unbiased estimator of \\(\\sigma^2\\) and is substituted for \\(\\sigma\\) in our expression for \\(\\sigma_b\\). However, any inferences employing \\(s^2\\) will not be as precise as they would be if we knew \\(\\sigma^2\\) and did not have to rely upon the random variable \\(s^2\\). In order to allow for this additional imprecision, we do not use probabilities from the normal distribution. Instead, we shall rely upon the \\(t\\)-distribution. The definition of the \\(t\\)-distribution is as follows: if \\(Z\\) is a standard normal variable, i.e., \\(Z\\) is \\(N(0,1)\\), and if \\(W^2\\) is an independently distributed chisquared with \\(n-3\\) degrees of freedom, then \\(Z / \\sqrt{W^2 /(n-3)}\\) is distributed according to the \\(t\\)-distribution with \\(n-3\\) degrees of freedom. We have demonstrated that \\((b-\\beta) / \\sigma_b\\) is \\(N(0,1)\\). It can further be shown that (Hoel, 1962, pp. 262-268) \\[\nW^2=\\sum e_i^2 / \\sigma^2 \\quad \\text { is } \\quad \\chi_{n-3}^2 .\n\\] For \\(b_2\\) we get \\[\n\\begin{aligned}\nt_{b_2} & =\\frac{Z}{\\sqrt{W^2 /(n-3)}}=\\frac{\\left(b_2-\\beta_2\\right) / \\sigma_{b_2}}{\\sqrt{\\sum e_i^2 / \\sigma^2(n-3)}} \\\\\n& =\\frac{\\left(b_2-\\beta_2\\right)}{(\\sigma / n)\\left[1 / V_2\\left(1-r_{23}^2\\right)\\right]} \\frac{1}{\\sqrt{\\sum e_i^2} / \\sigma \\sqrt{n-3}}=\\frac{b_2-\\beta_2}{(s / n)\\left(\\left(1 / V_2\\right) /\\left(1-r_{23}^2\\right)\\right)} \\\\\n& =\\frac{b_2-\\beta_2}{s_{b_2}},\n\\end{aligned}\n\\] where \\(s^2=\\sum e_i^2 /(n-3)\\). This variable \\(t_{b_2}\\) is distributed as Student’s \\(t\\) with \\(n-3\\) degrees of freedom.\n\\[\nt_b=|b| / s_b>t_{\\text {crit }(\\alpha / 2, n-3)},\n\\] \\(t_{\\text {crit }(\\alpha / 2, n-3)}\\) is the critical value for \\(n-3\\) degrees of freedon ificance level of \\(\\alpha\\). The significance level \\(\\alpha\\) is the size of a type I probability of rejecting the null hypothesis when it is in fact ole terms, the further \\(b\\) is from zero (i.e., the higher \\(t_b\\) ), the less lik \\(\\beta\\) is really zero. The general form of the hypothesis test for \\(H_0\\) : \\[\nt_b=\\left|b-\\beta^*\\right| / s_b .\n\\] \\(>t_{\\text {crit }(\\alpha / 2, n-3)}\\), the null hypothesis is rejected. In other words\n\n\n\n\n\n2.3.4.2 Unabhängigkeit der Fehler (V8.)\nDie Unabhängigkeit der Fehler ist eigentlich nur dann ein echtes Problem, wenn die Fehler in eine Reihenfolge gebracht werden können. Das wiederum passiert eher nur bei Zeitreihen, also wenn die Werte einer Erhebung zeitlich angeordnet sind. Dafür gibt es dann allerdings die recht komplexen Zeitreihenanalysen, die eher Statistik IV im Master darstellen. Wir können uns in der R-Übung mal den Durbin-Watson-Test anschauen (zum Spass die Formel @ref(eq:DWT), wo man schon sieht, dass nicht der Index i für Fälle, sondern t durchläuft für time), der prüft, ob die Fehler autokorreliert sind, also hoch mit der um eine Zeiteinheit versetzten Version ihrer selbst korrelieren. Was Sie mitnehmen sollten ist, dass sie bei Erhebungen über die Zeit (Longitudinalstudien), noch prüfen müssen, ob bzw. inwieweit die Fehler miteinander korrelieren.\n\\[\\begin{align}\n  d =& \\frac{\\sum_{i=2}^n(e_i-e_{i-1})^2}{\\sum_{i=1}^n e^2_i} \\label{eq:DWT}\n\\end{align}\\]"
  },
  {
    "objectID": "09_Clusteranalyse.html#voraussetzungen-von-clusteranalysen-generell",
    "href": "09_Clusteranalyse.html#voraussetzungen-von-clusteranalysen-generell",
    "title": "7  Clusteranalyse",
    "section": "7.1 Voraussetzungen von Clusteranalysen generell",
    "text": "7.1 Voraussetzungen von Clusteranalysen generell\n\nNicht zu viele fehlende Werte, da fehlende Werte die Clusterbildung verzerren.\nDas Skalenniveau spielt grundsätzlich keine Rolle, da Methoden der Clusteranalyse gibt (hierarchische Clusteranalyse), die auch mit kategorialen (nominale mit mehreren Ausprägungen) Variablen und ordinalen Variablen gut umgehen kann. Bei Clusteranalysemethoden mit Distanzmassen (wie k-Means-Clustering) müssen die Distanzen interpretierbar sein und mithin metrisch skaliert, was aber auch von den Dummys erfüllt wird.\nDie Fallzahl sollte nicht zu klein sein. Es werden für brauchbare Clusteranalysen ordentliche Fallzahlen benötigt. Vor allem gilt das, wenn einzelne Variablenkombinationen (wichtig für die Gruppenbildung) dünn besetzt sind.\nDie Variablen sollten ähnlich skaliert sein, damit nicht eine Variable mit einem deutlich grösserem Gewicht in die Clusteranalyse eingeht, nur weil sie breiter skaliert ist. Bei sehr unterschiedlichen Skalierungen empfiehlt sich eine vorherige Standardisierung (z-Transformation bzw. scale) der Variablen (bei vorheriger Faktorenanalyse ist das schon gegeben, weil Faktoren immer standardisiert sind (\\(\\overline{x}\\) = 0, sd = 1)).1 ## Vorgehensweise\n\nZunächst muss man festlegen, welches Mass für die Ähnlichkeit oder die Distanz stehen soll. Diese «Proximitätsmasse» können folgende Distanzmasse sein: euklidische Distanzen bei metrischen Variablen (Distanz d ist die Wurzel der Summe aller quadrierten Abstände in den Richtungen der Variablendimensionen: \\(d = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2}\\)).\n\n\n\nEuklidische-Distanz\n\n\nEs gibt weitere Distanzmasse auch für metrische Variablen. Bei dichotomen Variablen gibt es noch den M-Koeffizienten, der schlicht die Übereinstimmungen wiedergibt (also in allen Variablen eine 1 oder in allen Variablen eine 0 oder nur in Teilen usw.).\nNeben diesen Massen für die Distanz bzw. Nähe im geometrischen Sinne, gibt es noch Ähnlichkeitsmasse. Dazu zählt zum Beispiel der Q-Korrelationskoeffizient, der dasselbe ist, wie Pearsons Korrelationskoeffizient. Die Korrelationen können bei stetigen Variablen verwendet werden. Bei kategoriellen Merkmalen kann \\(\\chi^2\\) verwendet werden. Eine kleine Systematik der Clusteranalyse findet sich in Abbildung @ref(Clustersystematik).\n\n\n\nClustersystematik\n\n\nDann muss noch der Cluster-Algorithmus gewählt werden. Es gibt die «Hierarchische Clusteranalyse» mit «Single-Linkage» und mit «Complete Linkage». Für metrische Variablen können partionierende Clusteranalysen eingesetzt werden, wie der k-Means-Algorithmus oder der Two-Stage-Algorithmus. Die einfachste Methode ist im Grunde die k-Means-Cluster-Methode, die daher hier als erste etwas genauer angeschaut und in R berechnet werden sollte.\nNehmen Sie also den Code und probieren Sie es aus. So können Sie praktisch üben."
  },
  {
    "objectID": "09_Clusteranalyse.html#die-k-means-cluster-methode",
    "href": "09_Clusteranalyse.html#die-k-means-cluster-methode",
    "title": "7  Clusteranalyse",
    "section": "7.2 Die k-Means-Cluster-Methode",
    "text": "7.2 Die k-Means-Cluster-Methode\nIn der Schrittweisen «Animation» der k-Means-Clusterung in Abbildung @ref(kmeans-Animation) sieht man, wenn man genau hinschaut, wie die Cluster am Anfang zufällig verteilt werden, dann alle Fälle, den ihnen am nächsten gelegenen Clusterzentrum zugeordnet werden und die «Clusterzentren» eigentlich erst dann in das Zentrum ihres Clusters gelegt werden. Dann kann es vorkommen, dass einzelne Fälle dichter an einem anderen Cluster liegen und werden deshalb eben diesem Cluster zugeordnet, in dessen Nähe sie liegen. Danach liegen die Clusterzentren wieder nicht mehr genau im Zentrum ihres eigenen Clusters und werden erneut so verschoben, dass sie genau in dessen Mitte liegen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMit diesen Befehlen kann man eine schöne Clusteranalyse laufen lassen."
  },
  {
    "objectID": "04_Gruppenvergleiche.html#visualisierung-und-deskriptives",
    "href": "04_Gruppenvergleiche.html#visualisierung-und-deskriptives",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.1 Visualisierung und Deskriptives",
    "text": "4.1 Visualisierung und Deskriptives\nGruppenvergleiche können schon gut mit Boxplots gemacht werden. Dabei wird der Mittelwert in einer Box als Linie dargestellt und die das untere sowie das obere Quartil (25% bzw. 95% der Verteilung) als Ränder der Box.\n\n\n\n\n\nEs können aber auch Histogramme erstellt werden, mit Mittelwerten für zwei Gruppen, wobei die Balken für die einzelnen Werte bzw. Wertegruppen überlagert sind.\n\n\n\n\n\nSpätestens an dieser Stelle sollte man sich die Ausgangsvariablen mal angucken, um zu sehen, wie die verteilt ist und wo ihr Mittelwert liegt und wie sie um ihren Mittelwert streut und all das. Dafür ist es immer sinnvoll sich die Variablen als Häufigkeitsauszählung anzusehen.\n\n## Cloak of invisibility (Cloak) <numeric> \n## # total N=24 valid N=24 mean=0.50 sd=0.51\n## \n## Value |    Label |  N | Raw % | Valid % | Cum. %\n## ------------------------------------------------\n##     0 | No Cloak | 12 |    50 |      50 |     50\n##     1 |    Cloak | 12 |    50 |      50 |    100\n##  <NA> |     <NA> |  0 |     0 |    <NA> |   <NA>\n## \n## Mischievous Acts (Mischief) <numeric> \n## # total N=24 valid N=24 mean=4.38 sd=1.86\n## \n## Value | N | Raw % | Valid % | Cum. %\n## ------------------------------------\n##     0 | 1 |  4.17 |    4.17 |   4.17\n##     1 | 1 |  4.17 |    4.17 |   8.33\n##     2 | 2 |  8.33 |    8.33 |  16.67\n##     3 | 2 |  8.33 |    8.33 |  25.00\n##     4 | 5 | 20.83 |   20.83 |  45.83\n##     5 | 7 | 29.17 |   29.17 |  75.00\n##     6 | 4 | 16.67 |   16.67 |  91.67\n##     7 | 1 |  4.17 |    4.17 |  95.83\n##     8 | 1 |  4.17 |    4.17 | 100.00\n##  <NA> | 0 |  0.00 |    <NA> |   <NA>\n\nUnd man sollte sich die Mittelwerte der AV (hier Mischief) und die Gruppierungsvariable (auch UV und hier Cloak) ausgeben lassen.\n\n## # A tibble: 2 × 2\n##   Cloak        Mittelwerte\n##   <dbl+lbl>          <dbl>\n## 1 0 [No Cloak]        3.75\n## 2 1 [Cloak]           5"
  },
  {
    "objectID": "04_Gruppenvergleiche.html#mittelwertvergleich-für-zwei-gruppen",
    "href": "04_Gruppenvergleiche.html#mittelwertvergleich-für-zwei-gruppen",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.2 Mittelwertvergleich für zwei Gruppen",
    "text": "4.2 Mittelwertvergleich für zwei Gruppen\n\n4.2.1 mit dem t-Test\nMit dem t-Test kann geprüft werden, ob sich die Mittelwerte der beiden Gruppen unterscheiden. Es wird ein t-Test für unabhängige Stichproben gemacht. Dabei wird die Differenz der beiden Mittelwerte berechnet und gegen die H0 getestet, dass sie 0 sein könnte, also in der GG kein Unterschied zwischen der Gruppe Cloak = 1 und der Gruppe Cloak = 0.\n\n## \n##  Welch Two Sample t-test\n## \n## data:  Mischief by Cloak\n## t = -1.7135, df = 21.541, p-value = 0.101\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -2.764798  0.264798\n## sample estimates:\n## mean in group 0 mean in group 1 \n##            3.75            5.00\n\nDer Output sagt uns, dass die Mittelwerte von Mischief aufgeteilt nach Cloak angeschaut werden. Der t-Wert unter Annahme der Nullhypothese H0 ist -1.7135 und der zugehörige p-Wert ist 0.101. Im Text steht noch, dass die Alternativhypothese lautet: Der wahre Mittelwertunterschied zwischen der 0-Gruppe und der 1-Gruppe ist nicht gleich 0. Darunter steht das 95-prozentige Konfidenzintervall der Mittelwertdifferenz. In der untersten Zeile werden die beiden Mittelwerte der beiden Gruppen nochmals ausgegeben.\n\n\n4.2.2 Mit Korrelation\nWenn die Gruppenvariable eine Dummyvariable ist (also dichotom und nur aus 0 und 1 bestehend), dann kann auch eine Korrelation gerechnet werden, wobei der t-Wert dann derselbe ist, wie beim t-Test von Mittelwertvergleichen für unabhängige Stichproben.\n\n## \n##  Pearson's product-moment correlation\n## \n## data:  Invisibility$Mischief and Invisibility$Cloak\n## t = 1.7135, df = 22, p-value = 0.1007\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.06994687  0.65575942\n## sample estimates:\n##       cor \n## 0.3431318\n\nVergleichen Sie mal den t-Wert und den p-Wert der Korrelation mit dem des t-Test für Mittelwertunterschiede. Die sind (bis auf Rundungsunterschiede) identisch.\nEs gibt die einfache Varianzanalyse. Dabei wird geprüft, ob die Gruppierungsvariable signifikant Varianz der AV aufklärt. Der p-Wert ist derselbe, wie oben beim t-Test und der Korrelation, weil es dieselben Daten und Variablen sind.\n\n\n4.2.3 Gruppenvergleich mit Varianzanalyse\n\n\n# Mache eine Varianzanalyse (Analysis of Varian (aov bzw. ANOVA)) mit einer UV (one.way)\none.way <- aov(Mischief ~ Cloak, data = Invisibility)\n\n# Gib die Zusammenfassung der aov raus\nsummary(one.way)\n##             Df Sum Sq Mean Sq F value Pr(>F)\n## Cloak        1   9.38   9.375   2.936  0.101\n## Residuals   22  70.25   3.193\n\n# Berechne mal das R^2 durch die Quadratsumme (Sum Sq), die die Gruppierung (hier nach Cloak) aufklärt, durch die Gesamtquadratsumme (Sum Sq der Cloak + der der Residuals). Dann runde auf 4 Nachkommastellen.\n\nR2 <- round(9.38/(9.38 + 70.25),4)\n\n# Binde das R^2 in die Ausgabe ein, einfach für später\npaste0(\"R2 = Sum_Sq_Cloak / (Sum_Sq_Cloak + Risiduals_SumSq): \", R2,\" (12%)\")\n## [1] \"R2 = Sum_Sq_Cloak / (Sum_Sq_Cloak + Risiduals_SumSq): 0.1178 (12%)\"\n\nDie Varainzanalyse prüft, ob die Mittelwerte in einer AV für jede der UV-Gruppen identisch ist. Was in der Tabelle steht, sind lauter Hilfswerte für den einen relevantenn Wert: dem p-Wert (hier “Pr(>F)). Der p-Wert ist wieder derselbe wie oben bei der Korrelation und dem Mittelwertvergleich."
  },
  {
    "objectID": "04_Gruppenvergleiche.html#mittelwertvergleich-mit-regression",
    "href": "04_Gruppenvergleiche.html#mittelwertvergleich-mit-regression",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.3 Mittelwertvergleich mit Regression",
    "text": "4.3 Mittelwertvergleich mit Regression\nAm besten kann mit einer Regression ein Mittelwertvergleich durchgeführt werden. Das \\(R^2\\) entspricht dem Quadrat der Korrelation. Der F-Wert zum \\(R^2\\) ist gleich dem F-Wert aus der Varianzanalyse. Der b-Wert (hier von «Cloak of invisibility») in der Regression entspricht dem Mittelwertunterschied zwischen den beiden Gruppen. Der «Intercept» entspricht dem Mittelwert der 0-Gruppe (keine Cloak). Mit der Regression kann also alles abgedeckt werden, was mit den anderen Auwertungsmethoden auch erledigt wird. Die Regression kann aber mehr!\n\n\n\n\n\n\n \n\n\nMischievous Acts\n\n\n\n\nPredictors\n\n\nEstimates\n\n\nstd. Beta\n\n\nCI\n\n\nstandardized CI\n\n\np\n\n\n\n\n(Intercept)\n\n\n3.75\n\n\n-0.00\n\n\n2.68 – 4.82\n\n\n-0.41 – 0.41\n\n\n<0.001\n\n\n\n\nCloak of invisibility\n\n\n1.25\n\n\n0.34\n\n\n-0.26 – 2.76\n\n\n-0.07 – 0.76\n\n\n0.101\n\n\n\n\nObservations\n\n\n24\n\n\n\n\nR2 / R2 adjusted\n\n\n0.118 / 0.078\n\n\n\n\nRegression mit einer Dummmy als UV"
  },
  {
    "objectID": "04_Gruppenvergleiche.html#interaktionseffekte",
    "href": "04_Gruppenvergleiche.html#interaktionseffekte",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.4 Interaktionseffekte",
    "text": "4.4 Interaktionseffekte\nWerden wir mal erwachsen und schauen uns ein anderes Beispiel an, das auf eine Medienwirkgungsfrage zurück geht. Gehen wir also jetzt der Frage nach, ob gewalthaltige Videospiele antisozial machen. Dazu hat das britische Ofcom (Office of Communication) 2008 eine Studie herausgegeben [@Ofcom2008]. Für die Studie wurden 442 Jugendliche befragt. Im folgenden Chunkg wird der dazugehörige Datensatz heruntergeladen, umgewandelt und im Datenobjekt «Video_Games» gespeichert. Den analysieren wir im Folgenden. Die Variablen sind «Aggression» als Messung aggressiver Verhaltensweisen, «CaUnTs» als callous unemotional traits (affektiv-soziale defizite) und «Vid_Games» in Stunden Nutzung von Videospielen.\n\n## # A tibble: 442 × 4\n##      ID Aggression Vid_Games CaUnTs\n##   <dbl>      <dbl>     <dbl>  <dbl>\n## 1    69         13        16      0\n## 2    55         38        12      0\n## 3     7         30        32      0\n## 4    96         23        10      1\n## 5   130         25        11      1\n## 6   124         46        29      1\n## # … with 436 more rows\n\nSchauen wir uns das mal genauer an:\n\n## Warning in FUN(X[[i]], ...): NAs durch Umwandlung erzeugt\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nWie man sieht gibt es die zwei Gruppen. Wenn die Tage pro Woche mit Videospielen steigt, dann steigt die Aggression kaum an. Das ist für beide Gruppen so, ber für die 2-er-Gruppe (mittleres Antisoziales Verhalten) liegen die Werte im Mittel höher. Das haben wir jetzt gesehen, aber geschätzt und getestet haben wir es noch nicht. Das geht aber gut mit der Regression."
  },
  {
    "objectID": "04_Gruppenvergleiche.html#eine-dummy-als-uv",
    "href": "04_Gruppenvergleiche.html#eine-dummy-als-uv",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.5 Eine Dummy als UV",
    "text": "4.5 Eine Dummy als UV\nWenn wir eine Dummyvariable als UV haben, dann haben wir es eigentlich mit einem Unterschiedstest zu tun, also einem Mittelwertvergleich. Vergleichen werden dabei die Mittelwerte der UV für zwei Gruppen. Die Gruppen wiederum werden durch in der Dummyvariable festgelegt: Die eine Gruppe (G0) hat die 0 und die andere Gruppe (G1) die 1. Es wird also die Differenz in den Y-Werten (Y_Diff) durch die Dummyvariable erklärt.\n\\[\\begin{align}\n\\overline{Y}_{Diff}&=\\overline{Y}_{G1}-\\overline{Y}_{G0}&\\\\\n      Y_i&=b_1 + b_2X_{i2}\\\\\n      Y_i&=b_1 &\\text{ wenn } X_{i2}=0\\\\\n      Y_i&=b_1  + b_2 &\\text{ wenn } X_{i2}=1\\\\\n  \\text{Also ist:}\\overline{Y}_{Diff}&=b_2\\\\\n  t&=\\frac{\\overline{Y}_{G1}-\\overline{Y}_{G0}}{se_{\\overline{Y}_{Diff}}}=\\frac{b_2}{se_b}\n\\end{align}\\]\n\nHier ist nur «Anti_Soz_mittel» als UV im Modell.\n\n\n\n\n\n\n \n\n\nAgression\n\n\n\n\nPredictors\n\n\nb\n\n\nstd. b\n\n\nCI\n\n\nstandardized CI\n\n\np\n\n\n\n\n(Intercept)\n\n\n30.76\n\n\n-0.00\n\n\n28.71 – 32.81\n\n\n-0.09 – 0.09\n\n\n<0.001\n\n\n\n\nCallous UnemotionalTraits\n\n\n10.33\n\n\n0.40\n\n\n7.91 – 12.75\n\n\n0.30 – 0.49\n\n\n<0.001\n\n\n\n\nObservations\n\n\n381\n\n\n\n\nR2 / R2 adjusted\n\n\n0.157 / 0.155\n\n\n\n\nRegression mit einer Dummy als UV\n\n\nInterpretation der Regression: Der (Intercept) hat im b eine 30.76 und zeigt daher in diesem Modell an, wie gross der Mittelwert für die Referenzgruppe ist (0 für Anti_Soz_mittel «nicht mittel»). Das b für die «Callous Unemotional Traits» liegt bei 10.33. Das bedeutet, dass der Mittelwert der Gruppe Anti_Soz_mittel = 1 um 10.33 grösser ist als der Mittelwert der 0-Gruppe, also 41.09. Dieser Unterschied entspricht einem Zusammenhang von .4 als Korrelation, was an dem standardisierten b abgelesen werden kann, weil die standardisierten Regressionskoeffizienten (oft auch als BETA bezeichnet) sehr dicht an den Korrelationskoeffizienten sind. Das Konfidenzintervall für den Mittelwertunterschied liegt zwischen 7.91 und 12.75. Da 0 nicht mit im Intervall liegt, sehen wir schon, dass der Mittelwertunterschied signifikant ist. Wir sehen aber nicht nur, dass der Mittelwertunterschied signifikant von 0 verschieden ist, sondern auch, dass er signifikant von z.B. 5 verschieden ist. Wenn jetzt zum Beispiel andere Forscherinnen das Phänomen vorher schon untersucht gehabt hätten und die Mittelwertunterschied zwischen 1.93 und 4.25 gefunden hätten, dann könnten wir mit der Analyse hier sagen, dass sich die beiden Konfidenzintervalle nicht überschneiden, also unser Ergebnis signifikant von dem der anderen Forscher ist. Das geht schon in die Richtung Metaanalyse. Wenn wir nochmal in die Tabelle schauen, dann sehen wir hinten auch, dass die p-Werte unter .05 liegen, was eine Signifikanz auf dem 95%-igem Signifikanzniveau anzeigt. Das wussten wir über die CI aber auch schon vorher und da wussten wir sogar mehr!"
  },
  {
    "objectID": "04_Gruppenvergleiche.html#dummy-und-covariate",
    "href": "04_Gruppenvergleiche.html#dummy-und-covariate",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.6 Dummy und Covariate",
    "text": "4.6 Dummy und Covariate\nJetzt wird das Modell um eine Covariate ergänzt. Mit olsrr::ols_vif_tol(Modell3) werden die Toleranz und der VIF berechnet.\n\n##       Variables Tolerance     VIF\n## 1     Vid_Games 0.9998605 1.00014\n## 2 Anti_Soz_hoch 0.9998605 1.00014\n\n\n\n\n \nAgression\n\n\nPredictors\nb\nstd. b\nCI\nstandardized CI\np\n\n\n(Intercept)\n33.13\n-0.00\n29.54 – 36.73\n-0.09 – 0.09\n<0.001\n\n\nVideo Games(Hours perweek)\n0.23\n0.13\n0.07 – 0.39\n0.04 – 0.21\n0.004\n\n\nCallous UnemotionalTraits\n13.65\n0.37\n10.51 – 16.79\n0.29 – 0.46\n<0.001\n\n\nObservations\n442\n\n\nR2 / R2 adjusted\n0.157 / 0.153\n\n\n\n\n\n\nDie Toleranzwerte sind sehr hoch und daher völlig ok. Der Varianzinflationsfaktor ist fast genau 1. Es gibt also eigentlich keine Inflation der Fehlerstreuung der b’s (und allem was darauf aufbaut, wie die standardisierten Regressionskoeffizienten, Konfidenzintervalle, t-Wert zum t-Test und also auch die p-Werte). Also ist hier alles gut."
  },
  {
    "objectID": "04_Gruppenvergleiche.html#dummy-in-interaktion-mit-der-covariate",
    "href": "04_Gruppenvergleiche.html#dummy-in-interaktion-mit-der-covariate",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.7 Dummy in Interaktion mit der Covariate",
    "text": "4.7 Dummy in Interaktion mit der Covariate\nEs kann natürlich sein, dass eine Gruppe mal einen anderen Zusammenhang hat, also einen anderen Anstieg der Regressionsgeraden. Jetzt nehmen wir die Gruppe mit hohem Antisozialem Verhalten mit rein und gucken für die, ob das häufige Spielen von Videospielen einen Einfluss auf ihr aggressives Verhalten hat.\n\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nZusammenhang Videospiele zu Aggression für Menschen mit hohem vs. geringerem antisozialen Verhalten\n\n\n\n\nIn der Grafik ist gut zu erkennen, dass die Videospiele auf die Personen mit hohem antisozialen Verhalten eine deutlich stärkere Wirkung hat. Der Anstieg der Regressionsgerade ist für die Gruppe mit Anti_Soz_hoch deutlich steiler. Ihr Mittelwert liegt im Schnittpunkt der Mittelwert (in der Mitte der Wolke) auch höher als bei der Gruppe ohne hohes antisoziales Verhalten.\nDas lässt sich in einer Regression bauen, indem die Dummy so wie sie ist in das Modell aufgenommen wird und dann noch als Interaktion mit der Kovariaten.\n\n##                 Variables Tolerance      VIF\n## 1               Vid_Games 0.8318770 1.202101\n## 2           Anti_Soz_hoch 0.1049793 9.525692\n## 3 Vid_Games:Anti_Soz_hoch 0.1024746 9.758512\n\n\n\n\n \nAgression\n\n\nPredictors\nb\nstd. b\nCI\nstandardized CI\np\nstd. p\n\n\n(Intercept)\n35.95\n-0.00\n32.10 – 39.80\n-0.09 – 0.08\n<0.001\n0.968\n\n\nVideo Games(Hours perweek)\n0.10\n0.11\n-0.07 – 0.27\n0.03 – 0.20\n0.238\n0.008\n\n\nCallous UnemotionalTraits\n-3.28\n0.37\n-12.84 – 6.29\n0.29 – 0.46\n0.501\n<0.001\n\n\nVid_Games:Anti_Soz_hoch\n0.77\n0.15\n0.36 – 1.18\n0.07 – 0.23\n<0.001\n<0.001\n\n\nObservations\n442\n\n\nR2 / R2 adjusted\n0.183 / 0.177\n\n\n\n\n\n\nDabei bauen wir im Grunde die Variablen doppelt in das Modell ein. Das hat zur Folge, dass vor allem die Dummys mit dem Interaktionsterm sehr hoch Multikollinear sind. Da hilft es, wenn man alle Variablen im Modell mit scale()z-transformiert, also zentriert und so skaliert, dass die Standardabweichungen bei allen Variablen 1 sind.\n\n##                 Variables Tolerance      VIF\n## 1               Vid_Games 0.8318770 1.202101\n## 2           Anti_Soz_hoch 0.9993801 1.000620\n## 3 Vid_Games:Anti_Soz_hoch 0.8314800 1.202675\n\n\n\n\n \nAggression\n\n\nPredictors\nb\nstd. b\nCI\nstandardized CI\np\nstd. p\n\n\n(Intercept)\n-0.15\n-0.00\n-0.24 – -0.06\n-0.09 – 0.08\n0.001\n0.968\n\n\nVid Games\n0.06\n0.11\n-0.04 – 0.15\n0.03 – 0.20\n0.238\n0.008\n\n\nCallous UnemotionalTraits\n1.07\n0.37\n0.83 – 1.32\n0.29 – 0.46\n<0.001\n<0.001\n\n\nVid_Games:Anti_Soz_hoch\n0.42\n0.15\n0.20 – 0.65\n0.07 – 0.23\n<0.001\n<0.001\n\n\nObservations\n442\n\n\nR2 / R2 adjusted\n0.183 / 0.177"
  },
  {
    "objectID": "04_Gruppenvergleiche.html#eine-kategoriale-als-uv",
    "href": "04_Gruppenvergleiche.html#eine-kategoriale-als-uv",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.8 Eine Kategoriale als UV",
    "text": "4.8 Eine Kategoriale als UV\nEs kann natürlich auch sein, dass man eine Variable hat, die kategorial ist, also mehr als zwei Ausprägungen hat. Dann erstellt man zu jeder Ausprägung eine Dummyvariable und baut zwei davon in das Modell ein. Dann ist die Ausprägung der übrigen Variable die Referenzkategorie. Den Schnittpunkt mit den Mittelwerten der übrigen Kovariaten liegt für die Referenzkategorie beim \\(b_1\\). Der Anstieg der Referenzkategorie ist der des b’s für die Kovariate alleine.\n\n##                   Variables Tolerance      VIF\n## 1                 Vid_Games 0.9907942 1.009291\n## 2             Anti_Soz_hoch 0.7390622 1.353066\n## 3           Anti_Soz_mittel 0.7373241 1.356256\n## 4   Vid_Games:Anti_Soz_hoch 0.6855523 1.458678\n## 5 Vid_Games:Anti_Soz_mittel 0.6887144 1.451981\n\n\n\n\n \nAggression\n\n\nPredictors\nb\nstd. b\nCI\nstandardized CI\np\n\n\n(Intercept)\n-0.00\n-0.00\n-0.08 – 0.08\n-0.08 – 0.08\n0.958\n\n\nVid Games\n0.10\n0.10\n0.02 – 0.18\n0.02 – 0.18\n0.015\n\n\nAnti Soz hoch\n0.57\n0.57\n0.48 – 0.66\n0.48 – 0.66\n<0.001\n\n\nAnti Soz mittel\n0.40\n0.40\n0.31 – 0.49\n0.31 – 0.49\n<0.001\n\n\nVid Games × Anti Soz hoch\n0.16\n0.16\n0.07 – 0.25\n0.07 – 0.25\n<0.001\n\n\nVid Games × Anti Sozmittel\n0.01\n0.01\n-0.09 – 0.10\n-0.09 – 0.10\n0.865\n\n\nObservations\n442\n\n\nR2 / R2 adjusted\n0.299 / 0.291"
  },
  {
    "objectID": "04_Gruppenvergleiche.html#beispiel-vertrauen-in-politiker-nach-journalistischen-arbeitsjahren",
    "href": "04_Gruppenvergleiche.html#beispiel-vertrauen-in-politiker-nach-journalistischen-arbeitsjahren",
    "title": "4  Gruppenvergleiche (ANCOVA)",
    "section": "4.9 Beispiel Vertrauen in Politiker nach journalistischen Arbeitsjahren",
    "text": "4.9 Beispiel Vertrauen in Politiker nach journalistischen Arbeitsjahren\nIn diesem Beispiel wird untersucht, wie sich die Dauer der Arbeitsjahre von Journalisten auf ihr Vertrauen in Politiker auswirkt. Den Datensatz holen wir aus tidycomm::WoJ.\n\n\n\nIn dem Scatterplot sieht man gut, dass das Vertrauen in Deutschland und der Schweiz sinkt, wobei es in der Schweiz auf demselben Niveau beginnt, dann aber schneller abnimmt mit den Jahren an Erfahrungen. In Österreich ist das Vertrauen am Anfang schlechter und steigt dann mit der Zeit.\n\n## `geom_smooth()` using formula = 'y ~ x'\n## Warning: Removed 3 rows containing non-finite values (`stat_smooth()`).\n## Warning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\nHier das Ganze als Regressionsrechnung. Deutschland ist die Referenzkategorie und dann wird geschaut, wie die Schweiz und Österreich davon abweichen.\n\n##            Variables Tolerance      VIF\n## 1    work_experience 0.9258454 1.080094\n## 2                 CH 0.6292380 1.589224\n## 3                 AT 0.6558027 1.524849\n## 4 work_experience:CH 0.7298261 1.370189\n## 5 work_experience:AT 0.7190955 1.390636\n\n\n\n\n \ntrust politicians\n\n\nPredictors\nb\nstd. b\nCI\nstandardized CI\np\nstd. p\n\n\n(Intercept)\n-0.01\n-0.01\n-0.09 – 0.07\n-0.09 – 0.07\n0.799\n0.786\n\n\nwork experience\n-0.06\n-0.06\n-0.14 – 0.02\n-0.14 – 0.02\n0.165\n0.161\n\n\nCH\n-0.04\n-0.04\n-0.14 – 0.06\n-0.14 – 0.06\n0.420\n0.420\n\n\nAT\n-0.16\n-0.16\n-0.26 – -0.07\n-0.26 – -0.07\n0.001\n0.001\n\n\nwork experience × CH\n-0.03\n-0.03\n-0.13 – 0.06\n-0.13 – 0.06\n0.492\n0.492\n\n\nwork experience × AT\n0.06\n0.05\n-0.04 – 0.15\n-0.04 – 0.15\n0.259\n0.259\n\n\nObservations\n610\n\n\nR2 / R2 adjusted\n0.030 / 0.022"
  },
  {
    "objectID": "03_Regression_in_R.html#alternative-ausgaben-für-die-regressionsanalysen-mit-pakten",
    "href": "03_Regression_in_R.html#alternative-ausgaben-für-die-regressionsanalysen-mit-pakten",
    "title": "3  Regression in R",
    "section": "3.1 Alternative Ausgaben für die Regressionsanalysen mit Pakten",
    "text": "3.1 Alternative Ausgaben für die Regressionsanalysen mit Pakten\nSie merken, es ist schon wieder alles so zerrissen. Gerne hätten wir doch eine Regressionsanalyse, wo alles drin ist, was wir wollen. Also auch die standardisierten Regressionskoeffizienten, die std. BETAS und vielleicht die Konfedenzintervalle für dieses BETAS. Das finden wir im Paket sjplot.\n\n\n# Mache eine Regressionstabelle \n# mit dem Paket sjPlot und dem Befehl tab_model()\nsjPlot::tab_model(Modell)\n\n\n\n\n \nY\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n5.64\n4.02 – 7.26\n<0.001\n\n\nX 2\n0.68\n0.41 – 0.96\n<0.001\n\n\nX 3\n0.44\n0.03 – 0.86\n0.037\n\n\nObservations\n100\n\n\nR2 / R2 adjusted\n0.478 / 0.468\n\n\n\n\n\n\nDie «Predictors» sind die Variablen, die Estimates sind die b. Das CI ist das Konfidenzintervall der b und p ist der p-Wert. Unter der Tabelle steht das Bestimmtheitsmass \\(R^2\\). Das sieht also schon ganz gut aus, aber ich hätte gerne die standardisierten Regressionskoeffizienten, die BETAS statt der b’s (Estimates).\n\n\n# Passe weiter an.\nsjPlot::tab_model(Modell, \n                  show.std = TRUE, # zeige die standardisierten Koeffizienten\n                  show.est = FALSE, # zeige die unstandardisierten estimates nicht\n                  show.r2 = TRUE # zeige R^2\n                  )\n\n\n\n\n \nY\n\n\nPredictors\nstd. Beta\nstandardized CI\np\n\n\n(Intercept)\n-0.00\n-0.14 – 0.14\n<0.001\n\n\nX 2\n0.51\n0.31 – 0.72\n<0.001\n\n\nX 3\n0.22\n0.01 – 0.43\n0.037\n\n\nObservations\n100\n\n\nR2 / R2 adjusted\n0.478 / 0.468\n\n\n\n\n\n\nJetzt stehen an der Stelle der «Estimates» die standardisierten Regressionskoeffizienten mit der Bezeichnung «std. Beta», was ein bischen doppelt gemoppelt ist, weil «BETA» die Kennung für die standardisierten Regressionskoeffizienten ist. Die Konfidenzintervalle CI beziehen sich jetzt auch auf die «std. Beta». Der p-Wert ist noch derselbe - da kann ja auch nichts anderes rauskommen. Mit einem anderen Paket können wir uns auch eine ausführlichere «Summary» erzeugen. Das ist dann sinnvoll, wenn Sie sowieso in Word ihre Arbeit schreiben und die Werte aus R in ordentliche Tabellen in Word eintragen (so gut es geht).\n\n# Sehr ausführliche Ausgaben Modellgüte und Koeffizienten.\nolsrr::ols_regress(Modell)\n##                         Model Summary                          \n## --------------------------------------------------------------\n## R                       0.692       RMSE                2.904 \n## R-Squared               0.478       Coef. Var          25.838 \n## Adj. R-Squared          0.468       MSE                 8.434 \n## Pred R-Squared          0.446       MAE                 2.465 \n## --------------------------------------------------------------\n##  RMSE: Root Mean Square Error \n##  MSE: Mean Square Error \n##  MAE: Mean Absolute Error \n## \n##                                ANOVA                                 \n## --------------------------------------------------------------------\n##                 Sum of                                              \n##                Squares        DF    Mean Square      F         Sig. \n## --------------------------------------------------------------------\n## Regression     750.123         2        375.061    44.469    0.0000 \n## Residual       818.117        97          8.434                     \n## Total         1568.240        99                                    \n## --------------------------------------------------------------------\n## \n##                                 Parameter Estimates                                  \n## ------------------------------------------------------------------------------------\n##       model     Beta    Std. Error    Std. Beta      t       Sig     lower    upper \n## ------------------------------------------------------------------------------------\n## (Intercept)    5.641         0.817                 6.908    0.000    4.020    7.261 \n##         X_2    0.682         0.140        0.515    4.890    0.000    0.405    0.959 \n##         X_3    0.443         0.210        0.222    2.109    0.037    0.026    0.860 \n## ------------------------------------------------------------------------------------\n\nDa haben wir jetzt mehr als wir brauche, finden aber fast alles, was wir suchen (leider TOL und VIF nicht). Da gucken wir uns gleich an."
  },
  {
    "objectID": "03_Regression_in_R.html#checks-der-voraussetzungen",
    "href": "03_Regression_in_R.html#checks-der-voraussetzungen",
    "title": "3  Regression in R",
    "section": "3.2 Checks der Voraussetzungen",
    "text": "3.2 Checks der Voraussetzungen\n\n3.2.1 Multikollinearität\nWir wissen aus der Konstruktion der Excel-Datei, dass die UVs X_2 und X_3 miteinander korrellieren. Das können wir uns einfach mal als Korrelation anschauen.\n\n# nehme die Daten des Datenobjekts \"DATEN\"\nDATEN |> # ... und pipe (|>, mache also \n                         # mit dem Arbeitsschritt der nächsten Zeile weiter)\n  cor()\n##             ID          Y       X_2       X_3\n## ID  1.00000000 0.06560959 0.1430166 0.1161319\n## Y   0.06560959 1.00000000 0.6740839 0.5913754\n## X_2 0.14301664 0.67408390 1.0000000 0.7174257\n## X_3 0.11613194 0.59137538 0.7174257 1.0000000\n\nIn der Korrelation kann man sehen, dass X_2 und X_3 recht stark korrelieren. Schön anzusehen ist das allerdings nicht. Darum gleich nochmal schöner:\n\nDATEN |> # nehme DATEN und ...\n  cor() |> # berechne die Korrelation und mache dann noch weiter\n  corrplot::corrplot(type = \"lower\", # nimm mal nur die unter der Diagonalen\n                     method = \"square\", # Quadrate statt Kreise\n         addCoef.col = 'black', # füge mal die Korrelationskoeffizienten hinzu\n         diag = FALSE , # Die Einsen in der Diagonale lass mal weg (Korrelationen der Variablen mit sich selbst)\n         )\n\n\n\n\nKorrelationen mit corrplot\n\n\n\n\nOK, also wie sehen, dass X_2 und X_3 stark miteinander korrelieren! Das muss Multikollinearität geben. Oben in dem Summary-Output der Regression konnten wir das noch nicht sehen. Leider produziert die Standard-lm die Toleranzwerte und VIF-Werte nicht. Wir können die aber anfordern:\n\n## Es gibt ein Paket \"olsrr\" für die Prüfung der OLS-Voraussetzungen, \n## wo man sich den VIF und die Toleranz rauslassen kann:s\n\n##  Dafür wird also das Paket \"olsrr\" benötigt. \n## Entweder sie werden direkt gefragt, ob Sie es installieren möchten \n## oder Sie gehen auf Tools -> Install Package und holen es sich. \n\nolsrr::ols_vif_tol(Modell) \n##   Variables Tolerance     VIF\n## 1       X_2 0.4853003 2.06058\n## 2       X_3 0.4853003 2.06058\n\nDie Toleranz ist das \\(R^2\\), wenn die jeweilige UV quasi als AV einer Regression genommen werden würde und alle übrigen UV als UVs. Der VIF ist der «Varianzinflationsfaktor» und schlicht der Kehrwert (1/\\(R^2\\)) der Toleranz.\n\n\n3.2.2 Homoskedastizität\nDie Annahme der Homoskedastizität bedeutet, dass die Streuung der Residuen, also der Werte um die Regressionsgerade nicht mit den vorhergesagten oder den UVs zusammenhängen sollte. Sie sollten immer recht gleichmässig (homoskedastisch) um vorhergesagten Werte (die Regressionsgerade) streuen. Es sollen keine kurvenartigen Gebilde sichtbar werden und keine Trichter in irgendeine Richtung.\n\n\n# Plotte die geschätzten Werte auf der Regressionsgeraden (Y-Hut) \n# auf der X-Achse und die Residuen auf der Y-Achse\nolsrr::ols_plot_resid_fit(Modell)\n\n\n\n\nPlot für Fit und Residuen\n\n\n\n\nWenn wir die Residuen gegen die geschätzten Werte plotten, dann deutet nichts auf einen kurvlinearen Verlauf hin. Wenn der da wäre, würden wir eine recht klare Kurvenverteilung sehen. So eine Beziehung gibt es in der Regel nicht einfach zufällig und da wir so etwas nicht in die Daten imputiert haben, ist es eben auch nicht drin. Gut so. Eine Art Trichter sehen wir auch nicht, also ist die Streuung der Fehler um die Gerade überfall ungefähr gleich.\nWir können uns auch Tests für die Voraussetzungen rausgeben lassen. Homoskedastizität kann mit dem Breusch-Pagan-Test geprüft werden.\n\n# Führe einen Breusch-Pagan-Test aus\nolsrr::ols_test_breusch_pagan(Modell)\n## \n##  Breusch Pagan Test for Heteroskedasticity\n##  -----------------------------------------\n##  Ho: the variance is constant            \n##  Ha: the variance is not constant        \n## \n##             Data              \n##  -----------------------------\n##  Response : Y \n##  Variables: fitted values of Y \n## \n##         Test Summary         \n##  ----------------------------\n##  DF            =    1 \n##  Chi2          =    0.0161833 \n##  Prob > Chi2   =    0.8987714\n\nDer ist in diesem Fall nicht signifikant. Oben steht, dass wenn die H0 beibehalten werden kann, dann wird die Varianz der Fehler als konstant über die Werte der gefitteten Werte der AV betrachtet. Ist ok.\n\n\n3.2.3 Normalverteilung der Residuen\nDie Normalverteilung der Residuen kann man sich mit dem Normal-Q-Q-Plot ansehen (manchmal auch nur Q-Q oder Resid-QQ). Wenn die Residuen perfekt normalverteilt wären, dann lägen die blauen Punkte auf der Geraden.\n\n\n# Führe einen Normal-Q-Q-Plot aus\nolsrr::ols_plot_resid_qq(Modell)\n\n\n\n\nNormal-Q-Q-Plot\n\n\n\n\nAlso gut, der Normal-Q-Q-Plot sagt deutlich, dass die Residuen nicht normalverteilt sind. Das liegt vor allem daran, dass die Verteilungen in der Excel mit einfachen Zufallszahlen gemacht wurden und nicht mit normalverteilten Variablen. Da wir auch nicht so viele Fälle haben (100), gibt es so eine Kurve. Das ist für die Tests, die bekannte Verteilungsannahmen annehmen, nicht so toll. Wir müssen mit den Interpretationen unserer t-Test und F-Tests mindestens vorsichtig sein. Also vielleicht bei einer p-Wert von .04 nicht gleich von klarer Signifikanz reden. Allerdings sind unsere Schätzer (b’s und \\(R^2\\)) recht robust gegen diese Verletzung. Das heisst, dass wir bei bei p-Werten kleiner .01 auch dann signifikante b’s interpretieren können, wenn diese Annahme etwas verletzt ist.\nAm besten, wir schauen uns das nochmal in einem Histogramm an, da kann man mehr sehen:\n\n\n# Mache mal ein Histogramm der Residuen. Die sollten annähernd normalverteilt sein. \nolsrr::ols_plot_resid_hist(Modell)\n\n\n\n\nHistogramm der Residuen\n\n\n\n\nUnd da sehen Sie die Ursache. Und sie sehen, najaaaa, sooo schlimm ist das doch nicht.\nWir können auch hier Tests machen.\n\n# Führe Tests auf signifikante Verletzungen \n# der Normalverteilungsannahme aus.\n\nolsrr::ols_test_normality(Modell)\n## Warning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): für den Komogorov-Smirnov-Test\n## sollten keine Bindungen vorhanden sein\n## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9525         0.0012 \n## Kolmogorov-Smirnov        0.0788         0.5645 \n## Cramer-von Mises          6.2278         0.0000 \n## Anderson-Darling          1.2635         0.0026 \n## -----------------------------------------------\n\nEs werden gleich mehrere rausgelassen, die unterschiedliche Eigenschaften haben und damit so ihre Stärken und Schwächen, wie jeder andere Mensch auch. Für Statistik-Aufbau soll uns mal reichen, dass wir ein Problem haben, wenn alle Tests zu dem Ergebnis kommen, dass wir ein Problem haben (alle p-Werte sind signifikant mit p < .05). Kein Problem haben wir, wenn ein Test sagt, dass es ok ist. In diesem Fall rettet uns Kolmogorov-Smirnov. Wir behalten aber im Hinterkopf, dass es ein Problem gibt und überinterpretieren die Werte nicht, wenn sie nahe der Grenze liegen. Damit ist gemeint, dass p-Werte deutlich unter .05 liegen sollten, damit wir von signifikanten Effekten sprechen. Bei knapp interpretierbaren Ergebnissen, muss man einfach zugeben, dass man Probleme mit den Voraussetzungen hat und aus den Daten keine Sicherheit ziehen kann.\n\nWelche der im folgenden genannten Voraussetzungen müssen geprüft werden, um ein lineares Modell rechnen bzw. unverzerrt auswerten zu können?\n\n\n\n\n keine zu starke Multikollinearität Homoskedastizität keine zu starke Covarianz zwischen den einzelnen UVs und der AV Monokausalität"
  },
  {
    "objectID": "05_Dimensionsreduktion.html#the-r-anxiety",
    "href": "05_Dimensionsreduktion.html#the-r-anxiety",
    "title": "5  Faktorenanalyse",
    "section": "5.1 The R Anxiety",
    "text": "5.1 The R Anxiety\nAls Beispiel wird hierfür die R-Angst-Skala von Andy Field [@Field2022] verwendet. Die Fragen und zugehörigen Variablen sind:\n\nraq_01: Statistics make me cry\nraq_02: My friends will think I’m stupid for not being able to cope with R\nraq_03: Standard deviations excite me\nraq_04: I dream that Pearson is attacking me with correlation coefficients\nraq_05: I don’t understand statistics\nraq_06: I have little experience of computers\nraq_07: All computers hate me\nraq_08: I have never been good at mathematics\nraq_09: My friends are better at statistics than me\nraq_10: Computers are useful only for playing games\nraq_11: I did badly at mathematics at school\nraq_12: People try to tell you that R makes statistics easier to understand but it doesn’t\nraq_13: I worry that I will cause irreparable damage because of my incompetence with computers\nraq_14: Computers have minds of their own and deliberately go wrong whenever I use them\nraq_15: Computers are out to get me\nraq_16: I weep openly at the mention of central tendency\nraq_17: I slip into a coma whenever I see an equation\nraq_18: R always crashes when I try to use it\nraq_19: Everybody looks at me when I use R\nraq_20: I can’t sleep for thoughts of eigenvectors\nraq_21: I wake up under my duvet thinking that I am trapped under a normal distribution\nraq_22: My friends are better at R than I am\nraq_23: If I am good at statistics people will think I am a nerd"
  },
  {
    "objectID": "05_Dimensionsreduktion.html#korrelationsmatrix",
    "href": "05_Dimensionsreduktion.html#korrelationsmatrix",
    "title": "5  Faktorenanalyse",
    "section": "5.2 Korrelationsmatrix",
    "text": "5.2 Korrelationsmatrix\nDie Korrelationsmatrix ist die Basis für Faktorenanalysen (im Grunde braucht man nur die Korrelationsmatrix (+ Fallzahl) und die ursprünglichen Daten nicht). Mit dem folgenden Befehlen kann man sich die Korrelationsmatrix rausgeben lassen.\n\n# Lade den Datensatz \"raq.csv\" aus dem Ordner discovr_csv, den man hier herunterladen kann: https://www.discovr.rocks/csv/discovr_csv.zip\nraq.tib <- readr::read_csv(\"data/discovr_csv/raq.csv\")\n## Rows: 2571 Columns: 24\n## ── Column specification ──────────────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (1): id\n## dbl (23): raq_01, raq_02, raq_03, raq_04, raq_05, raq_06, raq_07, raq_08, raq_09, raq_...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Lösche die Variable \"id\", die ganz vorne im Datensatz steht\nraq_items_tib <- raq.tib |> \n  select(-id)\n\n# Berechne die Korrelationen für alle Variablen mit allen Variablen (items)\nraq_cor <- raq_items_tib |> \n  cor()\n\n# Gebe einen Korrelationsplot mit dem Paket \"psych\" raus. \n#psych::corPlot(raq_cor, upper = FALSE)\n\n# Sortiere die Variablen danach, wie stark sie miteinander korrelieren\norder.FPC <- corrplot::corrMatOrder(raq_cor, order = 'FPC')\norder.hc <- corrplot::corrMatOrder(raq_cor, order = 'hclust')\n\n# Speichere die Ordnung als Matrix\nraq_cor.FPC <- raq_cor[order.hc, order.hc]\n\n# Schönere Korrelationsplots gibt es mit dem Paket \"corrplot\" und dem Befehl \"corrplot\"\ncorrplot::corrplot(raq_cor.FPC, tl.col='black', tl.cex=.75) \n\n\n\n\nKorrelationsplot\n\n\n\n\nMan sieht hier schon, dass die Korrelationen nicht wahnsinnig gross sind, aber sich wie Haufen bilden. Die Variablengruppen, die untereinander hoch korrelieren, gehen vermutlich auf ein gemeinsames latentes Konstrukt zurück. Diese latenten Konstrukte werden im Folgenden auch als Faktoren bezeichnet."
  },
  {
    "objectID": "05_Dimensionsreduktion.html#anzahl-faktoren-bestimmen",
    "href": "05_Dimensionsreduktion.html#anzahl-faktoren-bestimmen",
    "title": "5  Faktorenanalyse",
    "section": "5.3 Anzahl Faktoren bestimmen",
    "text": "5.3 Anzahl Faktoren bestimmen\nWenn wir eine Faktorenlösung suchen, müssen wir erstmal die Anzahl sinnvoller Faktoren bestimmen. Das geht mit dem «psych»-Paket und der Analyse «fa.parallel». Dort werden die Eigenwerte (eigen values) der Faktoren angezeigt. Die Eigenwerte sind der Anteil der Varianzaufklärung eines Faktors relativ zur Anzahl der Variablen in der Faktorenanalyse. Wenn also ein Eigenvalue bei 1 ist, erklärt ein Faktor so viel wie eine einzelne Variable.\n\n## Parallel analysis suggests that the number of factors =  4  and the number of components =  NA\n\n\n\n\nAnalyse zur Bestimmung der Faktoren (über roter Linie)\n\n\n\n\nIm Plot kann man sehen, dass der erste Faktor einen Eigenwert von knapp 6 hat, also so viel Varianz aufnimmt, wie sechs Variablen im Ursprung. Der zweite Faktor ist noch über 1. Das bedeutet, er erklärt etwas mehr als eine Ursprungsvariable. Der dritte und der vierte Faktor erklären etwas weniger als eine Variable. Da das alte Kaiser-Kriterium (Eigenwerte müssen über 1 sein) etwas sehr holzschnittartig ist, haben sich findige Statistiker ausgedacht, dass man die FA simulieren könnte, unter der Annahme, dass die Faktoren nichts erklären. Diese Simulation durch mehrfaches ziehen von Stichproben aus den Daten (FA Resampled Data) ergibt, dass 4 Faktoren mehr besser sind als die informationslose Simulation. Also ist die Faktorlösung 4."
  },
  {
    "objectID": "05_Dimensionsreduktion.html#faktorladungen-und-uniqueness",
    "href": "05_Dimensionsreduktion.html#faktorladungen-und-uniqueness",
    "title": "5  Faktorenanalyse",
    "section": "5.4 Faktorladungen und Uniqueness",
    "text": "5.4 Faktorladungen und Uniqueness\nMit dieser Analyse können wir jetzt die Faktorenanalyse rechnen. Als Anzahl «n» der Faktoren (nfactors) geben wir die 4 aus der Analyse von oben ein (siehe Abbildung @ref(fig:Parallelanalyse)).\nDie Uniqueness ist der Varianzanteil, den eine Variable ganz alleine hat, also nicht mit den anderen teilt. Die Uniqueness ist das Gegenteil von Kommunalität (Communality), also der gemeinsamen Varianz mit der Faktorenlösung. Rechnerisch ergibt sich je Variable die Uniqueness aus 1 - Kommunalität. Hohe Uniqueness bedeutet, dass eine Variable nicht gut in die Faktorenanalyse passt, weil sie eben nicht gut durch die Faktoren abgebildet wird, sondern einzigartig (unique) ist. Für die Variable selbst und ggf. für ihre Integration in ein Modell ist eine hohe Uniqueness gut, da sie auch bedeutet, dass es keine Probleme mit Multikollinearität gibt. Die Variable kann also getrost aus der Faktorenanalyse entfernt und als eigenständige Variable in eine Modell aufgenommen werden.\nDie Complexity gibt an, wie viele Faktoren gebraucht werden, um die Variable abzubilden. Wenn sie 1 ist, dann wird eine Variable von einem Faktor abgebildet. Ist sie zum Beispiel 1.97 braucht es zwei Faktoren, um die Variable darzustellen. Geringe Komplexität ist in dem Fall gut, da sie zu einer klaren Faktorenlösung führt.\n\n## Loading required namespace: GPArotation\n## # Rotated loadings from Factor Analysis (oblimin-rotation)\n## \n## Variable | MR1  | MR2  |  MR4  | MR3  | Complexity | Uniqueness\n## ---------------------------------------------------------------\n## raq_06   | 0.84 |      |       |      |    1.00    |    0.27   \n## raq_18   | 0.63 |      |       |      |    1.03    |    0.57   \n## raq_13   | 0.57 |      |       |      |    1.02    |    0.66   \n## raq_07   | 0.56 |      |       |      |    1.02    |    0.65   \n## raq_10   | 0.49 |      |       |      |    1.08    |    0.74   \n## raq_15   | 0.48 |      |       |      |    1.04    |    0.71   \n## raq_05   | 0.45 |      | 0.39  |      |    1.97    |    0.46   \n## raq_14   | 0.42 |      |       |      |    1.06    |    0.78   \n## raq_09   |      | 0.81 |       |      |    1.02    |    0.38   \n## raq_23   |      | 0.79 |       |      |    1.02    |    0.41   \n## raq_19   | 0.26 | 0.56 |       |      |    1.41    |    0.50   \n## raq_22   |      | 0.52 |       |      |    1.29    |    0.59   \n## raq_02   | 0.25 | 0.48 |       |      |    1.54    |    0.62   \n## raq_21   |      |      | 0.59  |      |    1.04    |    0.60   \n## raq_04   |      |      | 0.56  |      |    1.01    |    0.67   \n## raq_20   |      |      | 0.54  |      |    1.02    |    0.68   \n## raq_16   |      |      | 0.51  |      |    1.02    |    0.77   \n## raq_03   |      |      | -0.43 |      |    1.01    |    0.80   \n## raq_01   |      |      | 0.39  |      |    1.06    |    0.83   \n## raq_12   |      |      | 0.37  |      |    1.07    |    0.89   \n## raq_08   |      |      |       | 0.88 |    1.00    |    0.25   \n## raq_11   |      |      |       | 0.72 |    1.00    |    0.45   \n## raq_17   |      |      |       | 0.68 |    1.00    |    0.51   \n## \n## The 4 latent factors (oblimin rotation) accounted for 40.12% of the total variance of the original data (MR1 = 13.23%, MR2 = 9.76%, MR4 = 8.83%, MR3 = 8.29%)."
  },
  {
    "objectID": "05_Dimensionsreduktion.html#interpretation-der-faktorenanalyse",
    "href": "05_Dimensionsreduktion.html#interpretation-der-faktorenanalyse",
    "title": "5  Faktorenanalyse",
    "section": "5.5 Interpretation der Faktorenanalyse",
    "text": "5.5 Interpretation der Faktorenanalyse\nMit dieser Faktorenlösung können wir jetzt die Faktoren interpretieren.\nDer erste Faktor lädt hoch auf folgenden Items. Wir können diesen Faktor als l «Probleme mit Computern»** labeln:\n\nraq_05: I don’t understand statistics\nraq_06: I have little experience of computers\nraq_07: All computers hate me\nraq_10: Computers are useful only for playing games\nraq_13: I worry that I will cause irreparable damage because of my incompetence with computers\nraq_14: Computers have minds of their own and deliberately go wrong whenever I use them\nraq_15: Computers are out to get me\nraq_18: R always crashes when I try to use it\n\nBeachte: Das Item «raq_05» lädt auch hoch auf dem zweiten Faktor MR2.\nWenn man die Fragen anschaut, die hoch auf dem zweiten Faktor MR2 laden, deuten darauf hin, dass es die Befragten Angst haben, von ihren Peers komisch angesehen zu werden. Nennen wir diesen Faktor «Angst vor sozialer Bewertung»:\n\nraq_02: My friends will think I’m stupid for not being able to cope with R\nraq_09: My friends are better at statistics than me\nraq_19: Everybody looks at me when I use R\nraq_22: My friends are better at R than I am\nraq_23: If I am good at statistics people will think I am a nerd\n\nBeim Faktor MR3 wird deutlich, dass es hier eine Angst vor Statistik gibt. Nennen wir den Faktor «Angst vor Stastik»:\n\nraq_01: Statistics make me cry\nraq_03: Standard deviations excite me\nraq_04: I dream that Pearson is attacking me with correlation coefficients\nraq_05: I don’t understand statistics\nraq_12: People try to tell you that R makes statistics easier to understand but it doesn’t\nraq_16: I weep openly at the mention of central tendency\nraq_20: I can’t sleep for thoughts of eigenvectors\nraq_21: I wake up under my duvet thinking that I am trapped under a normal distribution\n\nBei den übrigen Fragen, die auf dem Faktor MR4 laden, geht es eher um Mathematik. Wir könnten also sagen der Faktor MR4 ist «Angst vor Mathe».\n\nraq_08: I have never been good at mathematics\nraq_11: I did badly at mathematics at school\nraq_17: I slip into a coma whenever I see an equation\n\nWir können noch schauen, ob die Variablen mit Doppelladungen plausibel sind. Also schauen wir zum Beispiel auf das Item raq_05 «I don’t understand statistics». Das scheint mit einer geringen Selbstwirksamkeit in Bezug auf Computer und Statistik zusammenzuhängen. Es spiegelt die Selbsteinschätzung wieder, dass man Statistik und Computer «nicht kann»."
  },
  {
    "objectID": "05_Dimensionsreduktion.html#faktorendiagramm",
    "href": "05_Dimensionsreduktion.html#faktorendiagramm",
    "title": "5  Faktorenanalyse",
    "section": "5.6 Faktorendiagramm",
    "text": "5.6 Faktorendiagramm\nFaktorenanalysen kann man mit solchen Diagrammen darstellen. Hier sieht man auch, wie stark die einzelnen Faktoren miteinander korrelieren, wenn man die Faktoren nicht gezwungen hat, orthogonal zu sein, also unkorrliert.\n\n\n\n\n\nFaktorendiagramm\n\n\n\n\n\n## Warning: ggrepel: 15 unlabeled data points (too many overlaps). Consider increasing\n## max.overlaps\n## Saving 6 x 4 in image\n## Warning: ggrepel: 14 unlabeled data points (too many overlaps). Consider increasing\n## max.overlaps\n\n\n\n\nVariablen PCA"
  },
  {
    "objectID": "07_ML.html#daten-einlesen",
    "href": "07_ML.html#daten-einlesen",
    "title": "6  Machine Learning",
    "section": "6.1 Daten einlesen",
    "text": "6.1 Daten einlesen\nHier können Sie den Datensatz und die Beschreibung der Daten finden. Suchen und speichern Sie den Datensatz «train.csv».\nDownlaod der Daten: https://www.kaggle.com/competitions/titanic/data\n\n##   PassengerId       Survived          Pclass          Name               Sex           \n##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891         Length:891        \n##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character   Class :character  \n##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character   Mode  :character  \n##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                                        \n##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                                        \n##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                                        \n##                                                                                        \n##       Age            SibSp           Parch           Ticket               Fare       \n##  Min.   : 0.42   Min.   :0.000   Min.   :0.0000   Length:891         Min.   :  0.00  \n##  1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000   Class :character   1st Qu.:  7.91  \n##  Median :28.00   Median :0.000   Median :0.0000   Mode  :character   Median : 14.45  \n##  Mean   :29.70   Mean   :0.523   Mean   :0.3816                      Mean   : 32.20  \n##  3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000                      3rd Qu.: 31.00  \n##  Max.   :80.00   Max.   :8.000   Max.   :6.0000                      Max.   :512.33  \n##  NA's   :177                                                                         \n##     Cabin           Embarked     Kinder            Age_z         Pclass_f    Cabin_D     \n##  Length:891         S   :644   Mode :logical   Min.   :-29.279   3:491    Min.   :0.000  \n##  Class :character   C   :168   FALSE:643       1st Qu.: -9.574   2:184    1st Qu.:0.000  \n##  Mode  :character   Q   : 77   TRUE :71        Median : -1.699   1:216    Median :0.000  \n##                     NA's:  2   NA's :177       Mean   :  0.000            Mean   :0.229  \n##                                                3rd Qu.:  8.301            3rd Qu.:0.000  \n##                                                Max.   : 50.301            Max.   :1.000  \n##                                                NA's   :177\n\nDie PassengerId ist einefach eine Identifikationsnummer. Es gibt dann eine Variable, die «Survived» heisst, die ein Minimum von 0 hat und ein Maximum von 1. Das deutet sehr auf eine Dummy hin. Da der Durchschnitt («Mean») = 0.38 ist, wissen wir jetzt schon, dass 38 Prozent der Passagiere überlebt haben (der Mittelwert einer Dummy ist immer der Prozentsatz der 1er-Gruppe). Dann kommt noch der Name als Zeichenvariable, das Alter, das von 0.42 bis 80 geht. Von 177 Personen fehlen die Altersangaben. Bei den übrigen Variablen muss man nochmal nachschauen auf «kaggle». Dort steht:\n\n\n\n\n \n  \n    Variable \n    Definition \n    Key \n  \n \n\n  \n    survival \n    Survival \n    0 = No, 1 = Yes \n  \n  \n    pclass \n    Ticket class \n    1 = 1st, 2 = 2nd, 3 = 3rd \n  \n  \n    sex \n    Sex \n     \n  \n  \n    Age \n    Age in years \n     \n  \n  \n    sibsp \n    # of siblings / spouses aboard the Titanic \n     \n  \n  \n    parch \n    # of parents / children aboard the Titanic \n     \n  \n  \n    ticket \n    Ticket number \n     \n  \n  \n    fare \n    Passenger fare \n     \n  \n  \n    cabin \n    Cabin number \n     \n  \n  \n    embarked \n    Port of Embarkation \n    C = Cherbourg, Q = Queenstown, S = Southampton"
  },
  {
    "objectID": "07_ML.html#daten-in-trainings--und-testdaten-aufteilen",
    "href": "07_ML.html#daten-in-trainings--und-testdaten-aufteilen",
    "title": "6  Machine Learning",
    "section": "6.2 Daten in Trainings- und Testdaten aufteilen",
    "text": "6.2 Daten in Trainings- und Testdaten aufteilen\nWenn von Machine Learning (ML) die Rede ist, dann wird (wenn es um supervised learning geht) zunächst ein Modell an Trainingsdaten trainiert bzw. angelernt und später an Testdaten getestet. Darum nehmen wir den vorliegenden Datensatz mal auseinander und teilen die Fälle (Passagiere) zufällig den Trainingsdaten zu und später zu verwendenden Testdaten. In der Regel wird der Trainingsdatensatz grösser gewählt: Ich habe ihn auf 75% des Ursprungsdatensatzes festgelegt. Der Rest (#anti_join) wird für später als Testdatensatz aufbewahrt.\n\n\n\nSehen kann man nach diesem r-Chunk übrigens nichts, weil nur Datensätze im Hintergrund aufgeteilt wurden. Also suchen wir mal nach guten Datenvisualisierungen.\n\n6.2.1 Datenvisualisierung\nEine einfache Darstellungsmöglichkeit ist ein sogenannter «Scatterplot» der die Lage von Fällen in ein Koordinatensystem einteilt, das durch zwei Variablen gebildet wird. Im Beispiel (Abbildung @ref(titanic-Datenvisualisierungen)) ist es das Alter der Passagiere «Age» und der Fahrpreis «Fare». Als Zweites haben wir ein Balkendiagramm für die Passagierklassen. Im letzten sehr aufwendigen Plotgrafik werden die Zweierbeziehungen aller Variablen dargestellt, also wie sie miteinander korrelieren (obere Nebendiagonalen), wie ihre Verteilung ist (auf der Diagonale mit Namen) und wie ihre gemeinsame Streuung ist, also ein Scatterplot in der unteren Nebendiagonalen. Mehr zu diesen SPLOM finden Sie hier: https://cran.r-project.org/web/packages/psych/vignettes/intro.pdf.\n\n## Warning: Removed 177 rows containing missing values (`geom_point()`).\n## Warning in cor(x, y, use = \"pairwise\", method = method): the standard deviation is zero\n\n## Warning in cor(x, y, use = \"pairwise\", method = method): the standard deviation is zero\n\n\n\n\nDatenvisualisierungen\n\n\n\n\n\n\n\nDatenvisualisierungen\n\n\n\n\n\n\n\nDatenvisualisierungen"
  },
  {
    "objectID": "07_ML.html#modellbildung-für-den-fahrpreis",
    "href": "07_ML.html#modellbildung-für-den-fahrpreis",
    "title": "6  Machine Learning",
    "section": "6.3 Modellbildung für den Fahrpreis",
    "text": "6.3 Modellbildung für den Fahrpreis\nJetzt kommt das erste Modell. Wenn Sie mit diesem Syntax experimentieren, dann kopieren Sie sich mal die Zeile für das Modell, löschen aus von «Age_z» … bis «Kinder» heraus und schätzen Sie mal das mit den summarys. Schauen Sie sich die gut an und achten Sie darauf, was passiert, wenn Sie die Summanden für I(Age_z^2) usw. wieder in das Modell tun. Am Ende können Sie versuchen das Modell durch weitere Variablen ergänzen und verbessern oder andere Teile wieder herausnehmen. 1.\nIn der Summary des fits_titanic sehen wir zunächst ganz oben die Formel. Da der Preis schnell hoch ging, wird für den Preis mit «log» der natürliche Logarithmus gebildet (Es wird Fare + 1 gerechnet, weil der log für 0 nicht definiert ist und für einige Passagiere angegeben ist, dass ihr Fahrpreis 0 war.) Das Geschlecht ist eine Dummyvariable mit 1 für männlich. Das Alter ist eine zentrierte Variante der Variable Age. Die Zentrierung machen wir, weil die resultierende zentrierte Variable nicht mehr sehr stark mit ihrer quadrierten Version. Den Befehl für die Zentrierung finden Sie in der «Datenaufbereitung.Rmd». Dann folgt der etwas komische Ausdruck für die quadrierte Version der Altersvariable «I(Age_z^2)». Die Quadrierung machen wir, weil die Beziehung zwischen Alter und Fahrpreis vermutlich nicht linear ist, sondern kurvilinear quadratisch (Alter hat oft einen quadratischen Einfluss, weil in der Regel Ältere und Jüngere etwas weniger Geld haben als die mittlere Altersgruppe). Wenn nur das Alter als quadratische Funktion in der Gleichung wäre, dann müsste die quadratische Funktion einer um 0 zentrierten Variable immer wie eine summetrische Schüssel aussehen, die um 0 liegt. In der Regel ist die Schüssel aber gekippt. Dafür wird noch gebraucht, dass die Altersvariable «Age_z» auch noch in ihrer nichttransformierten Form Teil der Gleichung ist.\nWir lesen nun in der Tabelle die b, die in der Spalte «Estimate» stehen. Der Intercept hat ein \\(b_1\\) von 2.66 und keiner weiss, wie man das interpretieren soll. Ist aber auch nicht wichtig. Dann kommt schon die Variable «Sex» für das Geschlecht mit 1 für «männlich». Wir können hier sehen, dass männliche Mitreisende einen tieferen Fahrpreis gezahlt haben und zwar das auch signifikant, da der Pr(>|t|) bzw. einfach der p-Wert kleiner ist als .05 (Die «wissenschaftliche» Schreibweise ist etwas mühsam zu lesen. Der p-Wert für Sexmale ist 0.00000763). Der Einfluss des linearen Alters ist klein, negativ und nicht signifikant. In der Stichprobe findet es sich also, dass die Regressionsgerade für das Alter leicht nach unten schräg ist. Die «Schüssel», die vom nachfolgenden quadrierten Alter gebildet wird, neigt also leicht nach rechts. Da das quadrierte Alter auch negativ ist, sieht es aus als wäre die Beziehung zwischen Alter und Fahrpreis ein umgekehrter Bogen einer quadratischen Funktion, die sehr flach ist und rechts ein bischen stärker nach unten gebogen als links. Allerdings ist auch der Einfluss des quadrierten Alters nicht signifikant. Probieren Sie es mal ohne das quadrierte Alter, was dann passiert.\nDann folgt die Variable «Survived», die anscheinend angibt, dass Passagiere, die überlebt haben, etwas weniger zahlen mussten. Das ist intuitiv und theoretisch natürlich Quatsch, da hier die Kausalität zeitlich auf den Kopf gestellt wird. Die Varialbe müsste aus einem seriösen Modell wieder raus. Gut also, dass Sie das hier gelesen haben.\nDann kommen noch zwei Varialben für die Passagierklasse «Pclass». Hier hat R automatisch die Variable Pclass, die drei Ausprägungengen hatte in drei Dummys aufgeteilt, wobei die Ausprägung die im Level des Faktors an erster Stelle steht, als Referenzkategorie genommen wird, was im Titanicmodell die Klasse 3 ist. Wäre die auch im Modell, hätten wir perfekte Multikollinearität, weil wir immer schon wüssten, in welcher Klasse jemand eingecheckt sein musste, wenn es nicht die «first class» oder «second class» war. Wir sehen nun, dass die Passagiere der zweiten Klasse mit einem b von 0.49 einen signifikant höheren Preis bezahlt haben als die Personen der 3. Klasse. Der Unterschied ist vergleichsweise stark, was man daran erkennen kann, dass der t-Wert vergleichsweise hoch ist bei 6.859. Allerdings ist der t-Wert für den Unterschied zwischen 3. Klasse und 1. Klasse noch viel grösser. Für die «first class» musste also noch deutlich mehr gezahlt werden. Interessanterweise haben auch Kinder etwas mehr bezahlt als Erwachsene, obwohl die Zugehörigkeit der Klasse schon rausgerechnet ist. Offenbar waren Kinder auf dem Schiff eher besser untergebracht als der Schnitt oder wie würden Sie sich das erklären?\nDer Summary-Befehl gibt keinen sehr guten Output raus und lässt sich auch nicht gut anpassen. Für einen Abdruck in einem Forschungsbericht eignet sich das noch weniger. Daher versuchen wir hier eine schönere Ausgaben hinzubekommen\n\n6.3.1 Regressionsoutput\nMit der Funktion von sjPlot::tab_model können wir die standardisierten Regressionkoeffizienten rausgeben lassen (und sie auch als std. b bezeichnen). Zudem werden uns ein Konfidenzintervall «CI» für die b rausgegeben und eine «standardized CI» für die standardisierten Regressionskoeffizienten. Speziell ist, dass einfach p rausgegeben werden und standardisierte p. R gibt dazu als Hinweis, dass die p-Werte hier unteschiedlich sein können, weil in der Formel logarithmen und quadrierte Beziehungen drin sind. Im Zweifel interpretieren Sie besser die einfachen p und nicht die standardisierten «std. p».\n\n## Formula contains log- or sqrt-terms.\n##   See help(\"standardize\") for how such terms are standardized.\n\n\n\n\n \nlog(Fare+1)\n\n\nPredictors\nb\nstd. b\nCI\nstandardized CI\np\nstd. p\n\n\n(Intercept)\n2.66\n0.83\n2.51 – 2.81\n0.80 – 0.87\n<0.001\n<0.001\n\n\nSex [male]\n-0.32\n-0.06\n-0.45 – -0.18\n-0.10 – -0.02\n<0.001\n0.003\n\n\nAge z\n-0.00\n-0.02\n-0.01 – 0.00\n-0.05 – 0.00\n0.314\n0.063\n\n\nAge z^2\n-0.00\n-0.00\n-0.00 – 0.00\n-0.02 – 0.01\n0.685\n0.639\n\n\nSurvived\n-0.08\n-0.00\n-0.22 – 0.07\n-0.02 – 0.02\n0.294\n0.914\n\n\nPclass f [2]\n0.49\n0.06\n0.35 – 0.63\n0.02 – 0.10\n<0.001\n0.004\n\n\nPclass f [1]\n1.78\n0.44\n1.63 – 1.93\n0.39 – 0.48\n<0.001\n<0.001\n\n\nKinderTRUE\n0.61\n0.05\n0.27 – 0.95\n-0.04 – 0.15\n<0.001\n0.272\n\n\nObservations\n536\n\n\nR2 / R2 adjusted\n0.578 / 0.572\n\n\n\n\n\n\nAls auch nicht ganz schlechte Alternative kann gtsummary::tbl_regression benutzt werden, was durch weitere gepipte Befehle angepasst und ergänzt werden kann. Hier sind die b zu sehen und deren CI, sowie die p-Werte und gleich noch zwei Varianzinflationswerte. Wenn Sie schauen wollen, ob es ein Problem gibt, helfen die «Adjusted GVIF» schon, weil sie für grössere Stichproben kleiner sind als die VIF und damit die Probleme durch Multikollinearität angemessener abbilden [@Fox1992]. Ein Adjusted GVIF wird unangenehm, wenn es über 2 ist, also der VIF etwa bei 4. Schön an der Tabelle @ref(tab:gtsummary-tabelle) ist auch, dass immer auch die Referenzkategorie mit ausgewiesen wird, auch wenn sie immer die Ausstriche für die Tabellen aufweisen, wie zB bei Sex:female. Unter der Tabelle stehen noch einige Gütemasse für das Modell, wie \\(R^2\\) (.578) und die zugehörige F-Statistik als p-Wert (schön als <0.001, wobei ich die führende 0 weglassen würde, wenn die Werte praktisch immer etwas mit nach dem Komma sind).\nRecht blöd finde ich an gtsummary, dass die unstandardisierten Regressionskoeffizienten als «beta» bezeichnet werden und die standardisierten Regressionskoeffizienten (die hin und wieder auch als «BETA» bezeichnet werden) garnicht in die Ausgabe gepackt werden können.\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      b\n      95% CI1\n      p-value\n      GVIF1\n      Adjusted GVIF2,1\n    \n  \n  \n    Sex\n\n\n\n1.5\n1.2\n        female\n—\n—\n\n\n\n        male\n-0.32\n-0.45, -0.18\n<0.001\n\n\n    Age_z\n0.00\n-0.01, 0.00\n0.3\n3.1\n1.8\n    I(Age_z^2)\n0.00\n0.00, 0.00\n0.7\n2.5\n1.6\n    Survived\n-0.08\n-0.22, 0.07\n0.3\n1.7\n1.3\n    Pclass_f\n\n\n\n1.4\n1.1\n        3\n—\n—\n\n\n\n        2\n0.49\n0.35, 0.63\n<0.001\n\n\n        1\n1.8\n1.6, 1.9\n<0.001\n\n\n    Kinder\n\n\n\n3.5\n1.9\n        FALSE\n—\n—\n\n\n\n        TRUE\n0.61\n0.27, 0.95\n<0.001\n\n\n  \n  \n    \n      R² = 0.578; Adjusted R² = 0.572; Sigma = 0.641; Statistic = 103; p-value = <0.001; df = 7; Log-likelihood = -518; AIC = 1,054; BIC = 1,092; Deviance = 217; Residual df = 528; No. Obs. = 536\n    \n  \n  \n    \n      1 CI = Confidence Interval, GVIF = Generalized Variance Inflation Factor\n    \n    \n      2 GVIF^[1/(2*df)]\n    \n  \n\n\n\n\nBevor wir unsere Therie anhand unserer Ergebnisse auf den Prüfstand stellen können, müssen wir die Voraussetzungen für die Regressionsmodelle überprüfen.\n\n\n6.3.2 Voraussetzungschecks\nDie möglichen Verletzungen der Voraussetzungen sind:\n\nMultikollinearität\nDeutliche Abweichung von der Normalverteilung in den Fehlern\nHeteroskedastizität\n\n\n\n6.3.2.1 Multikollinearität\nDie Toleranzen bei Age_z und I(Age_z^2) sind nur knapp über .3 und knapp .4, also schon recht klein, auch wenn die VIF noch nicht zu sehr quietschen. Da aber beide Altersvariablen keine signifikanten Werte gezeigt haben, würde ich eine der beiden eliminieren und zwar das quadrierte Alter «I(Age_z^2)». Dann sollte sich auch der sehr kleine Toleranzwert bei «KinderTRUE» erledigt haben bzw. kleiner sein - vielleicht auch nicht. Schauen Sie sich das mal an! Wenn Age_z auch ohne die quadrierte Altersvariable nicht signifikant ist, würde ich nur «KinderTRUE» im Modell lassen. Könnte man eine Faktorenanalyse für die zwei aufgrund der Altersvariablen gebildeten Variablen machen? Neeeee! Was soll da für ein latenter Faktor rauskommen? Wieder das Alter? Das macht keinen Sinn. Die Variable «Kinder» ist ja direkt aus dem Alter bestimmt worden. Da brauchen wir nicht nach latenten Faktoren zu suchen.\n\n## Es gibt ein Paket \"olsrr\" für die Prüfung der OLS-Voraussetzungen, \n## wo man sich den VIF und die Toleranz rauslassen kann:s\n\nolsrr::ols_vif_tol(fit_titanic) \n##    Variables Tolerance      VIF\n## 1    Sexmale 0.6817534 1.466806\n## 2      Age_z 0.3252169 3.074871\n## 3 I(Age_z^2) 0.3995618 2.502742\n## 4   Survived 0.5967148 1.675842\n## 5  Pclass_f2 0.8292064 1.205972\n## 6  Pclass_f1 0.6550188 1.526674\n## 7 KinderTRUE 0.2878347 3.474217\n\n\n\n6.3.2.2 Residualplot\n\n\n# Mache mal ein Histogramm der Residuen. Die sollten annähernd normalverteilt sein. \nolsrr::ols_plot_resid_hist(fit_titanic)\n\n\n\n\nHistogramm der Residuen\n\n\n\n\nIm Residualplot ohne den Logarithmus für die AV «Fare» war schon ganz schön schief. Hier sieht es etwas besser aus mit der Verteilung. Das ist ja schon fast normalverteilt, auch wenn die mittlere Kategorie wie ein Stinkefinger in der Landschaft steht. Schauen wir mal den N-Q-Q-Plot an, wie der aussieht …\n\n\n6.3.2.3 N-Q-Q\n\n\n# Führe einen Normal-Q-Q-Plot aus\nolsrr::ols_plot_resid_qq(fit_titanic)\n\n\n\n\nNormal-Q-Q-Plot\n\n\n\n\nOK, es gibt rechts eine paar Werte über der Referenzlinie, die für die Normalverteilung steht und links ein paar unter der Referenzlinie. Das heisst, die Normalverteilung ist nicht perfekt getroffen. Wir sollten also zunächst die Resultate unseres Modells nicht überinterpretieren. Es gibt noch viele Anpassungen, mit denen man diese kleineren Verletzungen der Voraussetzungen für die Regression auflösen kann. Das geht aber für die Flughöhe dieser Veranstaltung zu hoch bzw. zu tief, wie Sie wollen.\n\n\n6.3.2.4 Heteroskedastizität\n\n\n# Plotte die geschätzten Werte auf der Regressionsgeraden (Y-Hut) \n# auf der X-Achse und die Residuen auf der Y-Achse\nolsrr::ols_plot_resid_fit(fit_titanic)\n\n\n\n\nPlot für Fit und Residuen\n\n\n\n\nEs gibt auch hier eine gewisse Heteroskedastizität, aber eigentlich sind die Werte schon relativ gleichmässig um 0 verteilt. Wir können das ja mal testen.\n\n# Führe einen Breusch-Pagan-Test aus \nolsrr::ols_test_breusch_pagan(fit_titanic)\n## \n##  Breusch Pagan Test for Heteroskedasticity\n##  -----------------------------------------\n##  Ho: the variance is constant            \n##  Ha: the variance is not constant        \n## \n##                   Data                    \n##  -----------------------------------------\n##  Response : log(Fare + 1) \n##  Variables: fitted values of log(Fare + 1) \n## \n##          Test Summary           \n##  -------------------------------\n##  DF            =    1 \n##  Chi2          =    51.25669 \n##  Prob > Chi2   =    8.104396e-13\n\nOha, Breusch und Pagan finden, dass unsere Varianz der Residuen heftig heterogen ist. Der p-Wert des Chi2 ist sehr weit von 0 entfernt. Das ist allerdings schnell so, wenn die Stichprobe recht gross ist. Dann wird irgendwann jedes Chi2 signifikant. Also auch hier: Wir gehen vorsichtig mit den Ergebnissen um, interpretieren nicht exakt die Nachkommastellen von bs und sagen auch bei einem p-Wert von .03, dass die Signifikanz hier nicht ganz klar ist (vor dem Hintergrund, dass einige Voraussetzungen verletzt sind).\n\n\n\n6.3.3 Algorithmus für den Fahrpreis\nWir haben jetzt also ein Modell und festegstellt, dass unsere Regression mit der log-Transformation für die AV und einer quadrierten UV nicht super durch die Prüfung der Voraussetzungen kommt. Also sind wir vorsichtig, lassen aber trotzdem mal einen Schätzalgorithmus für den Fahrpreis raus. Der Algorithmus ist schon da: Es ist der Fit des Modells. Dieses gefittete Modell können wir jetzt auf den Testdatensatz ansetzen und mal schauen, wie gut das Modell zu den Testdaten passt, anhand derer es nicht gebaut wurde, die aber auch die Outcomes enthalten, also den Fahrpreis.\nWir berechnen als vorhergesagte Werte die «preds» mit predict(fit_titanic, test). Dann Binden wir die an den Test-Datensatz «test», wobei wir dort auch noch schnell den natürlichen Logarithmus für «Fare» bilden, indem wir log(test$Fare) einsetzen, mit cbind (Spalten zusammenbinden) zusammenfassen und als tibble (tidydatentabelle) speichern. In der Grafik sieht man, das die Prognosen nicht perfekt sind, aber ok. Wenn Sie es mal ohne den schwieriger zu interpretierenden log machen, dann sehen Sie spätestens hier die Probleme, weil die hohen Fahrpreise schwer kalkuliert werden können.\n\n## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair`\n## is omitted as of tibble 2.0.0.\n## ℹ Using compatibility `.name_repair`.\n## Warning: Removed 45 rows containing missing values (`geom_point()`).\n\n\n\n\nJetzt hätten Sie also einen Algorithmus, den Sie nicht nur auf Test-Daten anlegen könnten, sondern an jede andere Konstellation von Daten, die die UVs enthält. Sie könnten also mit neuen Daten über predict(fit_titanic, neudaten) festlegen, was jede Person im Datensatz schätzungsweise für einen Preis für die Titanicüberfahrt gezahlt hätte. Das Modell ist nicht perfekt, aber Sie könnten es mit etwas Nachsteuern fair gestalten und jedem sagen, dass das die beste Anlehnung an die damalige (sicher eher analogen) Preisgestaltung ist.\nDas ist zwar nicht völlkommen überflüssig, aber wenn wir an «Titanic» denken, denken wir an Leonardo DiCaprio und an das Überleben und Sterben vor dem Fernseher und natürlich damals auf der Titanic. Damit haben wir nicht gleich angefangen, weil das «Überleben» eine dichotome Variable ist und damit eine Dummy als AV. Das macht das Ganze schon etwas komplizierter, aber klar, schauen wir uns das an."
  },
  {
    "objectID": "07_ML.html#überlebensprognose",
    "href": "07_ML.html#überlebensprognose",
    "title": "6  Machine Learning",
    "section": "6.4 Überlebensprognose",
    "text": "6.4 Überlebensprognose\nWenn man an die Titanic denkt, grübelt man in der Regel nicht lange, wie wohl die Preise auf der Titanic waren. Viel mehr ist «Titanic» mit dem Schiffsunglück verbunden (ok und mehr oder weniger guten Verfilmungen). Wenn wir von dem Unglück etwas lernen wollen («Learning from Desaster»), dann ist es sinnvoll, Prognosemodelle für die Überlebenswahrscheinlichkeit zu machen. Die Überlebenschancen für verschiedene Personengruppen auf der Titanic ergeben sich daraus, wie viele Personen der Gruppen überlebt haben. Ein Erklärungsmodell hat also zur abhängigen Variable (AV), ob eine Person überlebt hat (1) oder nicht (0). Die AV ist also eine Dummyvariable. Wenn die AV eine Dummyvariable ist, dann verweigert es R nicht, eine lineare Regression zu rechnen (das macht es ein bischen «gefährlich», weil viele Kolleg:innen und Reviewer:innen normale lineare Regressionen bei Dummys in der AV als grossen Spezifikations-Fehler betrachten).\n\n## \n## Call:\n## lm(formula = Survived ~ Pclass_f + Sex + Age_z + I(Age_z^2) + \n##     Kinder, data = train)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.96849 -0.25350 -0.06132  0.22493  0.97743 \n## \n## Coefficients:\n##                Estimate  Std. Error t value  Pr(>|t|)    \n## (Intercept)  0.57899126  0.03753055  15.427   < 2e-16 ***\n## Pclass_f2    0.17998008  0.04200296   4.285 0.0000217 ***\n## Pclass_f1    0.37609658  0.04287552   8.772   < 2e-16 ***\n## Sexmale     -0.50783615  0.03528671 -14.392   < 2e-16 ***\n## Age_z       -0.00292748  0.00200019  -1.464     0.144    \n## I(Age_z^2)  -0.00001618  0.00008790  -0.184     0.854    \n## KinderTRUE   0.12521089  0.10287435   1.217     0.224    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.382 on 529 degrees of freedom\n##   (132 observations deleted due to missingness)\n## Multiple R-squared:  0.4033, Adjusted R-squared:  0.3965 \n## F-statistic: 59.59 on 6 and 529 DF,  p-value: < 2.2e-16\n\nDie Werte sind jedenfalls nicht intuitiv interpretierbar, weil die AV nur 0 und 1 annehmen kann und keine Zwischenwerte. Zudem streuen die Fehler stark um die Normalverteilungskurve (N-Q-Q-Plot), sind also stark heteroskedastisch."
  },
  {
    "objectID": "07_ML.html#logistische-regression",
    "href": "07_ML.html#logistische-regression",
    "title": "6  Machine Learning",
    "section": "6.5 Logistische Regression",
    "text": "6.5 Logistische Regression\nDa eine lineare Regression b-Werte zur Folge hätte, die für reale Werte in den UVs Werte unter 0 und über 1 für die AV vorhersagen würde, wird eine logistische Regression gerechnet. Werte unter 0 und über 1 können nicht existieren, weil die AV eben eine Dummy ist und nur die Werte 0 und 1 kennt.\nDie Formel einer logistischen Regression sieht eigentlich so aus:\n\\[\\begin{align}\nP(y = 1)  = & \\frac{1}{1+e^{-z}} \\text{der Logit z ist die Regressionsgleichung}\\\\\n       Y_i=&b_1 + b_2X_{i2} + b_3X_{i3}+e_i\\\\\nP(Y_i) = & \\frac{1}{1+e^{-(b_1+b_2X_{2i})}}\\\\\nP(Y_i) = & \\frac{1}{1+e^{-(b_1+b_2X_{2i}+b_3X_{3i}+ \\text{\\dots} +  b_nX_{ni})}}\n\\end{align}\\]\nDie Gerade ist keine Gerade mehr, sondern eine S-Kurve und sieht in etwa so aus:\n\n\n\n\n\n\nDie berechneten b’s sind kaum inhaltlich interpretierbar. Im «summary» Output stehen in der Spalte «Estimates» die b’s. Was man sehen und sagen kann ist, dass die Mitfahrenden der 2. Klasse, im Vergleich zur 3. Klasse, eine bessere Chance hatten, zu überleben (der Estimate (b und keine OR) ist signifikant positiv). Die Mitreisenden der ersten Klasse hatten eine noch grössere Chance zu überleben (b ist positiv, grösser als bei Pclass_f2, hat auch einen grösseren z-Wert und einen kleineren p-Wert).\n\n## \n## Call:\n## glm(formula = Survived ~ Pclass_f + Sex + Kinder, family = binomial, \n##     data = train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2275  -0.6829  -0.4035   0.6743   2.2578  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)   0.2353     0.2209   1.065  0.28684    \n## Pclass_f2     1.1302     0.2864   3.946 7.94e-05 ***\n## Pclass_f1     2.1581     0.2871   7.518 5.57e-14 ***\n## Sexmale      -2.7026     0.2438 -11.087  < 2e-16 ***\n## KinderTRUE    1.1727     0.3732   3.142  0.00168 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 724.29  on 535  degrees of freedom\n## Residual deviance: 483.87  on 531  degrees of freedom\n##   (132 observations deleted due to missingness)\n## AIC: 493.87\n## \n## Number of Fisher Scoring iterations: 5\n\nBesser als die b’s können die exponentiellen b’s EXP(B) gelesen werden. Sie geben eine «Odds Ratio» an. Das kann so gelesen werden, wie Multiplikatoren von Wahrscheinlichkeiten. Die «Odds Ratios» in Tabelle @ref(tab:Publikationsoutput1) bwz. die «OR» in Tabelle @ref(tab:Publikationsoutput2) geben diese Werte raus, die man mit (Wett)quoten übersetzen könnte. Sie fangen bei >0 an und können unendlich gross werden. Wenn eine Variable keinen Einfluss auf die Wahrscheinlichkeit des Ausgangs der AV hat, dann ist ihr b = 1. Im Beipspiel kann man ablesen, dass im Vergleich zur 3. Passagierklasse (Pclass_f ist die Referenz und darum in Tabelle @ref(tab:Publikationsoutput1) gar nicht zu sehen und in Tabelle @ref(tab:Publikationsoutput2) ausgestrichen) die 2. Passagierklasse eine 3.1-fache Überlebenschance hatte und die 1. Passagierklasse eine 8.65-fache.\n\n\n\n\n(\\#tab:Publikationsoutput1) Überlebensanalyse zum Titanicunglück mit sjPlot\n\n \nSurvived\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.27\n0.82 – 1.96\n0.287\n\n\nPclass f [2]\n3.10\n1.77 – 5.47\n<0.001\n\n\nPclass f [1]\n8.65\n4.99 – 15.41\n<0.001\n\n\nSex [male]\n0.07\n0.04 – 0.11\n<0.001\n\n\nKinderTRUE\n3.23\n1.56 – 6.78\n0.002\n\n\nObservations\n536\n\n\nR2 Tjur\n0.404\n\n\n\n\n\n\nDie Modellausgabe kann auch mit gtsummary erfolgen (und es gibt einige weitere Pakete). Bei gtsummary werden die Spalten für die Analyse durch Befehle in der Pipe ergänzt. In der folgenden Variante werden die Odds-Ratios (OR) rausgelassen und ihre Konfidenzintervalle (CI) sowie die p-Werte und den (generalisierten) Varianzinflationsfaktor. Der kommt sogar noch mit einer Anpassung, dem «Adjusted GVIF». So lange der unter 2 liegt, wird die Analyse nicht zu sehr von Multikollinearität gestört.\n\n\n\n\n\n\n  Überlebensanalyse zum Titanicunglück mit gtsummary\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n      GVIF1\n      Adjusted GVIF2,1\n    \n  \n  \n    Pclass_f\n\n\n\n1.2\n1.0\n        3\n—\n—\n\n\n\n        2\n3.10\n1.77, 5.47\n<0.001\n\n\n        1\n8.65\n4.99, 15.4\n<0.001\n\n\n    Sex\n\n\n\n1.1\n1.0\n        female\n—\n—\n\n\n\n        male\n0.07\n0.04, 0.11\n<0.001\n\n\n    Kinder\n\n\n\n1.1\n1.0\n        FALSE\n—\n—\n\n\n\n        TRUE\n3.23\n1.56, 6.78\n0.002\n\n\n  \n  \n    \n      Null deviance = 724; Null df = 535; Log-likelihood = -242; AIC = 494; BIC = 515; Deviance = 484; Residual df = 531; No. Obs. = 536\n    \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval, GVIF = Generalized Variance Inflation Factor\n    \n    \n      2 GVIF^[1/(2*df)]"
  },
  {
    "objectID": "07_ML.html#voraussetzungschecks-1",
    "href": "07_ML.html#voraussetzungschecks-1",
    "title": "6  Machine Learning",
    "section": "6.6 Voraussetzungschecks",
    "text": "6.6 Voraussetzungschecks\n\n6.6.0.1 Multikollinearität\n\n## Es gibt ein Paket \"olsrr\" für die Prüfung der OLS-Voraussetzungen, \n## wo man sich den VIF und die Toleranz rauslassen kann:s\nlm(Survived ~ Pclass_f + Sex + Age_z + I(Age_z^2) + Kinder,  family=binomial, data=train) |> \n  olsrr::ols_vif_tol() \n## Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n##  extra argument 'family' will be disregarded\n##    Variables Tolerance      VIF\n## 1  Pclass_f2 0.8579868 1.165519\n## 2  Pclass_f1 0.7502936 1.332811\n## 3    Sexmale 0.9486830 1.054093\n## 4      Age_z 0.3265338 3.062470\n## 5 I(Age_z^2) 0.3995874 2.502582\n## 6 KinderTRUE 0.2886407 3.464515\n\n\n\n6.6.1 Residualplot\n\n\nplot(model)\n\n\n\n\nHistogramm der Residuen\n\n\n\n\n\n\n\nHistogramm der Residuen\n\n\n\n\n\n\n\nHistogramm der Residuen\n\n\n\n\n\n\n\nHistogramm der Residuen\n\n\n\n\n\n6.6.1.1 Vorhersagetest\n\n## Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n## ℹ Please use `as_tibble()` instead.\n## ℹ The signature and semantics have changed, see `?as_tibble`.\n##        predicted   0   1\n## actual                  \n## 0                100   6\n## 1                 31  41\n## # A tibble: 1 × 1\n##   Accuracy\n##      <dbl>\n## 1    0.792"
  },
  {
    "objectID": "80_Glossar.html",
    "href": "80_Glossar.html",
    "title": "8  Glossar",
    "section": "",
    "text": "Glossar"
  },
  {
    "objectID": "02_GLM.html#section",
    "href": "02_GLM.html#section",
    "title": "2  Das lineare Modell",
    "section": "2.3 :::",
    "text": "2.3 :::\nWas wissen Sie über das Bestimmtheitsmass \\(R^2\\)?\n\n\n\n\n \\(R^2\\) ist die Gesamtvarianz geteilt durch die Modellvarianz. \\(R^2\\) ist ein Zusammenhangsmass und wird für jede UV angegeben. \\(R^2\\) ist ein Mass für die Modellgüte. \\(R^2\\) liegt immer zwischen 0 und 1\n\n\n\n2.3.1 Kennwerte der Regression\n\n\n\n\n\nKennwerte auf Modellgüte\n\n\n\n\n\n\n\n\n\nKennwerte auf Variablenebene\n\n\n\n\n\n\n\n\n\nKennwerte der Signifikanz der b’s\n\n\n\n\n\n\n\n\n\nKennwerte der Multikollinearität"
  },
  {
    "objectID": "02_GLM.html#iyi-nur-für-formelliebende-ableitung-der-ols-funktion",
    "href": "02_GLM.html#iyi-nur-für-formelliebende-ableitung-der-ols-funktion",
    "title": "2  Das lineare Modell",
    "section": "2.3 IYI (nur für Formelliebende): Ableitung der OLS-Funktion",
    "text": "2.3 IYI (nur für Formelliebende): Ableitung der OLS-Funktion\n\\[\ne_t=Y_t-\\hat{Y}_t=Y_t-b_1-b_2 X_{t 2}-b_3 X_{t 3}\n\\tag{2.1}\\]\n\\[\n\\sum_{t=1}^T e_t^2=\\sum_{t=1}^T\\left(Y_t-\\hat{Y}_t\\right)^2=\\sum_{t=1}^T\\left(Y_t-b_1-b_2 X_2-b_3 X_{t 3}\\right)^2\n\\tag{2.2}\\]\n\\[\n\\frac{\\partial S S E}{\\partial b}=\\frac{\\partial\\left(\\sum_{t=1}^T e_t^2\\right)}{\\partial b}=\\frac{\\partial\\left(e_1^2\\right)}{\\partial b}+\\frac{\\partial\\left(e_2^2\\right)}{\\partial b}+\\cdots+\\frac{\\partial\\left(e_T^2\\right)}{\\partial b}=\\sum_{t=1}^T \\frac{\\partial\\left(e_t^2\\right)}{\\partial b}\n\\tag{2.3}\\]\n\\[\n\\frac{\\partial\\left(e_t^2\\right)}{\\partial b}=\\frac{\\partial\\left(e_t^2\\right)}{\\partial e_t} \\frac{\\partial e_t}{\\partial b}=2 e_t \\frac{\\partial e_t}{\\partial b}\n\\tag{2.4}\\]\nund nach Gleichung Gleichung 2.1, \\[\n\\begin{gathered}\n\\frac{\\partial e_t}{\\partial b_1}=\\frac{\\partial\\left(Y_t-b_1-b_2 X_{t 2}-b_3 X_{t 3}\\right)}{\\partial b_1}=-1,\n\\end{gathered}\n\\tag{2.5}\\]\n\\[\n\\begin{gathered}\n\\frac{\\partial e_t}{\\partial b_2}=\\frac{\\partial\\left(Y_t-b_1-b_2 X_{t 2}-b_3 X_{t 3}\\right)}{\\partial b_2}=-X_{t 2},\n\\end{gathered}\n\\tag{2.6}\\]\n\\[\n\\begin{gathered}\n\\frac{\\partial e_t}{\\partial b_3}=\\frac{\\partial\\left(Y_t-b_1-b_2 X_{t 2}-b_3 X_{t 3}\\right)}{\\partial b_3}=-X_{t 3} .\n\\end{gathered}\n\\tag{2.7}\\]\njetzt alle Formeln von Gleichung 2.5 bis Gleichung 2.7 zusammen und die einzelnen Ableitungen gleich 0 gesetzt, um die Funktionen zu minimieren: \\[\n\\begin{aligned}\n\\frac{\\partial S S E}{\\partial b_1} & =2 \\sum_{t=1}^T e_t \\frac{\\partial e_t}{\\partial b_1}=2 \\sum_{t=1}^T\\left(Y_t-b_1-b_2 X_{t 2}-b_3 X_{t 3}\\right)(-1) \\\\\n& =-2 \\sum_t Y_t+2 \\sum_t b_1+2 b_2 \\sum_t X_{t 2}+2 b_3 \\sum_t X_{t 3}\n\\end{aligned}\n\\]\ndafür können wir schreiben: \\[\n\\begin{gathered}\n-\\sum_t Y_t+T b_1+b_2 \\sum_t X_{t 2}+b_3 \\sum_t X_{13}=0 \\\\\n\\frac{\\partial S S E}{\\partial b_2}=2 \\sum_{t=1}^T e_t \\frac{\\partial e_t}{\\partial b_2}=2 \\sum_{t=1}^T\\left(Y_t-b_1-b_2 X_{t 2}-b_3 X_{t 3}\\right)\\left(-X_{t 2}\\right) \\\\\n\\end{gathered}\n\\tag{2.8}\\]\noder \\[\n\\begin{gathered}\n-\\sum_t Y_t X_{t 2}+b_1 \\sum_t X_{t 2}+b_2 \\sum_t X_{t 2}^2+b_3 \\sum_t X_{t 3} X_{t 2}=0 \\\\\n\\frac{\\partial S S E}{\\partial b_3}=2 \\sum_{t=1}^T e_t \\frac{\\partial e_t}{\\partial b_3}=2 \\sum_{t=1}^T\\left(Y_t-b_1-b_2 X_{t 2}-b_3 X_{t 3}\\right)\\left(-X_{t 3}\\right)\n\\end{gathered}\n\\tag{2.9}\\]\noder \\[-\n\\begin{gathered}\n-\\sum_t Y_t X_{t 3}+b_1 \\sum_t X_{t 3}+b_2 \\sum_t X_{t 2} X_{t 3}+b_3 \\sum_t X_{t 3}^2=0 .\n\\end{gathered}\n\\tag{2.10}\\]\nJetzt teilen wir jeweils die Gleichung 2.8 bis Gleichung 2.10 durch die Fallzahl, also \\(T\\), woraus sich ergibt:\n\\[\n\\begin{aligned}\nb_1+a_1 b_2+a_2 b_3 & =c_1, \\\\\na_1 b_1+a_3 b_2+a_4 b_3 & =c_2, \\\\\na_2 b_1+a_4 b_2+a_3 b_3 & =c_3,\n\\end{aligned}\n\\]\nwobei\n\\[\n\\begin{gathered}\na_1=\\frac{1}{T} \\sum X_{t 2}=\\bar{X}_2, \\quad a_2=\\frac{1}{T} \\sum X_{t 3}=\\bar{X}_3, \\quad a_3=\\frac{1}{T} \\sum X_{t 2}^2, \\\\\na_4=\\frac{1}{T} \\sum X_{t 2} X_{t 3}, \\quad a_5=\\frac{1}{T} \\sum X_{t 3}^2, \\\\\nc_1=\\frac{1}{T} \\sum Y_t=\\bar{Y}, \\quad c_2=\\frac{1}{T} \\sum Y_t X_{t 2}, \\quad c_3=\\frac{1}{T} \\sum Y_t X_{t 3} .\n\\end{gathered}\n\\tag{2.11}\\]\nDurch Einsetzten erhalten wir also: \\[\n\\bar{Y}=b_1+b_2 \\bar{X}_2+b_3 \\bar{X}_3 \\quad \\text { oder } \\quad b_1=\\bar{Y}-b_2 \\bar{X}_2-b_3 \\bar{X}_3\n\\tag{2.12}\\]\nund Gleichung 2.9 sowie Gleichung 2.10 sind\n\\[\n\\begin{aligned}\n& \\bar{X}_2 b_1+\\left(\\frac{1}{T} \\sum X_{t 2}^2\\right) b_2+\\left(\\frac{1}{T} \\sum X_{t 2} X_{t 3}\\right) b_3=\\frac{1}{T} \\sum Y_t X_{t 2} \\\\\n& \\bar{X}_3 b_1+\\left(\\frac{1}{T} \\sum X_{t 2} X_{t 3}\\right) b_2+\\left(\\frac{1}{T} \\sum X_{t 3}^2\\right) b_3=\\frac{1}{T} \\sum Y_t X_{t 3} .\n\\end{aligned}\n\\tag{2.13}\\]\nWenn man jetzt das \\(b_1\\) aus Gleichung 2.12 einsetzt, ergibt sich \\[\n\\begin{aligned}\n& b_2\\left(\\frac{1}{T} \\sum X_{t 2}^2-\\bar{X}_2^2\\right)+b_3\\left(\\frac{1}{T} \\sum X_{t 2} X_{t 3}-\\bar{X}_2 \\bar{X}_3\\right)=\\left(\\frac{1}{T} \\sum Y_t X_{t 2}-\\bar{Y} \\bar{X}_2\\right) \\\\\n& b_2\\left(\\frac{1}{T} \\sum X_{t 2} X_{t 3}-\\bar{X}_2 \\bar{X}_3\\right)+b_3\\left(\\frac{1}{T} \\sum X_{t 3}^2-\\bar{X}_3^2\\right)=\\left(\\frac{1}{T} \\sum Y_t X_{t 3}-\\bar{Y} \\bar{X}_3\\right)\n\\end{aligned}\n\\]\nBy definition of the variance of the variable \\(X\\left[V_X=(1 / T) \\sum X_t^2-\\bar{X}^2\\right]\\) and the covariance of two variables \\(X\\) and \\(Y\\left[C_{X Y}=(1 / T) \\sum X_1 Y_t-\\bar{X} \\bar{Y}\\right]\\), these two equations are \\[\nb_2 V_{X_2}+b_3 C_{X_2 X_3}=C_{Y X_2}, \\quad b_2 C_{X_2 X_3}+b_3 V_{X_3}=C_{Y X_3}\n\\] For notational convenience, from this point on we shall suppress the \\(X\\) subscript. Thus, \\(V_2\\) is the variance of \\(X_2, C_{23}\\) is the covariance of \\(X_2\\) and \\(X_3\\),\n\\[\n\\begin{aligned}\nb_3=\\left(V_2 C_{Y 3}-C_{23} C_{Y 2}\\right) /\\left(V_2 V_3-C_{23}^2\\right) .\n\\end{aligned}\n\\tag{2.14}\\]\n\\[\n\\begin{aligned}\n& b_3=\\left(V_2 C_{Y 3}-C_{23} C_{Y 2}\\right) /\\left(V_2 V_3-C_{23}^2\\right) .\n\\end{aligned}\n\\tag{2.15}\\]\nFür die Gleichung 2.14 kann man auch statt der Covarianzen und Varianzen schreiben\n\\[\nb_2=\\frac{r_{Y 2}-r_{23} r_{Y 3}}{\\left(1-r_{23}^2\\right)} \\frac{S_Y}{S_2} .\n\\]\nThe second necessary condition, and a relatively important one, is that \\(D=V_2 V_3-C_{23}^2\\) must not equal zero. If \\(D\\) does equal zero, then the required divisions cannot be executed, and we have no estimates. The simplest way to see how \\(D\\) might equal 0 is to divide \\(D\\) by \\(V_2 V_3\\) as in Eq. (2.15’). As long as \\(V_2 \\neq 0\\) and \\(V_3 \\neq 0, D=0\\) if and only if \\(1-r_{23}^2=0\\), or if \\(\\left|r_{23}\\right|=1\\). \\(\\left|r_{23}\\right|=1\\) only if \\(X_2\\) and \\(X_3\\) are perfectly correlated, that is, only if \\(X_2\\) and \\(X_3\\) are linearly related to each other as in \\(X_2=d_1+d_2 X_3\\). For example, if \\(X_2\\) is income in dollars and \\(X_3\\) is income in thousands of dollars \\(\\left(X_2=1000 X_3\\right)\\) or if \\(X_2\\) is a variable that is one for all males and \\(X_3\\) is a similar variable for all females \\(\\left(X_2=1-X_3\\right), D\\) would equal zero, and we could not estimate such a model. Thus, not only must there be some variance in \\(X_2\\) and \\(X_3\\), but they must also exhibit some independent variation \\(\\left(\\left|r_{23}\\right| \\neq 1\\right)\\) in order for us to estimate our model. \\({ }^8\\) This condition corresponds to the heuristic notion that \\(b_2\\) and \\(b_3\\) are computed on the basis of the independent variation that \\(X_2\\) and \\(X_3\\) exhibit. Equations (2.14)-(2.16) give us one way of estimating the coefficients in our behavioral relationship. There are, of course, many other possible estimators or ways of using sample data to estimate the \\(\\beta\\) ’s. For example, we could calculate the deviations of each variable about its mean, giving a set of \\(T\\) observations \\(X_{t 2}-\\bar{X}_2, X_{t 3}-\\bar{X}_3\\), and \\(Y_t-\\bar{Y}\\). From these we could estimate \\(\\beta_2\\) by taking the average of the ratios \\(\\left(Y_t-\\bar{Y}\\right) /\\left(X_{t 2}-\\bar{X}_2\\right)\\), giving the estimate \\[\nb_2^{\\prime}=\\frac{1}{T} \\sum \\frac{Y_t-\\bar{Y}}{X_{t 2}-\\bar{X}_2} \\quad \\text { and } \\quad b_3^{\\prime}=\\frac{1}{T} \\sum \\frac{Y_t-\\bar{Y}}{X_{t 3}-\\bar{X}_3} \\text {. }\n\\] \\(b_1^{\\prime}\\) can be calculated in a fashion similar to Eq. (2.14), \\(b_1^{\\prime}=\\bar{Y}-b_2^{\\prime} \\bar{X}_2-b_3^{\\prime} \\bar{X}_3\\), which puts the plane through the point of means. :::\n\nIch habe Ihnen eine Excel-Datei gebaut, mit der Sie sich das Prinzip von OLS interaktiv anschauen können:\n\n\n\nOLS-xlsx\n\n\n\n\nWelche Funktion und Eigenschaften hat OLS\n\n\n\n\n Mit OLS kann die Grösse der b's bestimmt werden. OLS ist voraussetzungslos Es gibt noch andere Optimierungsverfahren zur Bestimmung von b's.  OLS ist das englische Akronym für Only Linear Systems\n\n\n\n2.3.1 B’s\nWenn wir mit Hilfe der OLS-Methode eine Formel für die b’s gesucht haben, kommt folgende Formel @ref(eq:FormelFuerBs) für das \\(b_2\\) der Variable \\(x_2\\) heraus :\n\\[\\begin{align}\nb_2 = \\frac{r_{y2}-r_{23}r_{y3}}{(1-r_{23}^2)}\\frac{s_y}{s_2} \\label{eq:FormelFuerBs}\n\\end{align}\\]\nDie Formel hat es in sich. Aber schauen Sie sich die Formel mal ganz in Ruhe und stückchenweise an. Als eines der ersten Elemente taucht \\(r_{y2}\\) auf, was so viel heisst, wie die einfache Korrelation zwischen y und der ersten x-Variable, die ja das \\(b_2\\) hat und darum kurz und knapp nur noch mit dem Subscript 2 bedacht wird. Also hängt das b mit der Korrelation zwischen der zugehörigen x-Variable und y zusammen. Da b skalenabhängig ist und r nicht, steht hinten noch dieses \\(\\frac{S_y}{S_2}\\). Dieser Termin sorgt nur dafür, dass b in der Skala von y angegeben ist (darum auch multipliziert mit \\(s_y\\)) – den Teil können Sie schon mal vergessen. Interessanter ist der zweite Teil der Gleichung über dem Bruchstrich: Wir ziehen da das Produkt aus \\(r_{23}\\) und \\(r_{y3}\\) ab. Das heisst, wir gehen von der bivariaten Korrelation aus, rechnen jetzt aber noch die Korrelation raus, die die beiden unabhängigen Variablen \\(x_2\\) und \\(x_3\\) untereinander haben. Wir ziehen allerdings nicht einfach \\(r_{23}\\) ab, sondern multiplizieren das auch noch mit \\(r_{y3}\\). Das bedeutet, wir haben einen Zusammenhang \\(r_{y2}\\) und rechnen aus dem den Anteil gemeinsamer Varianz, also der Zusammenhänge der Varialbe \\(x_2\\) heraus, die diese mit \\(x_3\\), wobei wir nur so viel rausrechnen, wie die dritte Variable \\(x_3\\) wiederum mit y gemeinsam hat. Wären die beiden Variablen \\(x_2\\) und \\(x_3\\) unkorrelliert, dann wäre auch das Produkt \\(r_{23}r_{y3} = 0\\), weil \\(0 \\cdot r_{y3} = 0\\). Wenn \\(x_2\\) und \\(x_3\\) korrellieren, aber \\(x_3\\) und y nicht, dann würden wir auch nichts von \\(r_{y2}\\) abziehen. Im Storchenbeispiel würden wir also sagen, wir sehen den Zusammenhang zwischen Geburtenrate und Anzahl Störche. Wir müssen aber aus dieser Korrelation herausrechnen, dass die Drittvariable (\\(x_3\\)) Bevölkerungsdichte (Stadt vs. Land) stark mit der Geburtenrate korrelliert und mit der Anzahl der Störche, die in einer Region leben.\n\n\n2.3.2 Das Bestimttheitsmass \\(R^2\\)\nDas Bestimmtheitsmass gibt an, wie gut die Werte der AV durch die Werte der UV vorhergesagt werden können.\nWie viel von der Varianz der AV durch ein Modell aufgeklärt werden kann, stellt man fest, indem zunächst die Summe der quadrierten Abweichungen (Sum of Squares) für alle \\(Y_i\\) Werte gezählt werden. Also die totale Varianz der AV, die geschrieben wird als \\(SS_T\\) (Sum of Squares Total). Jetzt ist die Frage, wie viel von dieser Sum of Squares Total durch die Sum of Squares des Modells (\\(SS_M\\)) erklärt werden kann. Darum setzen wir diese beiden Summen der Quadrate (wenn man jeweils durch n teilen würde, wären das die Varianzen) ins Verhältnis zueinander und bekommen einen Prozentwert. Also rechnen wir \\(\\frac{SS_M}{SS_T}\\) und bekommen einen Wert zwischen 0 und 1 bzw. 0% und 100% (% heisst ja «von Hundert» bzw. «geteilt durch 100»). Das ist der aufgeklärte Varianzanteil und den nennen wir \\(R^2\\).\n\n\\(SS_T\\): Summe der quadrierten Abweichungen für die AV (Y).\n\\(SS_M\\): Summe der quadrierten Abweichungen des Modells (der Punkte auf der Geraden, bzw. die geschätzten \\(\\hat{Y_i}\\)-Werte).\n\nAlso: \\(R^2 = \\frac{SS_M}{SS_T}\\)\nBei dieser Formel @ref(eq:Varianzaufklaerung) können wir durch n teilen, also über und unter dem Bruch \\(1/n\\) ergänzen und hätten:\n\\[\\begin{align}\nR^2 = \\frac{SS_M/n}{SS_T/n} \\label{eq:Varianzaufklaerung}\n\\end{align}\\]\nWas in Worten ausgedrückt bedeutet:\n\\[\\begin{align}\nR^2 = \\frac{\\text{aufgeklärte Varianz}}{\\text{Gesamtvarianz}} \\label{eq:Varianzaufklaerung-verbal}\n\\end{align}\\]\n\n\n\n\n\nR-Quadrat\n\n\n\n\nMit dem Bestimmtheitsmass können wir angeben, wie gut ein Modell insgesamt ist. Wir werden später noch diskutieren, wie sinnvoll das ist. Spoiler: Nicht immer sehr sinnvoll, weil \\(R^2\\) eigentlich mehr eine Stichprobeneigenschaft ist und wenig über die Welt sagt und recht einfach hochgeschraubt werden kann, indem man triviale und langweilige Variablen in ein Modell einbaut.\n\n\n\n\n\n\nIYI: Ableitung für \\(R^2\\)\n\n\n\n\n\n\\[\ns^2=\\frac{1}{T-3} \\sum\\left(e_t-\\bar{e}\\right)^2=\\frac{1}{T-3} \\sum e_i^2 .\n\\] ar \\(\\bar{e}=0\\).) The denominator \\(T-3\\) reflects the fac and \\(b_3\\) ) have been estimated; in the general cs tions minus the number of parameters estims alue of \\(\\sum e_t^2, s^2\\) can be shown to be an unbias\n\\(s_{b_2}^2=\\frac{s^2}{T} \\frac{1 / V_2}{1-r_{23}^2} \\quad\\) and \\(\\quad s_{b_3}^2=\\frac{s^2}{T} \\frac{1 / V_3}{1-r_{23}^2}\\) where \\(s_{b_2}^2\\) and \\(s_{b_3}^2\\) denote our estimates of the coefficients’ variance. Since \\(E\\left(s^2\\right)=\\sigma^2\\), and the \\(X^{\\prime}\\) ‹s are fixed, this gives an unbiased estimate of the coefficients› variance. The estimated standard errors of the coefficients, symbolized as \\(s_{b_2}\\) and \\(s_{b_3}\\), are simply the square root of these expressions. The information about the error variance-as estimated from the sum of squared residuals-supplies us with a description of the probability function that generated the errors in the true model. However, there are two transformations of \\(s^2\\) that produce more easily interpreted statistics.\nThe first transformation is simply the square root of \\(s^2\\). This statistic \\(s\\) is important enough to deserve its own name: the standard error of estimate. ’In the first place, \\(s\\) is measured in the same units as \\(Y\\) (whereas \\(s^2\\) is in the units of \\(Y\\) squared). The standard error of estimate is also useful because it gives some feel for the size of the dispersion when compared to tables for the normal distribution. For a normal distribution, we expect about \\(95 \\%\\) of all values to lie within plus or minus two standard deviations of the mean. Thus, if the \\(U_t\\) are normally distributed (as we often assume), we can expect \\(95 \\%\\) of all actual values of \\(Y_t\\) in our sample to be within plus or minus two standard errors of estimate away from the estimated line. \\({ }^8\\) The estimate \\(s\\) thus allows an easier interpretation of the magnitude of the error terms.\nThe second transformation of the error variance makes a comparison with the total amount of variance in behavior \\(\\operatorname{var}(Y)\\) existing in the sample. One may wish to compare how well an estimated model does when matched with a «naïve» guess at the behavior in question.\nWith no information about the underlying behavioral relationships, one guess of the value of any \\(Y_t\\) is the mean value of all the observed \\(Y^{\\prime}\\) s. The variance of \\(Y\\) is then the sum of squared deviations around this guess divided by the number of observations on the behavior. The sum of squared residuals \\(\\left(\\sum e_t^2\\right)\\) is a measure of how far our «sophisticated» guesses, represented by \\(\\hat{Y}_t=b_1+b_2 X_{t 2}+b_3 X_{t 3}\\), diverge from the actual values of \\(Y_t\\). Thus, comparing the residual variance \\(\\left(\\sum e_t^2 / T\\right)\\) to the variance of \\(Y\\) gives some indication of the overall performance of our model relative to the simpler «model.» If we look at \\(\\Sigma e_t^2 / \\Sigma\\left(Y_t-\\bar{Y}\\right)^2\\), we see that this can range from zero when \\(\\Sigma e_t^2=0\\) to\na maximum value of one. (Why, according to the least squares procedure, can this ratio never exceed one?) By convention, we create a new statistic, defined as, \\[\nR^2=1-\\left[\\sum e_t^2 / \\sum\\left(Y_t-\\bar{Y}\\right)^2\\right] .\n\\] This statistic, referred to simply as \\(R\\)-squared (or as the coefficient of determination) has the following properties: (1) when all points fall on the estimated plane so that \\(Y_t \\equiv \\hat{Y}_t, R^2\\) equals one (its maximum); (2) when the mean does as well at predicting \\(Y_t\\) as the estimated equation, \\(R^2\\) equals zero (its minimum); (3) between these two extremes, \\(R^2\\) gives an ordinal measure of how well the model predicts the sample values of \\(Y\\).\nAnother way to look at \\(R^2\\) is found by transforming \\(\\Sigma\\left(Y_t-\\bar{Y}\\right)^2\\) as follows: \\[\n\\begin{aligned}\n\\sum\\left(Y_t-\\bar{Y}\\right)^2 & =\\sum\\left[\\left(Y_t-\\hat{Y}_t\\right)+\\left(\\hat{Y}_t-\\bar{Y}\\right)\\right]^2 \\\\\n& =\\sum\\left(Y_t-\\hat{Y}_t\\right)^2+\\sum\\left(\\hat{Y}_t-\\bar{Y}\\right)^2+2 \\sum\\left(Y_t-\\hat{Y}_t\\right)\\left(\\hat{Y}_t-\\bar{Y}\\right) .\n\\end{aligned}\n\\] Since \\(Y_t-\\hat{Y}_t=e_t\\) and \\(\\hat{Y}_t-\\bar{Y}=b_2\\left(X_{t 2}-\\bar{X}_2\\right)+b_3\\left(X_{t 3}-\\bar{X}_3\\right)\\), the last summation is simply equal to \\(2 b_2 \\Sigma\\left(X_{t 2}-\\bar{X}_2\\right) e_t+2 b_3 \\Sigma\\left(X_{t 3}-\\bar{X}_3\\right) e_t\\). However, by the arithmetic properties of least squares demonstrated above, this is identically zero. Thus \\[\n\\sum\\left(Y_t-\\bar{Y}\\right)^2=\\sum\\left(Y_t-\\hat{Y}\\right)^2+\\sum\\left(\\hat{Y}_t-\\bar{Y}\\right)^2=\\sum e_t^2+\\sum\\left(\\hat{Y}_t-\\bar{Y}\\right)^2\n\\] This says that the variance of \\(Y\\) can be divided into two components. The first \\(\\left(\\Sigma e_t^2\\right)\\) is the «unexplained» portion or the residual portion of the model. The second, called the «explained» portion, indicates how much better the estimated model does than using a fixed estimate of the mean would do. By rearranging (3.12), we see that \\[\nR^2=\\sum\\left(\\hat{Y}_t-\\bar{Y}\\right)^2 / \\sum\\left(Y_t-\\bar{Y}\\right)^2\n\\] Thus, \\(R^2\\) can be interpreted as the proportion of the variation in the sample \\(Y_t\\) explained by the regression equation.\nFinally, if one correlates the actual and predicted values of \\(Y\\), one arrives at the correlation coefficient \\(R\\). Squaring this yields the same value of \\(R^2\\) as found in (3.12) and (3.14).\n\n\n\n\nWas wissen Sie über das Bestimmtheitsmass \\(R^2\\)?\n\n\n\n\n \\(R^2\\) ist die Gesamtvarianz geteilt durch die Modellvarianz. \\(R^2\\) ist ein Zusammenhangsmass und wird für jede UV angegeben. \\(R^2\\) ist ein Mass für die Modellgüte. \\(R^2\\) liegt immer zwischen 0 und 1\n\n\n\n\n2.3.3 Kennwerte der Regression\n\n\n\n\n\nKennwerte auf Modellgüte\n\n\n\n\n\n\n\n\n\nKennwerte auf Variablenebene\n\n\n\n\n\n\n\n\n\nKennwerte der Signifikanz der b’s\n\n\n\n\n\n\n\n\n\nKennwerte der Multikollinearität"
  }
]